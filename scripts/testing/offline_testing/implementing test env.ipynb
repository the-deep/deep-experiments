{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offilne Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Query the leads\n",
    "2. Extract their text\n",
    "3. Split into sentences with preprocessing\n",
    "4. Classify the extracted sentences with the model\n",
    "5. Match the sentences with entries created by human taggers\n",
    "6. Group the output: Entry, Sentence, Original Tag, Predicted Tag \n",
    "- Post-processing: merge contiguous sentences with the same predicted tags.\n",
    "- Add tagger names\n",
    "7. Present the results with streamlit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-- Query to get leads\n",
    "SELECT \n",
    "  ll.*, \n",
    "  COUNT(ee.id) AS number_of_entries, \n",
    "  auth_user.first_name :: text || ' ' || auth_user.last_name AS tagger_name, \n",
    "  af.title analysis_framework_title,\n",
    "  pp.title AS project_title\n",
    "FROM \n",
    "  lead_lead ll \n",
    "  INNER JOIN entry_entry ee ON ee.lead_id = ll.id \n",
    "  INNER JOIN auth_user ON auth_user.id = ee.created_by_id \n",
    "  INNER JOIN analysis_framework_analysisframework af ON af.id = ee.analysis_framework_id \n",
    "  INNER JOIN project_project pp ON pp.id = ee.project_id \n",
    "WHERE \n",
    "  ll.created_at >= '2021-06-04' :: date \n",
    "  and coalesce(TRIM(ll.url), '') != ''\n",
    "  and pp.title IN (\n",
    "    'Americas Regional Population Movement', \n",
    "    'IFRC - Cyclone Idai, March 2019', \n",
    "    'Venezuela crisis 2019', 'Nigeria Situation Analysis (OA)', \n",
    "    'Libya Situation Analysis (OA)', \n",
    "    '2020 DFS Nigeria', '2020 DFS Libya', \n",
    "    'UNHCR Venezuela', 'Yemen Situation Analysis (OA)', \n",
    "    'UNHCR El Salvador', 'UNHCR Peru', \n",
    "    'UNHCR Chile', 'UNHCR Argentina', \n",
    "    'UNHCR Panama', 'UNHCR Guyana', \n",
    "    'UNHCR Dominican Republic', 'UNHCR Costa Rica', \n",
    "    'UNHCR Guatemala', 'UNHCR Honduras', \n",
    "    'UNHCR Curacao', 'UNHCR Colombia', \n",
    "    'UNHCR Uruguay', 'UNHCR Bolivia', \n",
    "    'UNHCR Ecuador', 'UNHCR Aruba', \n",
    "    'UNHCR Trinidad and Tobago', 'UNHCR Mexico', \n",
    "    'UNHCR Paraguay', 'GIMAC Sudan', \n",
    "    'GIMAC Chad', 'GIMAC Cameroon', \n",
    "    'GIMAC South Sudan', 'GIMAC Afghanistan', \n",
    "    'GIMAC Niger', 'GIMAC Somalia', \n",
    "    'IMMAP/DFS Bangladesh', 'IMMAP/DFS Burkina Faso', \n",
    "    'IMMAP/DFS RDC', 'IMMAP/DFS Syria', \n",
    "    'IMMAP/DFS Nigeria', 'IMMAP/DFS Colombia', \n",
    "    'Sudan Floods - September 2020', \n",
    "    'IFRC Democratic Republic of Congo', \n",
    "    'IFRC Guatemala', 'IFRC Kenya', \n",
    "    'The Bahamas - Hurricane Dorian - Early Recovery Assessment', \n",
    "    'Central America - Dengue Outbreak 2019', \n",
    "    'IFRC Turkey', 'IFRC Uganda', 'IFRC Niger', \n",
    "    'IFRC Nigeria', 'IFRC Peru', 'IFRC Philippines', \n",
    "    'IFRC Tajikistan', 'IFRC Yemen', \n",
    "    'Bosnia and Herzegovina_Population Movement Report', \n",
    "    'COVID-19 Americas Region Multi-Sectorial Assessment', \n",
    "    'IFRC Yemen Sit Analysis, July 2020', \n",
    "    'Central America: Hurricanes Eta and Iota', \n",
    "    'Lebanon Situation Analysis', 'IFRC Chile', \n",
    "    'IFRC India'\n",
    "  ) \n",
    "GROUP BY \n",
    "  ll.id, \n",
    "  auth_user.first_name, \n",
    "  auth_user.last_name, \n",
    "  af.title,\n",
    "  pp.title\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-- Query to get entries\n",
    "SELECT \n",
    "  ee.*,\n",
    "  auth_user.first_name :: text || ' ' || auth_user.last_name AS tagger_name, \n",
    "  pp.title as project_title,\n",
    "  af.title as analysis_framework_title\n",
    "FROM \n",
    "  entry_entry ee \n",
    "  INNER JOIN auth_user ON auth_user.id = ee.created_by_id \n",
    "  INNER JOIN project_project pp ON pp.id = ee.project_id\n",
    "  INNER JOIN lead_lead ll ON ll.id = ee.lead_id\n",
    "  INNER JOIN analysis_framework_analysisframework af ON af.id = ee.analysis_framework_id\n",
    "WHERE \n",
    "  ee.entry_type = 'excerpt'\n",
    "  and ll.created_at >= '2021-06-04' :: date \n",
    "  and coalesce(TRIM(ll.url), '') != ''\n",
    "  and pp.title IN (\n",
    "    'Americas Regional Population Movement', \n",
    "    'IFRC - Cyclone Idai, March 2019', \n",
    "    'Venezuela crisis 2019', 'Nigeria Situation Analysis (OA)', \n",
    "    'Libya Situation Analysis (OA)', \n",
    "    '2020 DFS Nigeria', '2020 DFS Libya', \n",
    "    'UNHCR Venezuela', 'Yemen Situation Analysis (OA)', \n",
    "    'UNHCR El Salvador', 'UNHCR Peru', \n",
    "    'UNHCR Chile', 'UNHCR Argentina', \n",
    "    'UNHCR Panama', 'UNHCR Guyana', \n",
    "    'UNHCR Dominican Republic', 'UNHCR Costa Rica', \n",
    "    'UNHCR Guatemala', 'UNHCR Honduras', \n",
    "    'UNHCR Curacao', 'UNHCR Colombia', \n",
    "    'UNHCR Uruguay', 'UNHCR Bolivia', \n",
    "    'UNHCR Ecuador', 'UNHCR Aruba', \n",
    "    'UNHCR Trinidad and Tobago', 'UNHCR Mexico', \n",
    "    'UNHCR Paraguay', 'GIMAC Sudan', \n",
    "    'GIMAC Chad', 'GIMAC Cameroon', \n",
    "    'GIMAC South Sudan', 'GIMAC Afghanistan', \n",
    "    'GIMAC Niger', 'GIMAC Somalia', \n",
    "    'IMMAP/DFS Bangladesh', 'IMMAP/DFS Burkina Faso', \n",
    "    'IMMAP/DFS RDC', 'IMMAP/DFS Syria', \n",
    "    'IMMAP/DFS Nigeria', 'IMMAP/DFS Colombia', \n",
    "    'Sudan Floods - September 2020', \n",
    "    'IFRC Democratic Republic of Congo', \n",
    "    'IFRC Guatemala', 'IFRC Kenya', \n",
    "    'The Bahamas - Hurricane Dorian - Early Recovery Assessment', \n",
    "    'Central America - Dengue Outbreak 2019', \n",
    "    'IFRC Turkey', 'IFRC Uganda', 'IFRC Niger', \n",
    "    'IFRC Nigeria', 'IFRC Peru', 'IFRC Philippines', \n",
    "    'IFRC Tajikistan', 'IFRC Yemen', \n",
    "    'Bosnia and Herzegovina_Population Movement Report', \n",
    "    'COVID-19 Americas Region Multi-Sectorial Assessment', \n",
    "    'IFRC Yemen Sit Analysis, July 2020', \n",
    "    'Central America: Hurricanes Eta and Iota', \n",
    "    'Lebanon Situation Analysis', 'IFRC Chile', \n",
    "    'IFRC India'\n",
    "  ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T09:58:50.053077Z",
     "start_time": "2021-06-23T09:58:48.140098Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/abdullah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nostril import nonsense\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "MIN_NUM_TOKENS = 5\n",
    "MIN_WORD_LEN = 4\n",
    "\n",
    "url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "url_regex = re.compile(url_regex)\n",
    "\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    tokens = sentence.split(\" \")\n",
    "    if len(tokens) < MIN_NUM_TOKENS:\n",
    "        return \"\"\n",
    "    sensible_token_count = 0\n",
    "    for token in tokens:\n",
    "        if len(token) > MIN_WORD_LEN or (len(token) > 7 and not nonsense(token)):\n",
    "            sensible_token_count += 1\n",
    "    if sensible_token_count < MIN_NUM_TOKENS:\n",
    "        return \"\"\n",
    "    sentence = \" \".join(tokens)\n",
    "    sentence = url_regex.sub(\"\", sentence)\n",
    "    keep = re.escape(\"/\\\\$.:,;-_()[]{}!'\\\"% \")\n",
    "    sentence = re.sub(r\"[^\\w\" + keep + \"]\", \"\", sentence)\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def page_to_sentences(page):\n",
    "    page = re.sub(r\"\\s+\", \" \", page)\n",
    "    sentences = sent_tokenize(page)\n",
    "    sentences = [preprocess_sentence(sentence) for sentence in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T09:58:50.171322Z",
     "start_time": "2021-06-23T09:58:50.064357Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "blacklist = [\n",
    "    '[document]',\n",
    "    'noscript',\n",
    "    'header',\n",
    "    'html',\n",
    "    'meta',\n",
    "    'head', \n",
    "    'input',\n",
    "    'script',\n",
    "    # there may be more elements you don't want, such as \"style\", etc.\n",
    "]\n",
    "\n",
    "\n",
    "def extract_html_body(url):\n",
    "    res = requests.get(url)\n",
    "    html_page = res.content\n",
    "    soup = BeautifulSoup(html_page, 'html.parser')\n",
    "    text = soup.find_all(text=True)\n",
    "\n",
    "    output = ''\n",
    "    for t in text:\n",
    "        if t.parent.name not in blacklist:\n",
    "            output += '{} '.format(t)\n",
    "    return output\n",
    "\n",
    "\n",
    "def html_to_sentences(url):\n",
    "    text = extract_html_body(url)\n",
    "    sentences = page_to_sentences(text)\n",
    "\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    sentences = [s for s in sentences if not (s in seen or seen_add(s))]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T09:58:50.863306Z",
     "start_time": "2021-06-23T09:58:50.182256Z"
    }
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "\n",
    "def pdf_to_sentences(file_path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    # Process each page contained in the document.\n",
    "    sentences = []\n",
    "    with open(file_path, \"rb\") as fp:\n",
    "        for page in PDFPage.get_pages(fp):\n",
    "            interpreter.process_page(page)\n",
    "            parsed_page = retstr.getvalue()\n",
    "            sentences.extend(page_to_sentences(parsed_page))\n",
    "\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    sentences = [s for s in sentences if not (s in seen or seen_add(s))]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T09:59:27.194840Z",
     "start_time": "2021-06-23T09:59:27.179883Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "leads = pd.read_csv(\"leads_from_04062021_to_14062021.csv\")\n",
    "entries = pd.read_csv(\"entries_from_04062021_to_14062021.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T09:59:28.052558Z",
     "start_time": "2021-06-23T09:59:28.046552Z"
    }
   },
   "outputs": [],
   "source": [
    "leads = leads[[\n",
    "    'id', 'created_at', 'title', 'status', 'url', 'number_of_entries',\n",
    "    'tagger_name', 'analysis_framework_title', 'project_title'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T09:59:28.589425Z",
     "start_time": "2021-06-23T09:59:28.583224Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PDF_DIR = \"./pdf_leads\"\n",
    "if not os.path.exists(PDF_DIR):\n",
    "    os.makedirs(PDF_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T09:59:29.166369Z",
     "start_time": "2021-06-23T09:59:29.158583Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "\n",
    "def download_file(download_url, file_path):\n",
    "    response = urllib.request.urlopen(download_url)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(response.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T09:59:30.417687Z",
     "start_time": "2021-06-23T09:59:30.362989Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T09:59:31.939231Z",
     "start_time": "2021-06-23T09:59:31.866342Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = pd.read_csv(\n",
    "    \"paraphrase-multilingual-mpnet-base-v2_secotrs-and-pillars_preds.csv\")\n",
    "entries = pd.read_csv(\"entries_from_04062021_to_14062021.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T09:59:32.715922Z",
     "start_time": "2021-06-23T09:59:32.707191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'created_at', 'modified_at', 'excerpt', 'image_raw',\n",
       "       'analysis_framework_id', 'created_by_id', 'lead_id', 'modified_by_id',\n",
       "       'entry_type', 'information_date', 'order', 'client_id', 'project_id',\n",
       "       'tabular_field_id', 'dropped_excerpt', 'highlight_hidden', 'verified',\n",
       "       'verification_last_changed_by_id', 'image_id', 'tagger_name',\n",
       "       'project_title', 'analysis_framework_title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T09:59:33.964702Z",
     "start_time": "2021-06-23T09:59:33.950285Z"
    }
   },
   "outputs": [],
   "source": [
    "def entry_pp(s):\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = re.sub(r\"^\\[.+\\] \", \"\", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "entries[\"excerpt_clean\"] = entries[\"excerpt\"].apply(entry_pp)\n",
    "entries = entries[~entries[\"excerpt\"].isna()]\n",
    "entries = entries[~entries[\"excerpt\"].eq(\"\")]\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T09:59:34.563988Z",
     "start_time": "2021-06-23T09:59:34.557598Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = sentences[~sentences[\"sentence\"].isna()]\n",
    "sentences = sentences[~sentences[\"sentence\"].eq(\"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T10:02:16.996799Z",
     "start_time": "2021-06-23T10:00:18.708849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7b668f7ba7439f808268f8a6adc729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13850.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# sentences: for each sentence: fuzzymatch its entries that have the same lead_id\n",
    "# take a sentence\n",
    "threshold = 70\n",
    "matches = []\n",
    "for i, sentence_row in tqdm(enumerate(sentences.itertuples()),\n",
    "                            total=len(sentences)):\n",
    "    sentence = sentence_row.sentence  # sentence_row[1][\"excerpt_text\"]\n",
    "    lead_id = sentence_row.lead_id  # sentence_row[1][\"lead_id\"]\n",
    "    # extract entries with same lead_id\n",
    "    candidates = entries[entries[\"lead_id\"] == lead_id]\n",
    "    candidates = candidates[\"excerpt_clean\"].to_dict()\n",
    "    matching_output = process.extractOne(sentence,\n",
    "                                         candidates,\n",
    "                                         scorer=fuzz.token_set_ratio)\n",
    "    if matching_output is None:\n",
    "        continue\n",
    "    matching_entry_text, ratio, matching_entry_idx = matching_output\n",
    "    if ratio >= threshold:\n",
    "        matches.append((sentence_row[0], matching_entry_idx, ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T11:17:15.993162Z",
     "start_time": "2021-06-23T11:17:11.687277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1144a4726fa4f3da9bfbf237e013c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1572.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "long = 0\n",
    "short = 0\n",
    "long_matches = []\n",
    "short_matches = []\n",
    "for orig_i, mod_i, _ in tqdm(matches):\n",
    "    s1 = sentences.loc[orig_i, \"sentence\"]\n",
    "    s2 = entries.loc[mod_i, \"excerpt_clean\"]\n",
    "    if len(s2) > len(s1):\n",
    "        s1, s2 = s2, s1\n",
    "    match = SequenceMatcher(None, s1, s2,\n",
    "                            False).find_longest_match(0, len(s1), 0, len(s2))\n",
    "    match_ratio = match.size / len(s2)\n",
    "    match_len = len(s1[match.a:match.a + match.size].split())\n",
    "    if match_len > 5:\n",
    "        long += 1\n",
    "        long_matches.append((orig_i, mod_i, match.size, match_ratio))\n",
    "    else:\n",
    "        short += 1\n",
    "        short_matches.append((orig_i, mod_i, match.size, match_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T11:31:02.341895Z",
     "start_time": "2021-06-23T11:31:02.332911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['entry_id', 'lead_id', 'project_id', 'project_title',\n",
       "       'analysis_framework_id', 'analysis_framework_title', 'excerpt',\n",
       "       'dropped_excerpt', 'created_by_id', 'tagger_name', 'modified_by_id',\n",
       "       'verified', 'verification_last_changed_by_id', 'sectors', 'pillars',\n",
       "       'subpillars'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries_labeled = pd.read_csv(\"data_exported.csv\")\n",
    "entries_labeled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T11:31:04.865621Z",
     "start_time": "2021-06-23T11:31:03.383933Z"
    }
   },
   "outputs": [],
   "source": [
    "ready_matches = []\n",
    "for sen_idx, ent_idx, score in matches:\n",
    "\n",
    "    entry = entries.at[ent_idx, \"excerpt_clean\"]\n",
    "    ent_id = entries.at[ent_idx, \"id\"]\n",
    "    if ent_id in entries_labeled[\"entry_id\"].values:\n",
    "        orig_sectors = entries_labeled.loc[\n",
    "            entries_labeled[\"entry_id\"].eq(ent_id), \"sectors\"].iloc[0]\n",
    "        orig_pillars = entries_labeled.loc[\n",
    "            entries_labeled[\"entry_id\"].eq(ent_id), \"pillars\"].iloc[0]\n",
    "        tagger_name = entries_labeled.loc[\n",
    "            entries_labeled[\"entry_id\"].eq(ent_id), \"tagger_name\"].iloc[0]\n",
    "        project_title = entries_labeled.loc[\n",
    "            entries_labeled[\"entry_id\"].eq(ent_id), \"project_title\"].iloc[0]\n",
    "        analysis_framework_title = entries_labeled.loc[\n",
    "            entries_labeled[\"entry_id\"].eq(ent_id),\n",
    "            \"analysis_framework_title\"].iloc[0]\n",
    "    else:\n",
    "        orig_sectors = None\n",
    "        orig_pillars = None\n",
    "\n",
    "    sentence = sentences.at[sen_idx, \"sentence\"]\n",
    "    p_sectors = sentences.at[sen_idx, \"sectors\"]\n",
    "    p_pillars = sentences.at[sen_idx, \"pillars\"]\n",
    "\n",
    "    lead_id = sentences.at[sen_idx, \"lead_id\"]\n",
    "    lead_url = leads.loc[leads[\"id\"].eq(lead_id), \"url\"].iloc[0]\n",
    "    if score < 70:\n",
    "        # lead_url, entry, original_sector, original_pillar, matched_sentence, predicted_sector, predicted_pillar\n",
    "        ready_matches.append(\n",
    "            (lead_url, analysis_framework_title, project_title, None, None,\n",
    "             None, None, sentence, p_sectors, p_pillars))\n",
    "    else:\n",
    "        ready_matches.append((lead_url, analysis_framework_title,\n",
    "                              project_title, entry, tagger_name, orig_sectors,\n",
    "                              orig_pillars, sentence, p_sectors, p_pillars))\n",
    "ready_matches = pd.DataFrame(ready_matches,\n",
    "                             columns=[\n",
    "                                 \"Lead URL\", \"Analysis Framework Title\",\n",
    "                                 \"Project Title\", \"Entry\", \"Tagger Name\",\n",
    "                                 \"Sectors\", \"Pillars\", \"Matched Sentence\",\n",
    "                                 \"Predicted Sectors\", \"Predicted Pillars\"\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T11:31:09.265534Z",
     "start_time": "2021-06-23T11:31:09.263373Z"
    }
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T11:31:09.649333Z",
     "start_time": "2021-06-23T11:31:09.579592Z"
    }
   },
   "outputs": [],
   "source": [
    "ready_matches.loc[ready_matches[\"Pillars\"].isna(), \"Pillars\"] = \"None\"\n",
    "ready_matches[\"Pillars\"] = ready_matches[\"Pillars\"].apply(literal_eval)\n",
    "ready_matches.loc[ready_matches[\"Sectors\"].isna(), \"Sectors\"] = \"None\"\n",
    "ready_matches[\"Sectors\"] = ready_matches[\"Sectors\"].apply(literal_eval)\n",
    "ready_matches.loc[ready_matches[\"Predicted Pillars\"].isna(), \"Predicted Pillars\"] = \"None\"\n",
    "ready_matches[\"Predicted Pillars\"] = ready_matches[\"Predicted Pillars\"].apply(literal_eval)\n",
    "ready_matches.loc[ready_matches[\"Predicted Sectors\"].isna(), \"Predicted Sectors\"] = \"None\"\n",
    "ready_matches[\"Predicted Sectors\"] = ready_matches[\"Predicted Sectors\"].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T11:31:15.116339Z",
     "start_time": "2021-06-23T11:31:14.583380Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_matched_sentences(group):\n",
    "    url = group[\"Lead URL\"].iloc[0]\n",
    "    orig_pillars = group[~group[\"Pillars\"].isna()][\"Pillars\"]\n",
    "    if len(orig_pillars):\n",
    "        orig_pillars = orig_pillars.iloc[0]\n",
    "    else:\n",
    "        orig_pillars = None\n",
    "    orig_sectors = group[~group[\"Sectors\"].isna()][\"Sectors\"]\n",
    "    if len(orig_sectors):\n",
    "        orig_sectors = orig_sectors.iloc[0]\n",
    "    else:\n",
    "        orig_sectors = None\n",
    "    entry = group[\"Entry\"].iloc[0]\n",
    "    af_title = group[\"Analysis Framework Title\"].iloc[0]\n",
    "    p_title = group[\"Project Title\"].iloc[0]\n",
    "    tagger = group[\"Tagger Name\"].iloc[0]\n",
    "    ##\n",
    "    concat_sens = \". \".join(group[\"Matched Sentence\"])\n",
    "    p_pillars = list(set([p for ps in group[\"Predicted Pillars\"] for p in ps]))\n",
    "    p_sectors = list(set([s for ss in group[\"Predicted Sectors\"] for s in ss]))\n",
    "    return {\n",
    "        \"Lead URL\": url,\n",
    "        \"Analysis Framework Title\": af_title,\n",
    "        \"Project Title\": p_title,\n",
    "        \"Entry\": entry,\n",
    "        \"Tagger Name\": tagger,\n",
    "        \"Pillars\": orig_pillars,\n",
    "        \"Sectors\": orig_sectors,\n",
    "        \"Matched Sentences\": concat_sens,\n",
    "        \"Predicted Pillars\": p_pillars,\n",
    "        \"Predicted Sectors\": p_sectors\n",
    "    }\n",
    "\n",
    "\n",
    "grouped_matches = ready_matches.groupby(\"Entry\").apply(merge_matched_sentences)\n",
    "grouped_matches = pd.DataFrame.from_dict(grouped_matches.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T11:31:16.050703Z",
     "start_time": "2021-06-23T11:31:16.015534Z"
    }
   },
   "outputs": [],
   "source": [
    "grouped_matches.to_csv(\"entries_vs_sentences_tags.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd0f487e277ea6a75fd1c7c341a1deb40c7861148cbc006695943c5304af00fedbe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
