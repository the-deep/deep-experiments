{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These requirements are necessary if you launch this notebook from SageMaker instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install mlflow\\n!pip install pytorch-lightning\\n!pip install transformers\\n!pip install tqdm\\n!pip install sagemaker\\n\\n!pip install s3fs\\n!pip install smdebug'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!pip install mlflow\n",
    "!pip install pytorch-lightning\n",
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "!pip install sagemaker\n",
    "\n",
    "!pip install s3fs\n",
    "!pip install smdebug\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from scripts.script_token_mean_postprocessing.merge_leads_excerpts import get_training_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local constants, regarding the data, MLFlow server, paths, etc..: use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('../../../../')\n",
    "from deep.constants import *\n",
    "from deep.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/selim/anaconda3/envs/deepl/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3357: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "Using custom data configuration default-039825f33ea33186\n",
      "Reusing dataset json (/home/selim/.cache/huggingface/datasets/json/default-039825f33ea33186/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n",
      "Loading cached processed dataset at /home/selim/.cache/huggingface/datasets/json/default-039825f33ea33186/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-f116e2040d270b6c.arrow\n"
     ]
    }
   ],
   "source": [
    "use_sample = True\n",
    "\n",
    "DATA_PATH = os.path.join(\n",
    "    \"..\", \"..\", \"..\", \"..\", \"data\", \"frameworks_data\", \"data_v0.7.1\"\n",
    ")\n",
    "EXCERPTS_PATH = os.path.join(DATA_PATH, \"full_dataset_with_translations.csv\")\n",
    "LEADS_PATH = os.path.join(DATA_PATH, \"leads_data.json\")\n",
    "\n",
    "data_folder = \"data\"\n",
    "if use_sample:\n",
    "    sample_percentage = 0.01\n",
    "    data_file_name = \"sample_data.csv\"  # sample data\n",
    "else:\n",
    "    data_file_name = \"full_data.csv\"  # full data\n",
    "    sample_percentage = 1\n",
    "\n",
    "\n",
    "# tbd each time to make sure changes in files are taken into account.\n",
    "data_df = get_training_dict(\n",
    "    leads_data_path=LEADS_PATH,\n",
    "    excerpts_df_path=EXCERPTS_PATH,\n",
    "    use_sample=use_sample,\n",
    "    sample_percentage=sample_percentage,\n",
    ")\n",
    "data_df.to_csv(os.path.join(data_folder, data_file_name), index=None)\n",
    "\n",
    "# do ths because\n",
    "data_df = pd.read_csv(os.path.join(data_folder, data_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T15:42:32.024647Z",
     "start_time": "2021-05-27T15:42:31.984694Z"
    }
   },
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:29:20.899415Z",
     "start_time": "2021-06-09T08:29:19.327852Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session(default_bucket=DEV_BUCKET.name)\n",
    "role = SAGEMAKER_ROLE\n",
    "role_arn = SAGEMAKER_ROLE_ARN\n",
    "tracking_uri = MLFLOW_SERVER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to upload data to an S3 bucket. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLFLOW_SERVER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### send data to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f\"pytorch-{formatted_time()}-entry-extraction\"  # change it as you prefer\n",
    "input_path = DEV_BUCKET / 'training' / 'input_data' / job_name  # Do not change this\n",
    "\n",
    "data_path = str(input_path / 'data.pickle') # keep it as it is\n",
    "\n",
    "# send data to s3 bucket\n",
    "# need too check protocol, depending on data type (protocol 4 was made for pandas data inputs)\n",
    "\n",
    "#data_df = get_df_from_dict(data)\n",
    "data_df.to_pickle(data_path, protocol=4)  # protocol 4 is necessary, since SageMaker uses python 3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:43.284096Z",
     "start_time": "2021-06-09T08:31:43.206457Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPU instances\n",
    "instances = [\n",
    "    'ml.p2.xlarge',\n",
    "    'ml.p3.2xlarge'\n",
    "]\n",
    "\n",
    "# CPU instances\n",
    "instances = [\n",
    "    'ml.c4.2xlarge',\n",
    "    'ml.c4.4xlarge',\n",
    "    'ml.c5n.2xlarge'\n",
    "]\n",
    "\n",
    "# https://aws.amazon.com/sagemaker/pricing/instance-types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters are passed as command line arguments to the training script. \n",
    "\n",
    "You can add/change them as you like. It's important to keep the `tracking_uri` and the `experiment_name` which are used by MLFlow.\n",
    "\n",
    "The class `PyTorch` is part of the `SageMaker` python API. The parameters are important and you should probably not change most of them. The ones you may want to change are:\n",
    "\n",
    "- `instance_type`, specify the instance you want\n",
    "- `source_dir`, specify your script directory. Try to use global variable as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:43.458886Z",
     "start_time": "2021-06-09T08:31:43.304626Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.p3.2xlarge\"\n",
    "\n",
    "experiment_name = \"entry-extraction\"\n",
    "run_name = experiment_name  \n",
    "\n",
    "hyperparameters = {\n",
    "    \"instance_type\": instance_type,\n",
    "    \"tracking_uri\": MLFLOW_SERVER,\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"model_name_or_path\": \"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\n",
    "    \"tokenizer_name_or_path\": \"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"n_epochs\": 1 if use_sample else 3,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"dataloader_num_workers\": 6,\n",
    "    \"val_batch_size\": 16,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"max_len\": 512,\n",
    "    \"extra_context_length\": 48,\n",
    "    \"dropout\": 0.2,\n",
    "    \"tokens_focal_loss_gamma\": 1,\n",
    "    \"cls_focal_loss_gamma\": 1,\n",
    "    \"fbeta\": 1,\n",
    "    \"sample_percentage\": sample_percentage,\n",
    "    \"proportions_pow\": 0.2,\n",
    "    #\"n_separate_layers\": 1,\n",
    "    #\"per_device_train_batch_size\": 1,\n",
    "    #\"per_device_eval_batch_size\": 1,\n",
    "    #\"gradient_accumulation_steps\": 8,\n",
    "    #\"save_strategy\": \"epoch\",\n",
    "    #\"adam_beta1\": 0.9,\n",
    "    #\"adam_beta2\": 0.98,\n",
    "    #\"adam_epsilon\": 1e-6,\n",
    "    #\"warmup_ratio\": 0.3,\n",
    "    #\"fp16\": true,\n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=str(\n",
    "        \"scripts/script_token_mean_postprocessing\"\n",
    "    ),\n",
    "    output_path=str(DEV_BUCKET / \"models/\"),\n",
    "    code_location=str(input_path),\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    framework_version=\"1.8\",\n",
    "    py_version=\"py3\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    job_name=job_name,\n",
    "    debugger_hook_config=False\n",
    "    #     train_instance_count=2,\n",
    "    #     train_instance_type=\"ml.c4.xlarge\",\n",
    ")\n",
    "\n",
    "fit_arguments = {\"train\": str(input_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:45.995868Z",
     "start_time": "2021-06-09T08:31:43.484212Z"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-16 07:48:38 Starting - Starting the training job...\n",
      "2022-12-16 07:49:04 Starting - Preparing the instances for trainingProfilerReport-1671176913: InProgress\n",
      ".........\n",
      "2022-12-16 07:50:50 Downloading - Downloading input data\n",
      "2022-12-16 07:50:50 Training - Downloading the training image..................\n",
      "2022-12-16 07:54:26 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-12-16 07:54:44,295 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-12-16 07:54:44,323 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-12-16 07:54:44,325 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-12-16 07:54:44,536 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.8.2\n",
      "  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting torchmetrics==0.4.1\n",
      "  Downloading torchmetrics-0.4.1-py3-none-any.whl (234 kB)\u001b[0m\n",
      "\u001b[34mCollecting pytorch-lightning==1.3.8\n",
      "  Downloading pytorch_lightning-1.3.8-py3-none-any.whl (813 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm==4.61.2 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (4.61.2)\u001b[0m\n",
      "\u001b[34mCollecting mlflow==1.19.0\n",
      "  Downloading mlflow-1.19.0-py3-none-any.whl (14.4 MB)\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn==0.24\n",
      "  Downloading scikit_learn-0.24.0-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker==2.49.1\n",
      "  Downloading sagemaker-2.49.1.tar.gz (421 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3fs==2021.07.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (2021.7.0)\u001b[0m\n",
      "\u001b[34mCollecting smdebug==1.0.11\n",
      "  Downloading smdebug-1.0.11-py2.py3-none-any.whl (269 kB)\u001b[0m\n",
      "\u001b[34mCollecting pytest==7.0\n",
      "  Downloading pytest-7.0.0-py3-none-any.whl (296 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (2.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (4.8.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.10.31-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (756 kB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub==0.0.12\n",
      "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (3.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3.1 in /opt/conda/lib/python3.6/site-packages (from torchmetrics==0.4.1->-r requirements.txt (line 2)) (1.8.1)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard!=2.5.0,>=2.2.0\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 3)) (0.18.2)\u001b[0m\n",
      "\u001b[34mCollecting pyDeprecate==0.3.0\n",
      "  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow!=8.3.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 3)) (8.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 3)) (2021.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: entrypoints in /opt/conda/lib/python3.6/site-packages (from mlflow==1.19.0->-r requirements.txt (line 5)) (0.3)\u001b[0m\n",
      "\u001b[34mCollecting querystring-parser\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.6/site-packages (from mlflow==1.19.0->-r requirements.txt (line 5)) (8.0.3)\u001b[0m\n",
      "\u001b[34mCollecting databricks-cli>=0.8.7\n",
      "  Downloading databricks-cli-0.17.4.tar.gz (82 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting alembic<=1.4.1\n",
      "  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting sqlparse>=0.3.1\n",
      "  Downloading sqlparse-0.4.3-py3-none-any.whl (42 kB)\u001b[0m\n",
      "\u001b[34mCollecting prometheus-flask-exporter\n",
      "  Downloading prometheus_flask_exporter-0.21.0-py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting gunicorn\n",
      "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.6/site-packages (from mlflow==1.19.0->-r requirements.txt (line 5)) (2.0.0)\u001b[0m\n",
      "\u001b[34mCollecting sqlalchemy\n",
      "  Downloading SQLAlchemy-1.4.45-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from mlflow==1.19.0->-r requirements.txt (line 5)) (1.1.5)\u001b[0m\n",
      "\u001b[34mCollecting gitpython>=2.1.0\n",
      "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\u001b[0m\n",
      "\u001b[34mCollecting docker>=4.0.0\n",
      "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from mlflow==1.19.0->-r requirements.txt (line 5)) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Flask in /opt/conda/lib/python3.6/site-packages (from mlflow==1.19.0->-r requirements.txt (line 5)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.7.0 in /opt/conda/lib/python3.6/site-packages (from mlflow==1.19.0->-r requirements.txt (line 5)) (3.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.24->-r requirements.txt (line 6)) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.24->-r requirements.txt (line 6)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.24->-r requirements.txt (line 6)) (1.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs in /opt/conda/lib/python3.6/site-packages (from sagemaker==2.49.1->-r requirements.txt (line 7)) (21.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3>=1.16.32 in /opt/conda/lib/python3.6/site-packages (from sagemaker==2.49.1->-r requirements.txt (line 7)) (1.20.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.6/site-packages (from sagemaker==2.49.1->-r requirements.txt (line 7)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.6/site-packages (from sagemaker==2.49.1->-r requirements.txt (line 7)) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker==2.49.1->-r requirements.txt (line 7)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.6/site-packages (from sagemaker==2.49.1->-r requirements.txt (line 7)) (0.2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiobotocore>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from s3fs==2021.07.0->-r requirements.txt (line 8)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyinstrument>=3.1.3 in /opt/conda/lib/python3.6/site-packages (from smdebug==1.0.11->-r requirements.txt (line 9)) (3.4.2)\u001b[0m\n",
      "\u001b[34mCollecting tomli>=1.0.0\n",
      "  Downloading tomli-1.2.3-py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting py>=1.8.2\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from huggingface-hub==0.0.12->transformers==4.8.2->-r requirements.txt (line 1)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aioitertools>=0.5.1 in /opt/conda/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 8)) (0.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.23.25,>=1.23.24 in /opt/conda/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 8)) (1.23.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp>=3.3.1 in /opt/conda/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 8)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wrapt>=1.10.10 in /opt/conda/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 8)) (1.13.3)\u001b[0m\n",
      "\u001b[34mCollecting Mako\n",
      "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34mCollecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.6/site-packages (from alembic<=1.4.1->mlflow==1.19.0->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker==2.49.1->-r requirements.txt (line 7)) (0.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker==2.49.1->-r requirements.txt (line 7)) (0.10.0)\u001b[0m\n",
      "\u001b[34mCollecting pyjwt>=1.7.0\n",
      "  Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.1.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.6/site-packages (from databricks-cli>=0.8.7->mlflow==1.19.0->-r requirements.txt (line 5)) (0.8.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from databricks-cli>=0.8.7->mlflow==1.19.0->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.6/site-packages (from docker>=4.0.0->mlflow==1.19.0->-r requirements.txt (line 5)) (1.2.3)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.8.2->-r requirements.txt (line 1)) (3.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.8.2->-r requirements.txt (line 1)) (3.0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyinstrument-cext>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from pyinstrument>=3.1.3->smdebug==1.0.11->-r requirements.txt (line 9)) (0.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.8.2->-r requirements.txt (line 1)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.8.2->-r requirements.txt (line 1)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.8.2->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.8.2->-r requirements.txt (line 1)) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.6/site-packages (from sqlalchemy->mlflow==1.19.0->-r requirements.txt (line 5)) (1.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 3)) (2.0.2)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 3)) (58.0.4)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.48.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 3)) (0.36.2)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.15.0-py2.py3-none-any.whl (177 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.6/site-packages (from Flask->mlflow==1.19.0->-r requirements.txt (line 5)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.6/site-packages (from Flask->mlflow==1.19.0->-r requirements.txt (line 5)) (3.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker==2.49.1->-r requirements.txt (line 7)) (0.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker==2.49.1->-r requirements.txt (line 7)) (0.70.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker==2.49.1->-r requirements.txt (line 7)) (1.6.6.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker==2.49.1->-r requirements.txt (line 7)) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.6/site-packages (from prometheus-flask-exporter->mlflow==1.19.0->-r requirements.txt (line 5)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 8)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna-ssl>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 8)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 8)) (0.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 8)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 8)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 8)) (5.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 8)) (1.7.2)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 3)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.6/site-packages (from Jinja2>=3.0->Flask->mlflow==1.19.0->-r requirements.txt (line 5)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 3)) (0.4.8)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker, alembic, databricks-cli, sacremoses\n",
      "  Building wheel for sagemaker (setup.py): started\n",
      "  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-2.49.1-py2.py3-none-any.whl size=591938 sha256=7c1a3ce39f43750518d9b9d57f2d27b5f30d599f0f54626178614b269a29e071\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/af/ea/8ff5943a87155df5b184e54474fbf2b59b75e5c172854643c6\n",
      "  Building wheel for alembic (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for alembic (setup.py): finished with status 'done'\n",
      "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158170 sha256=35c836543aeaa625b5faeaa3ff54f96a32e172300dd6818d9a0aeefa58bd6b89\n",
      "  Stored in directory: /root/.cache/pip/wheels/e9/7b/aa/e18c983d8236b141f85838ba0f8e4e4ae9bcf7f1e00ff726ec\n",
      "  Building wheel for databricks-cli (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
      "  Created wheel for databricks-cli: filename=databricks_cli-0.17.4-py3-none-any.whl size=142893 sha256=f9e9a2913ff011085810225f0890adc5af0d983a97ebff94fb37910b924b7b44\n",
      "  Stored in directory: /root/.cache/pip/wheels/3a/02/39/c20d43e7613f33717df248d07e972f02c706054d69fd20029e\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=c506c545834137a118be650f37c44af8c402a9c69f9c0c019377eeb96dfa4386\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/64/31/e9900a234b23fb3e9dc565d6114a9d6ff84a72dbdd356502b4\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker alembic databricks-cli sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pyasn1-modules, oauthlib, cachetools, smmap, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, sqlalchemy, regex, python-editor, pyjwt, markdown, Mako, grpcio, google-auth-oauthlib, gitdb, absl-py, torchmetrics, tomli, tokenizers, tensorboard, sqlparse, sacremoses, querystring-parser, pyDeprecate, py, prometheus-flask-exporter, pluggy, iniconfig, huggingface-hub, gunicorn, gitpython, docker, databricks-cli, alembic, transformers, smdebug, scikit-learn, sagemaker, pytorch-lightning, pytest, mlflow\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: smdebug\n",
      "    Found existing installation: smdebug 1.0.9\n",
      "    Uninstalling smdebug-1.0.9:\n",
      "      Successfully uninstalled smdebug-1.0.9\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\u001b[0m\n",
      "\u001b[34m    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.72.0\n",
      "    Uninstalling sagemaker-2.72.0:\n",
      "      Successfully uninstalled sagemaker-2.72.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed Mako-1.1.6 absl-py-1.3.0 alembic-1.4.1 cachetools-4.2.4 databricks-cli-0.17.4 docker-5.0.3 gitdb-4.0.9 gitpython-3.1.18 google-auth-2.15.0 google-auth-oauthlib-0.4.6 grpcio-1.48.2 gunicorn-20.1.0 huggingface-hub-0.0.12 iniconfig-1.1.1 markdown-3.3.7 mlflow-1.19.0 oauthlib-3.2.2 pluggy-1.0.0 prometheus-flask-exporter-0.21.0 py-1.11.0 pyDeprecate-0.3.0 pyasn1-modules-0.2.8 pyjwt-2.4.0 pytest-7.0.0 python-editor-1.0.4 pytorch-lightning-1.3.8 querystring-parser-1.2.4 regex-2022.10.31 requests-oauthlib-1.3.1 sacremoses-0.0.53 sagemaker-2.49.1 scikit-learn-0.24.0 smdebug-1.0.11 smmap-5.0.0 sqlalchemy-1.4.45 sqlparse-0.4.3 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tokenizers-0.10.3 tomli-1.2.3 torchmetrics-0.4.1 transformers-4.8.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-12-16 07:55:11,994 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"cls_focal_loss_gamma\": 1,\n",
      "        \"dataloader_num_workers\": 6,\n",
      "        \"dropout\": 0.2,\n",
      "        \"experiment_name\": \"entry-extraction\",\n",
      "        \"extra_context_length\": 48,\n",
      "        \"fbeta\": 1,\n",
      "        \"instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"learning_rate\": 0.0001,\n",
      "        \"max_len\": 512,\n",
      "        \"model_name_or_path\": \"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\n",
      "        \"n_epochs\": 1,\n",
      "        \"proportions_pow\": 0.2,\n",
      "        \"run_name\": \"entry-extraction\",\n",
      "        \"sample_percentage\": 0.01,\n",
      "        \"tokenizer_name_or_path\": \"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\n",
      "        \"tokens_focal_loss_gamma\": 1,\n",
      "        \"tracking_uri\": \"http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\",\n",
      "        \"train_batch_size\": 8,\n",
      "        \"val_batch_size\": 16,\n",
      "        \"weight_decay\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-2022-12-16-13-33-29-546-entry-extraction\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-deep-experiments-dev/training/input_data/pytorch-2022-12-16-13-33-29-546-entry-extraction/pytorch-2022-12-16-13-33-29-546-entry-extraction/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"cls_focal_loss_gamma\":1,\"dataloader_num_workers\":6,\"dropout\":0.2,\"experiment_name\":\"entry-extraction\",\"extra_context_length\":48,\"fbeta\":1,\"instance_type\":\"ml.p3.2xlarge\",\"learning_rate\":0.0001,\"max_len\":512,\"model_name_or_path\":\"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\"n_epochs\":1,\"proportions_pow\":0.2,\"run_name\":\"entry-extraction\",\"sample_percentage\":0.01,\"tokenizer_name_or_path\":\"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\"tokens_focal_loss_gamma\":1,\"tracking_uri\":\"http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\",\"train_batch_size\":8,\"val_batch_size\":16,\"weight_decay\":0.01}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-deep-experiments-dev/training/input_data/pytorch-2022-12-16-13-33-29-546-entry-extraction/pytorch-2022-12-16-13-33-29-546-entry-extraction/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"cls_focal_loss_gamma\":1,\"dataloader_num_workers\":6,\"dropout\":0.2,\"experiment_name\":\"entry-extraction\",\"extra_context_length\":48,\"fbeta\":1,\"instance_type\":\"ml.p3.2xlarge\",\"learning_rate\":0.0001,\"max_len\":512,\"model_name_or_path\":\"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\"n_epochs\":1,\"proportions_pow\":0.2,\"run_name\":\"entry-extraction\",\"sample_percentage\":0.01,\"tokenizer_name_or_path\":\"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\"tokens_focal_loss_gamma\":1,\"tracking_uri\":\"http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\",\"train_batch_size\":8,\"val_batch_size\":16,\"weight_decay\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-2022-12-16-13-33-29-546-entry-extraction\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-deep-experiments-dev/training/input_data/pytorch-2022-12-16-13-33-29-546-entry-extraction/pytorch-2022-12-16-13-33-29-546-entry-extraction/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--cls_focal_loss_gamma\",\"1\",\"--dataloader_num_workers\",\"6\",\"--dropout\",\"0.2\",\"--experiment_name\",\"entry-extraction\",\"--extra_context_length\",\"48\",\"--fbeta\",\"1\",\"--instance_type\",\"ml.p3.2xlarge\",\"--learning_rate\",\"0.0001\",\"--max_len\",\"512\",\"--model_name_or_path\",\"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\"--n_epochs\",\"1\",\"--proportions_pow\",\"0.2\",\"--run_name\",\"entry-extraction\",\"--sample_percentage\",\"0.01\",\"--tokenizer_name_or_path\",\"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\"--tokens_focal_loss_gamma\",\"1\",\"--tracking_uri\",\"http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\",\"--train_batch_size\",\"8\",\"--val_batch_size\",\"16\",\"--weight_decay\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_CLS_FOCAL_LOSS_GAMMA=1\u001b[0m\n",
      "\u001b[34mSM_HP_DATALOADER_NUM_WORKERS=6\u001b[0m\n",
      "\u001b[34mSM_HP_DROPOUT=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_EXPERIMENT_NAME=entry-extraction\u001b[0m\n",
      "\u001b[34mSM_HP_EXTRA_CONTEXT_LENGTH=48\u001b[0m\n",
      "\u001b[34mSM_HP_FBETA=1\u001b[0m\n",
      "\u001b[34mSM_HP_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_LEN=512\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\u001b[0m\n",
      "\u001b[34mSM_HP_N_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_PROPORTIONS_POW=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_RUN_NAME=entry-extraction\u001b[0m\n",
      "\u001b[34mSM_HP_SAMPLE_PERCENTAGE=0.01\u001b[0m\n",
      "\u001b[34mSM_HP_TOKENIZER_NAME_OR_PATH=nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\u001b[0m\n",
      "\u001b[34mSM_HP_TOKENS_FOCAL_LOSS_GAMMA=1\u001b[0m\n",
      "\u001b[34mSM_HP_TRACKING_URI=http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_VAL_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --cls_focal_loss_gamma 1 --dataloader_num_workers 6 --dropout 0.2 --experiment_name entry-extraction --extra_context_length 48 --fbeta 1 --instance_type ml.p3.2xlarge --learning_rate 0.0001 --max_len 512 --model_name_or_path nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large --n_epochs 1 --proportions_pow 0.2 --run_name entry-extraction --sample_percentage 0.01 --tokenizer_name_or_path nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large --tokens_focal_loss_gamma 1 --tracking_uri http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/ --train_batch_size 8 --val_batch_size 16 --weight_decay 0.01\u001b[0m\n",
      "\u001b[34m#015Validation sanity check: 0it [00:00, ?it/s]#015Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s][2022-12-16 07:56:06.885 algo-1:57 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-12-16 07:56:06.913 algo-1:57 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m#015Validation sanity check:  50%|█████     | 1/2 [00:01<00:01,  1.53s/it]#015Validation sanity check: 100%|██████████| 2/2 [00:01<00:00,  1.31it/s]#015                                                                      #015#015Training: 0it [00:00, ?it/s]#015Training:   0%|          | 0/23 [00:00<?, ?it/s]#015Epoch 0:   0%|          | 0/23 [00:00<?, ?it/s] #015Epoch 0:   4%|▍         | 1/23 [00:00<00:19,  1.12it/s]#015Epoch 0:   4%|▍         | 1/23 [00:00<00:19,  1.12it/s, loss=13, val_loss=13.10]#015Epoch 0:   9%|▊         | 2/23 [00:01<00:14,  1.41it/s, loss=13, val_loss=13.10]#015Epoch 0:   9%|▊         | 2/23 [00:01<00:14,  1.41it/s, loss=12.6, val_loss=13.10]#015Epoch 0:  13%|█▎        | 3/23 [00:01<00:12,  1.54it/s, loss=12.6, val_loss=13.10]#015Epoch 0:  13%|█▎        | 3/23 [00:01<00:12,  1.54it/s, loss=12, val_loss=13.10]  #015Epoch 0:  17%|█▋        | 4/23 [00:02<00:11,  1.62it/s, loss=12, val_loss=13.10]#015Epoch 0:  17%|█▋        | 4/23 [00:02<00:11,  1.62it/s, loss=11.6, val_loss=13.10]#015Epoch 0:  22%|██▏       | 5/23 [00:03<00:10,  1.67it/s, loss=11.6, val_loss=13.10]#015Epoch 0:  22%|██▏       | 5/23 [00:03<00:10,  1.67it/s, loss=11.1, val_loss=13.10]#015Epoch 0:  26%|██▌       | 6/23 [00:03<00:09,  1.70it/s, loss=11.1, val_loss=13.10]#015Epoch 0:  26%|██▌       | 6/23 [00:03<00:10,  1.70it/s, loss=10.6, val_loss=13.10]#015Epoch 0:  30%|███       | 7/23 [00:04<00:09,  1.73it/s, loss=10.6, val_loss=13.10]#015Epoch 0:  30%|███       | 7/23 [00:04<00:09,  1.73it/s, loss=10.1, val_loss=13.10]#015Epoch 0:  35%|███▍      | 8/23 [00:04<00:08,  1.75it/s, loss=10.1, val_loss=13.10]#015Epoch 0:  35%|███▍      | 8/23 [00:04<00:08,  1.75it/s, loss=9.65, val_loss=13.10]#015Epoch 0:  39%|███▉      | 9/23 [00:05<00:07,  1.76it/s, loss=9.65, val_loss=13.10]#015Epoch 0:  39%|███▉      | 9/23 [00:05<00:07,  1.76it/s, loss=9.18, val_loss=13.10]#015Epoch 0:  43%|████▎     | 10/23 [00:05<00:07,  1.78it/s, loss=9.18, val_loss=13.10]#015Epoch 0:  43%|████▎     | 10/23 [00:05<00:07,  1.78it/s, loss=8.74, val_loss=13.10]#015Epoch 0:  48%|████▊     | 11/23 [00:06<00:06,  1.79it/s, loss=8.74, val_loss=13.10]#015Epoch 0:  48%|████▊     | 11/23 [00:06<00:06,  1.78it/s, loss=8.31, val_loss=13.10]#015Epoch 0:  52%|█████▏    | 12/23 [00:06<00:06,  1.79it/s, loss=8.31, val_loss=13.10]#015Epoch 0:  52%|█████▏    | 12/23 [00:06<00:06,  1.79it/s, loss=7.9, val_loss=13.10] #015Epoch 0:  57%|█████▋    | 13/23 [00:07<00:05,  1.80it/s, loss=7.9, val_loss=13.10]#015Epoch 0:  57%|█████▋    | 13/23 [00:07<00:05,  1.80it/s, loss=7.52, val_loss=13.10]#015Epoch 0:  61%|██████    | 14/23 [00:07<00:04,  1.81it/s, loss=7.52, val_loss=13.10]#015Epoch 0:  61%|██████    | 14/23 [00:07<00:04,  1.81it/s, loss=7.17, val_loss=13.10]#015Epoch 0:  65%|██████▌   | 15/23 [00:08<00:04,  1.82it/s, loss=7.17, val_loss=13.10]#015Epoch 0:  65%|██████▌   | 15/23 [00:08<00:04,  1.82it/s, loss=6.84, val_loss=13.10]#015Epoch 0:  70%|██████▉   | 16/23 [00:08<00:03,  1.82it/s, loss=6.84, val_loss=13.10]#015Epoch 0:  70%|██████▉   | 16/23 [00:08<00:03,  1.82it/s, loss=6.56, val_loss=13.10]#015Epoch 0:  74%|███████▍  | 17/23 [00:09<00:03,  1.82it/s, loss=6.56, val_loss=13.10]#015Epoch 0:  74%|███████▍  | 17/23 [00:09<00:03,  1.82it/s, loss=6.29, val_loss=13.10]#015Epoch 0:  78%|███████▊  | 18/23 [00:09<00:02,  1.83it/s, loss=6.29, val_loss=13.10]#015Epoch 0:  78%|███████▊  | 18/23 [00:09<00:02,  1.83it/s, loss=6.05, val_loss=13.10]#015Epoch 0:  83%|████████▎ | 19/23 [00:10<00:02,  1.85it/s, loss=6.05, val_loss=13.10]#015Epoch 0:  83%|████████▎ | 19/23 [00:10<00:02,  1.85it/s, loss=5.83, val_loss=13.10]\u001b[0m\n",
      "\u001b[34m#015Validating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:   0%|          | 0/4 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  25%|██▌       | 1/4 [00:00<00:01,  2.42it/s]#033[A#015Epoch 0:  91%|█████████▏| 21/23 [00:10<00:01,  1.96it/s, loss=5.83, val_loss=13.10]\u001b[0m\n",
      "\u001b[34m#015Validating:  50%|█████     | 2/4 [00:00<00:00,  3.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  75%|███████▌  | 3/4 [00:00<00:00,  3.70it/s]#033[A#015Epoch 0: 100%|██████████| 23/23 [00:11<00:00,  2.06it/s, loss=5.83, val_loss=13.10]\u001b[0m\n",
      "\u001b[34m#015Validating: 100%|██████████| 4/4 [00:00<00:00,  4.78it/s]#033[A#015Epoch 0: 100%|██████████| 23/23 [00:11<00:00,  2.01it/s, loss=5.83, val_loss=1.640, train_loss=6.020]\u001b[0m\n",
      "\u001b[34m#015                                                         #033[A#015Epoch 0: 100%|██████████| 23/23 [00:14<00:00,  1.64it/s, loss=5.83, val_loss=1.640, train_loss=6.020]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/357 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 357/357 [00:00<00:00, 570kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/488 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 488/488 [00:00<00:00, 770kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]#015Downloading:  92%|█████████▏| 4.65M/5.07M [00:00<00:00, 42.6MB/s]#015Downloading: 100%|██████████| 5.07M/5.07M [00:00<00:00, 43.0MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/9.08M [00:00<?, ?B/s]#015Downloading:  55%|█████▌    | 5.02M/9.08M [00:00<00:00, 50.2MB/s]#015Downloading: 100%|██████████| 9.08M/9.08M [00:00<00:00, 52.0MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 239/239 [00:00<00:00, 348kB/s]\u001b[0m\n",
      "\u001b[34mGPU available: True, used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/215M [00:00<?, ?B/s]#015Downloading:   2%|▏         | 4.79M/215M [00:00<00:04, 47.9MB/s]#015Downloading:   5%|▍         | 10.1M/215M [00:00<00:04, 50.7MB/s]#015Downloading:   7%|▋         | 15.5M/215M [00:00<00:03, 52.3MB/s]#015Downloading:  10%|▉         | 20.9M/215M [00:00<00:03, 53.0MB/s]#015Downloading:  12%|█▏        | 26.2M/215M [00:00<00:03, 53.1MB/s]#015Downloading:  15%|█▍        | 31.7M/215M [00:00<00:03, 53.6MB/s]#015Downloading:  17%|█▋        | 37.1M/215M [00:00<00:03, 54.0MB/s]#015Downloading:  20%|█▉        | 42.6M/215M [00:00<00:03, 54.1MB/s]#015Downloading:  22%|██▏       | 48.0M/215M [00:00<00:03, 54.2MB/s]#015Downloading:  25%|██▍       | 53.5M/215M [00:01<00:02, 54.5MB/s]#015Downloading:  27%|██▋       | 59.0M/215M [00:01<00:02, 54.5MB/s]#015Downloading:  30%|██▉       | 64.4M/215M [00:01<00:02, 54.5MB/s]#015Downloading:  33%|███▎      | 70.0M/215M [00:01<00:02, 54.7MB/s]#015Downloading:  35%|███▌      | 75.4M/215M [00:01<00:02, 54.7MB/s]#015Downloading:  38%|███▊      | 80.9M/215M [00:01<00:02, 54.6MB/s]#015Downloading:  40%|████      | 86.5M/215M [00:01<00:02, 54.9MB/s]#015Downloading:  43%|████▎     | 92.0M/215M [00:01<00:02, 54.8MB/s]#015Downloading:  45%|████▌     | 97.4M/215M [00:01<00:02, 54.7MB/s]#015Downloading:  48%|████▊     | 103M/215M [00:01<00:02, 54.8MB/s] #015Downloading:  50%|█████     | 109M/215M [00:02<00:01, 55.0MB/s]#015Downloading:  53%|█████▎    | 114M/215M [00:02<00:01, 55.1MB/s]#015Downloading:  56%|█████▌    | 120M/215M [00:02<00:01, 55.1MB/s]#015Downloading:  58%|█████▊    | 125M/215M [00:02<00:01, 55.3MB/s]#015Downloading:  61%|██████    | 131M/215M [00:02<00:01, 55.1MB/s]#015Downloading:  63%|██████▎   | 136M/215M [00:02<00:01, 55.0MB/s]#015Downloading:  66%|██████▌   | 142M/215M [00:02<00:01, 55.3MB/s]#015Downloading:  69%|██████▊   | 147M/215M [00:02<00:01, 55.3MB/s]#015Downloading:  71%|███████   | 153M/215M [00:02<00:01, 55.4MB/s]#015Downloading:  74%|███████▎  | 158M/215M [00:02<00:01, 55.4MB/s]#015Downloading:  76%|███████▋  | 164M/215M [00:03<00:00, 55.3MB/s]#015Downloading:  79%|███████▉  | 169M/215M [00:03<00:00, 55.3MB/s]#015Downloading:  81%|████████▏ | 175M/215M [00:03<00:00, 55.4MB/s]#015Downloading:  84%|████████▍ | 181M/215M [00:03<00:00, 55.5MB/s]#015Downloading:  87%|████████▋ | 186M/215M [00:03<00:00, 55.5MB/s]#015Downloading:  89%|████████▉ | 192M/215M [00:03<00:00, 55.2MB/s]#015Downloading:  92%|█████████▏| 197M/215M [00:03<00:00, 55.2MB/s]#015Downloading:  94%|█████████▍| 203M/215M [00:03<00:00, 55.2MB/s]#015Downloading:  97%|█████████▋| 208M/215M [00:03<00:00, 55.2MB/s]#015Downloading: 100%|█████████▉| 214M/215M [00:03<00:00, 55.3MB/s]#015Downloading: 100%|██████████| 215M/215M [00:03<00:00, 54.7MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m2022-12-16 07:57:21,106 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "  | Name                   | Type           | Params\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------\u001b[0m\n",
      "\u001b[34m0 | entry_extraction_model | EntryExtractor | 149 M \u001b[0m\n",
      "\u001b[34m1 | token_focal_loss       | FocalLoss      | 0     \u001b[0m\n",
      "\u001b[34m2 | cls_focal_loss         | FocalLoss      | 0     \u001b[0m\n",
      "\u001b[34m----------------------------------------------------------\u001b[0m\n",
      "\u001b[34m51.6 M    Trainable params\u001b[0m\n",
      "\u001b[34m98.0 M    Non-trainable params\u001b[0m\n",
      "\u001b[34m149 M     Total params\u001b[0m\n",
      "\u001b[34m598.324   Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "  \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/callback_hook.py:101: LightningDeprecationWarning: The signature of `Callback.on_train_epoch_end` has changed in v1.3. `outputs` parameter has been removed. Support for the old signature will be removed in v1.5\n",
      "  \"The signature of `Callback.on_train_epoch_end` has changed in v1.3.\"\u001b[0m\n",
      "\u001b[34mEpoch 0, global step 18: val_loss reached 1.63657 (best 1.63657), saving model to \"/opt/ml/model/transformer_model.ckpt\" as top 1\u001b[0m\n",
      "\u001b[34mFIT Profiler Report\u001b[0m\n",
      "\u001b[34mAction                             #011|  Mean duration (s)#011|Num calls      #011|  Total time (s) #011|  Percentage %   #011|\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mTotal                              #011|  -              #011|_              #011|  64.569         #011|  100 %          #011|\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mrun_training_epoch                 #011|  14.013         #011|1              #011|  14.013         #011|  21.703         #011|\u001b[0m\n",
      "\u001b[34mrun_training_batch                 #011|  0.52561        #011|19             #011|  9.9866         #011|  15.466         #011|\u001b[0m\n",
      "\u001b[34moptimizer_step_and_closure_0       #011|  0.5251         #011|19             #011|  9.977          #011|  15.452         #011|\u001b[0m\n",
      "\u001b[34mtraining_step_and_backward         #011|  0.44397        #011|19             #011|  8.4354         #011|  13.064         #011|\u001b[0m\n",
      "\u001b[34mbackward                           #011|  0.25517        #011|19             #011|  4.8482         #011|  7.5085         #011|\u001b[0m\n",
      "\u001b[34mmodel_forward                      #011|  0.17929        #011|19             #011|  3.4066         #011|  5.2758         #011|\u001b[0m\n",
      "\u001b[34mtraining_step                      #011|  0.17873        #011|19             #011|  3.3958         #011|  5.2592         #011|\u001b[0m\n",
      "\u001b[34mon_validation_end                  #011|  1.2879         #011|2              #011|  2.5757         #011|  3.9891         #011|\u001b[0m\n",
      "\u001b[34mevaluation_step_and_end            #011|  0.28806        #011|6              #011|  1.7284         #011|  2.6768         #011|\u001b[0m\n",
      "\u001b[34mvalidation_step                    #011|  0.28774        #011|6              #011|  1.7265         #011|  2.6738         #011|\u001b[0m\n",
      "\u001b[34mon_train_start                     #011|  0.77141        #011|1              #011|  0.77141        #011|  1.1947         #011|\u001b[0m\n",
      "\u001b[34mget_train_batch                    #011|  0.013983       #011|19             #011|  0.26569        #011|  0.41147        #011|\u001b[0m\n",
      "\u001b[34mon_validation_epoch_end            #011|  0.051871       #011|2              #011|  0.10374        #011|  0.16067        #011|\u001b[0m\n",
      "\u001b[34mon_train_batch_end                 #011|  0.0010451      #011|19             #011|  0.019856       #011|  0.030751       #011|\u001b[0m\n",
      "\u001b[34mon_validation_batch_end            #011|  0.00056949     #011|6              #011|  0.0034169      #011|  0.0052919      #011|\u001b[0m\n",
      "\u001b[34mcache_result                       #011|  2.5699e-05     #011|117            #011|  0.0030068      #011|  0.0046567      #011|\u001b[0m\n",
      "\u001b[34mon_train_epoch_end                 #011|  0.00094919     #011|1              #011|  0.00094919     #011|  0.00147        #011|\u001b[0m\n",
      "\u001b[34mon_validation_start                #011|  0.00036874     #011|2              #011|  0.00073749     #011|  0.0011422      #011|\u001b[0m\n",
      "\u001b[34mon_after_backward                  #011|  3.2251e-05     #011|19             #011|  0.00061278     #011|  0.00094902     #011|\u001b[0m\n",
      "\u001b[34mon_batch_start                     #011|  2.988e-05      #011|19             #011|  0.00056772     #011|  0.00087924     #011|\u001b[0m\n",
      "\u001b[34mon_batch_end                       #011|  2.4632e-05     #011|19             #011|  0.00046802     #011|  0.00072483     #011|\u001b[0m\n",
      "\u001b[34mon_before_zero_grad                #011|  2.2419e-05     #011|19             #011|  0.00042595     #011|  0.00065968     #011|\u001b[0m\n",
      "\u001b[34mon_train_epoch_start               #011|  0.00042297     #011|1              #011|  0.00042297     #011|  0.00065506     #011|\u001b[0m\n",
      "\u001b[34mon_train_end                       #011|  0.00041635     #011|1              #011|  0.00041635     #011|  0.00064481     #011|\u001b[0m\n",
      "\u001b[34mon_train_batch_start               #011|  1.8566e-05     #011|19             #011|  0.00035276     #011|  0.00054632     #011|\u001b[0m\n",
      "\u001b[34mon_validation_batch_start          #011|  5.041e-05      #011|6              #011|  0.00030246     #011|  0.00046842     #011|\u001b[0m\n",
      "\u001b[34mtraining_step_end                  #011|  1.4572e-05     #011|19             #011|  0.00027687     #011|  0.00042879     #011|\u001b[0m\n",
      "\u001b[34mvalidation_step_end                #011|  1.5802e-05     #011|6              #011|  9.4809e-05     #011|  0.00014683     #011|\u001b[0m\n",
      "\u001b[34mon_epoch_start                     #011|  3.1333e-05     #011|3              #011|  9.3999e-05     #011|  0.00014558     #011|\u001b[0m\n",
      "\u001b[34mon_epoch_end                       #011|  2.2299e-05     #011|3              #011|  6.6896e-05     #011|  0.0001036      #011|\u001b[0m\n",
      "\u001b[34mon_validation_epoch_start          #011|  1.7841e-05     #011|2              #011|  3.5682e-05     #011|  5.5262e-05     #011|\u001b[0m\n",
      "\u001b[34mon_fit_start                       #011|  3.3866e-05     #011|1              #011|  3.3866e-05     #011|  5.2449e-05     #011|\u001b[0m\n",
      "\u001b[34mon_val_dataloader                  #011|  2.2516e-05     #011|1              #011|  2.2516e-05     #011|  3.4871e-05     #011|\u001b[0m\n",
      "\u001b[34mon_train_dataloader                #011|  2.0908e-05     #011|1              #011|  2.0908e-05     #011|  3.2381e-05     #011|\u001b[0m\n",
      "\u001b[34mon_before_accelerator_backend_setup#011|  1.8401e-05     #011|1              #011|  1.8401e-05     #011|  2.8498e-05     #011|\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 230, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 230, in _feed\n",
      "    close()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\u001b[0m\n",
      "\u001b[34mOSError: [Errno 9] Bad file descriptor\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\u001b[0m\n",
      "\u001b[34mOSError: [Errno 9] Bad file descriptor\u001b[0m\n",
      "\n",
      "2022-12-16 07:57:33 Uploading - Uploading generated training model\n",
      "2022-12-16 08:02:34 Completed - Training job completed\n",
      "ProfilerReport-1671176913: NoIssuesFound\n",
      "Training seconds: 704\n",
      "Billable seconds: 704\n"
     ]
    }
   ],
   "source": [
    "# Fit the estimator\n",
    "estimator.fit(fit_arguments, job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a5ddf8e25d962f331e8059973cfd97c5aef9d0ccfdd243943e9f1f512e91043"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
