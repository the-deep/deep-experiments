{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-06T10:09:49.697692Z",
     "iopub.status.busy": "2021-07-06T10:09:49.697042Z",
     "iopub.status.idle": "2021-07-06T10:09:49.703994Z",
     "shell.execute_reply": "2021-07-06T10:09:49.703522Z",
     "shell.execute_reply.started": "2021-07-06T10:09:49.697577Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from typing import Any, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:49:30.843642Z",
     "start_time": "2021-06-01T14:49:30.663973Z"
    },
    "execution": {
     "iopub.execute_input": "2021-07-06T10:09:49.704970Z",
     "iopub.status.busy": "2021-07-06T10:09:49.704844Z",
     "iopub.status.idle": "2021-07-06T10:09:52.266844Z",
     "shell.execute_reply": "2021-07-06T10:09:52.266447Z",
     "shell.execute_reply.started": "2021-07-06T10:09:49.704953Z"
    }
   },
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.functional import accuracy, f1, auroc\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.core.decorators import auto_move_data\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from matplotlib import rc\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from transformers.optimization import (\n",
    "    Adafactor,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local constants, regarding the data, MLFlow server, paths, etc..: use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-06T10:09:52.267823Z",
     "iopub.status.busy": "2021-07-06T10:09:52.267728Z",
     "iopub.status.idle": "2021-07-06T10:09:52.356581Z",
     "shell.execute_reply": "2021-07-06T10:09:52.356216Z",
     "shell.execute_reply.started": "2021-07-06T10:09:52.267810Z"
    }
   },
   "outputs": [],
   "source": [
    "from deep.constants import *\n",
    "from deep.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-06T10:09:52.357547Z",
     "iopub.status.busy": "2021-07-06T10:09:52.357436Z",
     "iopub.status.idle": "2021-07-06T10:09:52.387209Z",
     "shell.execute_reply": "2021-07-06T10:09:52.386828Z",
     "shell.execute_reply.started": "2021-07-06T10:09:52.357534Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging and styling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:49:31.657777Z",
     "start_time": "2021-06-01T14:49:31.631040Z"
    },
    "execution": {
     "iopub.execute_input": "2021-07-06T10:09:52.387873Z",
     "iopub.status.busy": "2021-07-06T10:09:52.387784Z",
     "iopub.status.idle": "2021-07-06T10:09:52.420117Z",
     "shell.execute_reply": "2021-07-06T10:09:52.419753Z",
     "shell.execute_reply.started": "2021-07-06T10:09:52.387861Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:49:32.921745Z",
     "start_time": "2021-06-01T14:49:32.910873Z"
    },
    "execution": {
     "iopub.execute_input": "2021-07-06T10:09:52.421631Z",
     "iopub.status.busy": "2021-07-06T10:09:52.421520Z",
     "iopub.status.idle": "2021-07-06T10:09:52.452037Z",
     "shell.execute_reply": "2021-07-06T10:09:52.451519Z",
     "shell.execute_reply.started": "2021-07-06T10:09:52.421618Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:49:41.389959Z",
     "start_time": "2021-06-01T14:49:41.387543Z"
    },
    "execution": {
     "iopub.execute_input": "2021-07-06T10:09:52.453520Z",
     "iopub.status.busy": "2021-07-06T10:09:52.453351Z",
     "iopub.status.idle": "2021-07-06T10:09:52.484466Z",
     "shell.execute_reply": "2021-07-06T10:09:52.484073Z",
     "shell.execute_reply.started": "2021-07-06T10:09:52.453505Z"
    }
   },
   "outputs": [],
   "source": [
    "ic.configureOutput(outputFunction=sys.stdout.write, includeContext=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:57:02.359181Z",
     "start_time": "2021-06-01T14:57:02.353630Z"
    },
    "execution": {
     "iopub.execute_input": "2021-07-06T10:09:52.485097Z",
     "iopub.status.busy": "2021-07-06T10:09:52.485007Z",
     "iopub.status.idle": "2021-07-06T10:09:52.511531Z",
     "shell.execute_reply": "2021-07-06T10:09:52.511182Z",
     "shell.execute_reply.started": "2021-07-06T10:09:52.485084Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:49:35.745930Z",
     "start_time": "2021-06-01T14:49:35.741002Z"
    },
    "execution": {
     "iopub.execute_input": "2021-07-06T10:09:52.512154Z",
     "iopub.status.busy": "2021-07-06T10:09:52.512058Z",
     "iopub.status.idle": "2021-07-06T10:09:52.542386Z",
     "shell.execute_reply": "2021-07-06T10:09:52.542051Z",
     "shell.execute_reply.started": "2021-07-06T10:09:52.512141Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED=2021\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:57:29.882333Z",
     "start_time": "2021-06-01T14:57:28.547379Z"
    },
    "execution": {
     "iopub.execute_input": "2021-07-06T10:09:52.543181Z",
     "iopub.status.busy": "2021-07-06T10:09:52.543080Z",
     "iopub.status.idle": "2021-07-06T10:09:54.337519Z",
     "shell.execute_reply": "2021-07-06T10:09:54.337117Z",
     "shell.execute_reply.started": "2021-07-06T10:09:52.543167Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(LATEST_DATA_PATH / \"data_v0.5_train.csv\")\n",
    "val_dataset = pd.read_csv(LATEST_DATA_PATH / \"data_v0.5_val.csv\")\n",
    "##\n",
    "train_dataset[\"sectors\"] = train_dataset[\"sectors\"].apply(literal_eval)\n",
    "val_dataset[\"sectors\"] = val_dataset[\"sectors\"].apply(literal_eval)\n",
    "##\n",
    "sector_set = set()\n",
    "for sectors_i in train_dataset[\"sectors\"]:\n",
    "    sector_set.update(sectors_i)\n",
    "sectorname_to_sectorid = {sector:i for i, sector in enumerate(list(sorted(sector_set)))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T15:42:32.024647Z",
     "start_time": "2021-05-27T15:42:31.984694Z"
    }
   },
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:29:20.899415Z",
     "start_time": "2021-06-09T08:29:19.327852Z"
    },
    "execution": {
     "iopub.execute_input": "2021-07-06T10:09:54.338285Z",
     "iopub.status.busy": "2021-07-06T10:09:54.338194Z",
     "iopub.status.idle": "2021-07-06T10:09:54.430784Z",
     "shell.execute_reply": "2021-07-06T10:09:54.430409Z",
     "shell.execute_reply.started": "2021-07-06T10:09:54.338272Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session(default_bucket=DEV_BUCKET.name)\n",
    "role = SAGEMAKER_ROLE\n",
    "role_arn = 'arn:aws:iam::961104659532:role/service-role/AmazonSageMaker-ExecutionRole-20210519T102514'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:43.201910Z",
     "start_time": "2021-06-09T08:29:28.837139Z"
    },
    "execution": {
     "iopub.execute_input": "2021-07-06T10:09:54.431537Z",
     "iopub.status.busy": "2021-07-06T10:09:54.431429Z",
     "iopub.status.idle": "2021-07-06T10:09:59.381620Z",
     "shell.execute_reply": "2021-07-06T10:09:59.380072Z",
     "shell.execute_reply.started": "2021-07-06T10:09:54.431523Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = True\n",
    "\n",
    "if sample:\n",
    "    train_dataset = train_dataset.sample(100)\n",
    "    val_dataset = val_dataset.sample(100)\n",
    "    \n",
    "job_name = f\"pytorch-{formatted_time()}-test\"\n",
    "input_path = DEV_BUCKET / 'training' / 'input_data' / job_name\n",
    "\n",
    "train_path = str(input_path / 'train.pickle')\n",
    "val_path = str(input_path / 'val.pickle')\n",
    "\n",
    "\n",
    "train_dataset.to_pickle(train_path, protocol=4)\n",
    "val_dataset.to_pickle(val_path, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:43.284096Z",
     "start_time": "2021-06-09T08:31:43.206457Z"
    },
    "execution": {
     "iopub.execute_input": "2021-07-06T10:09:59.385724Z",
     "iopub.status.busy": "2021-07-06T10:09:59.385329Z",
     "iopub.status.idle": "2021-07-06T10:09:59.456086Z",
     "shell.execute_reply": "2021-07-06T10:09:59.455538Z",
     "shell.execute_reply.started": "2021-07-06T10:09:59.385683Z"
    }
   },
   "outputs": [],
   "source": [
    "instances = [\n",
    "    'ml.p2.xlarge',\n",
    "    'ml.p3.2xlarge'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:43.303558Z",
     "start_time": "2021-06-09T08:31:43.285723Z"
    },
    "execution": {
     "iopub.execute_input": "2021-07-06T10:09:59.457455Z",
     "iopub.status.busy": "2021-07-06T10:09:59.457331Z",
     "iopub.status.idle": "2021-07-06T10:09:59.487018Z",
     "shell.execute_reply": "2021-07-06T10:09:59.486661Z",
     "shell.execute_reply.started": "2021-07-06T10:09:59.457442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S3Path('s3://sagemaker-deep-experiments-dev/training/input_data/pytorch-2021-07-06-12-09-54-485-test')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:43.458886Z",
     "start_time": "2021-06-09T08:31:43.304626Z"
    },
    "execution": {
     "iopub.execute_input": "2021-07-06T10:09:59.487807Z",
     "iopub.status.busy": "2021-07-06T10:09:59.487702Z",
     "iopub.status.idle": "2021-07-06T10:09:59.656173Z",
     "shell.execute_reply": "2021-07-06T10:09:59.655797Z",
     "shell.execute_reply.started": "2021-07-06T10:09:59.487794Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "hyperparameters={\n",
    "    'tracking_uri': MLFLOW_SERVER,\n",
    "    'experiment_name': 'pl_test',\n",
    "    'max_len': 200,\n",
    "    'epochs': 1,\n",
    "    'train_batch_size': 16,\n",
    "    'eval_batch_size': 16,\n",
    "    'model_name': 'sentence-transformers/paraphrase-mpnet-base-v2',\n",
    "    'classes': str(SECTORS)\n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    source_dir=str(SCRIPTS_EXAMPLES_PATH / 'sector-pl'),\n",
    "    output_path=str(DEV_BUCKET / 'models/'),\n",
    "    code_location=str(input_path),\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    framework_version='1.8',\n",
    "    py_version='py36',\n",
    "    hyperparameters = hyperparameters,\n",
    "    job_name=job_name,\n",
    "#     train_instance_count=2,\n",
    "#     train_instance_type=\"ml.c4.xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:43.482969Z",
     "start_time": "2021-06-09T08:31:43.459884Z"
    },
    "execution": {
     "iopub.execute_input": "2021-07-06T10:09:59.656875Z",
     "iopub.status.busy": "2021-07-06T10:09:59.656776Z",
     "iopub.status.idle": "2021-07-06T10:09:59.685169Z",
     "shell.execute_reply": "2021-07-06T10:09:59.684821Z",
     "shell.execute_reply.started": "2021-07-06T10:09:59.656862Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_arguments = {\n",
    "    'train': str(input_path),\n",
    "    'test': str(input_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:45.995868Z",
     "start_time": "2021-06-09T08:31:43.484212Z"
    },
    "execution": {
     "iopub.execute_input": "2021-07-06T10:09:59.685953Z",
     "iopub.status.busy": "2021-07-06T10:09:59.685849Z",
     "iopub.status.idle": "2021-07-06T10:18:33.556255Z",
     "shell.execute_reply": "2021-07-06T10:18:33.554914Z",
     "shell.execute_reply.started": "2021-07-06T10:09:59.685940Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-06 10:10:02 Starting - Starting the training job...\n",
      "2021-07-06 10:10:04 Starting - Launching requested ML instancesProfilerReport-1625566199: InProgress\n",
      "......\n",
      "2021-07-06 10:11:29 Starting - Preparing the instances for training......\n",
      "2021-07-06 10:12:39 Downloading - Downloading input data...\n",
      "2021-07-06 10:13:09 Training - Downloading the training image....................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-07-06 10:16:42,167 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-07-06 10:16:42,193 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-07-06 10:16:48,437 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-07-06 10:16:48,761 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting mlflow==1.18.0\n",
      "  Downloading mlflow-1.18.0-py3-none-any.whl (14.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.6.1\n",
      "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting pytorch-lightning==1.3.3\n",
      "  Downloading pytorch_lightning-1.3.3-py3-none-any.whl (806 kB)\u001b[0m\n",
      "\u001b[34mCollecting torchmetrics==0.3.2\n",
      "  Downloading torchmetrics-0.3.2-py3-none-any.whl (274 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitpython>=2.1.0\n",
      "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from mlflow==1.18.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from mlflow==1.18.0->-r requirements.txt (line 1)) (2021.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from mlflow==1.18.0->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from mlflow==1.18.0->-r requirements.txt (line 1)) (1.1.5)\u001b[0m\n",
      "\u001b[34mCollecting sqlparse>=0.3.1\n",
      "  Downloading sqlparse-0.4.1-py3-none-any.whl (42 kB)\u001b[0m\n",
      "\u001b[34mCollecting Flask\n",
      "  Downloading Flask-2.0.1-py3-none-any.whl (94 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.17.3 in /opt/conda/lib/python3.6/site-packages (from mlflow==1.18.0->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.7.0 in /opt/conda/lib/python3.6/site-packages (from mlflow==1.18.0->-r requirements.txt (line 1)) (3.17.1)\u001b[0m\n",
      "\u001b[34mCollecting sqlalchemy\n",
      "  Downloading SQLAlchemy-1.4.20-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting entrypoints\n",
      "  Downloading entrypoints-0.3-py2.py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting alembic<=1.4.1\n",
      "  Downloading alembic-1.4.1.tar.gz (1.1 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from mlflow==1.18.0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.6/site-packages (from mlflow==1.18.0->-r requirements.txt (line 1)) (1.6.0)\u001b[0m\n",
      "\u001b[34mCollecting docker>=4.0.0\n",
      "  Downloading docker-5.0.0-py2.py3-none-any.whl (146 kB)\u001b[0m\n",
      "\u001b[34mCollecting prometheus-flask-exporter\n",
      "  Downloading prometheus_flask_exporter-0.18.2.tar.gz (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting querystring-parser\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting databricks-cli>=0.8.7\n",
      "  Downloading databricks-cli-0.14.3.tar.gz (54 kB)\u001b[0m\n",
      "\u001b[34mCollecting gunicorn\n",
      "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.6/site-packages (from mlflow==1.18.0->-r requirements.txt (line 1)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (4.4.0)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.7.6-cp36-cp36m-manylinux2014_x86_64.whl (722 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.6.1->-r requirements.txt (line 2)) (4.51.0)\u001b[0m\n",
      "\u001b[34mCollecting pyDeprecate==0.3.0\n",
      "  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==1.3.3->-r requirements.txt (line 3)) (0.18.2)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard!=2.5.0,>=2.2.0\n",
      "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.4.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==1.3.3->-r requirements.txt (line 3)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==1.3.3->-r requirements.txt (line 3)) (1.8.1)\u001b[0m\n",
      "\u001b[34mCollecting Mako\n",
      "  Downloading Mako-1.1.4-py2.py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34mCollecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.6/site-packages (from alembic<=1.4.1->mlflow==1.18.0->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.6/site-packages (from databricks-cli>=0.8.7->mlflow==1.18.0->-r requirements.txt (line 1)) (0.8.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from databricks-cli>=0.8.7->mlflow==1.18.0->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.6/site-packages (from docker>=4.0.0->mlflow==1.18.0->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp\n",
      "  Downloading aiohttp-3.7.4.post0-cp36-cp36m-manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.0 in /opt/conda/lib/python3.6/site-packages (from gitpython>=2.1.0->mlflow==1.18.0->-r requirements.txt (line 1)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34mCollecting smmap<5,>=3.0.1\n",
      "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.17.3->mlflow==1.18.0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.17.3->mlflow==1.18.0->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.17.3->mlflow==1.18.0->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.17.3->mlflow==1.18.0->-r requirements.txt (line 1)) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.6/site-packages (from sqlalchemy->mlflow==1.18.0->-r requirements.txt (line 1)) (1.1.0)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.38.1-cp36-cp36m-manylinux2014_x86_64.whl (4.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.32.1-py2.py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.3->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.3->-r requirements.txt (line 3)) (49.6.0.post20210108)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.3->-r requirements.txt (line 3)) (0.35.1)\u001b[0m\n",
      "\n",
      "2021-07-06 10:17:10 Training - Training image download completed. Training in progress.\u001b[34mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.3->-r requirements.txt (line 3)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.3->-r requirements.txt (line 3)) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\u001b[0m\n",
      "\u001b[34mCollecting idna-ssl>=1.0\n",
      "  Downloading idna-ssl-1.1.0.tar.gz (3.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<4.0,>=3.0\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.3->-r requirements.txt (line 3)) (21.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.6/site-packages (from Flask->mlflow==1.18.0->-r requirements.txt (line 1)) (3.0.1)\u001b[0m\n",
      "\u001b[34mCollecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.6/site-packages (from Jinja2>=3.0->Flask->mlflow==1.18.0->-r requirements.txt (line 1)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->mlflow==1.18.0->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mCollecting prometheus_client\n",
      "  Downloading prometheus_client-0.11.0-py2.py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1->-r requirements.txt (line 2)) (1.0.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: alembic, databricks-cli, idna-ssl, prometheus-flask-exporter\n",
      "  Building wheel for alembic (setup.py): started\n",
      "  Building wheel for alembic (setup.py): finished with status 'done'\n",
      "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158155 sha256=753c4d687250094a59f67c4e182afbd6bfb3bfe7364ba1d9a0f3b46e72e16ba7\n",
      "  Stored in directory: /root/.cache/pip/wheels/e9/7b/aa/e18c983d8236b141f85838ba0f8e4e4ae9bcf7f1e00ff726ec\n",
      "  Building wheel for databricks-cli (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
      "  Created wheel for databricks-cli: filename=databricks_cli-0.14.3-py3-none-any.whl size=100556 sha256=8a39cff53da8809157fc84ffe693b410dbe3ee5e86f7c8cca3ca3c9149490901\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/88/95/bd32d0e2dc0cf30e55574faab3118df2bb9ebebc60978c147b\n",
      "  Building wheel for idna-ssl (setup.py): started\n",
      "  Building wheel for idna-ssl (setup.py): finished with status 'done'\n",
      "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-py3-none-any.whl size=3161 sha256=80d562c09349aa92217c29b0bf6fbb3fe65c56589df16cfc702d95eaa0c9be58\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/f5/9c/f8331a854f7a8739cf0e74c13854e4dd7b1af11b04fe1dde13\n",
      "  Building wheel for prometheus-flask-exporter (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for prometheus-flask-exporter (setup.py): finished with status 'done'\n",
      "  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.2-py3-none-any.whl size=17398 sha256=d4665afad343ec3e525167f88cb18d000d1a8f37ba70fb34ce22d0dd0ae56b36\n",
      "  Stored in directory: /root/.cache/pip/wheels/15/77/e8/3ca90b66243b0b58d5a5323a3da02cc8c5daf1de7a65141701\u001b[0m\n",
      "\u001b[34mSuccessfully built alembic databricks-cli idna-ssl prometheus-flask-exporter\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pyasn1-modules, oauthlib, multidict, cachetools, yarl, smmap, requests-oauthlib, itsdangerous, idna-ssl, google-auth, async-timeout, tensorboard-plugin-wit, sqlalchemy, regex, python-editor, prometheus-client, markdown, Mako, grpcio, google-auth-oauthlib, gitdb, Flask, filelock, aiohttp, absl-py, torchmetrics, tokenizers, tensorboard, sqlparse, sacremoses, querystring-parser, pyDeprecate, prometheus-flask-exporter, huggingface-hub, gunicorn, gitpython, entrypoints, docker, databricks-cli, alembic, transformers, pytorch-lightning, mlflow\u001b[0m\n",
      "\u001b[34mSuccessfully installed Flask-2.0.1 Mako-1.1.4 absl-py-0.13.0 aiohttp-3.7.4.post0 alembic-1.4.1 async-timeout-3.0.1 cachetools-4.2.2 databricks-cli-0.14.3 docker-5.0.0 entrypoints-0.3 filelock-3.0.12 gitdb-4.0.7 gitpython-3.1.18 google-auth-1.32.1 google-auth-oauthlib-0.4.4 grpcio-1.38.1 gunicorn-20.1.0 huggingface-hub-0.0.8 idna-ssl-1.1.0 itsdangerous-2.0.1 markdown-3.3.4 mlflow-1.18.0 multidict-5.1.0 oauthlib-3.1.1 prometheus-client-0.11.0 prometheus-flask-exporter-0.18.2 pyDeprecate-0.3.0 pyasn1-modules-0.2.8 python-editor-1.0.4 pytorch-lightning-1.3.3 querystring-parser-1.2.4 regex-2021.7.6 requests-oauthlib-1.3.0 sacremoses-0.0.45 smmap-4.0.0 sqlalchemy-1.4.20 sqlparse-0.4.1 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 tokenizers-0.10.3 torchmetrics-0.3.2 transformers-4.6.1 yarl-1.6.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "\u001b[34m2021-07-06 10:17:10,337 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"experiment_name\": \"pl_test\",\n",
      "        \"classes\": \"['Agriculture', 'Cross', 'Education', 'Food Security', 'Health', 'Livelihoods', 'Logistics', 'Nutrition', 'Protection', 'Shelter', 'WASH']\",\n",
      "        \"eval_batch_size\": 16,\n",
      "        \"max_len\": 200,\n",
      "        \"train_batch_size\": 16,\n",
      "        \"model_name\": \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
      "        \"epochs\": 1,\n",
      "        \"tracking_uri\": \"http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-2021-07-06-12-09-54-485-test\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-deep-experiments-dev/training/input_data/pytorch-2021-07-06-12-09-54-485-test/pytorch-2021-07-06-12-09-54-485-test/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"classes\":\"['Agriculture', 'Cross', 'Education', 'Food Security', 'Health', 'Livelihoods', 'Logistics', 'Nutrition', 'Protection', 'Shelter', 'WASH']\",\"epochs\":1,\"eval_batch_size\":16,\"experiment_name\":\"pl_test\",\"max_len\":200,\"model_name\":\"sentence-transformers/paraphrase-mpnet-base-v2\",\"tracking_uri\":\"http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\",\"train_batch_size\":16}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-deep-experiments-dev/training/input_data/pytorch-2021-07-06-12-09-54-485-test/pytorch-2021-07-06-12-09-54-485-test/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"classes\":\"['Agriculture', 'Cross', 'Education', 'Food Security', 'Health', 'Livelihoods', 'Logistics', 'Nutrition', 'Protection', 'Shelter', 'WASH']\",\"epochs\":1,\"eval_batch_size\":16,\"experiment_name\":\"pl_test\",\"max_len\":200,\"model_name\":\"sentence-transformers/paraphrase-mpnet-base-v2\",\"tracking_uri\":\"http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\",\"train_batch_size\":16},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-2021-07-06-12-09-54-485-test\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-deep-experiments-dev/training/input_data/pytorch-2021-07-06-12-09-54-485-test/pytorch-2021-07-06-12-09-54-485-test/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--classes\",\"['Agriculture', 'Cross', 'Education', 'Food Security', 'Health', 'Livelihoods', 'Logistics', 'Nutrition', 'Protection', 'Shelter', 'WASH']\",\"--epochs\",\"1\",\"--eval_batch_size\",\"16\",\"--experiment_name\",\"pl_test\",\"--max_len\",\"200\",\"--model_name\",\"sentence-transformers/paraphrase-mpnet-base-v2\",\"--tracking_uri\",\"http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\",\"--train_batch_size\",\"16\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EXPERIMENT_NAME=pl_test\u001b[0m\n",
      "\u001b[34mSM_HP_CLASSES=['Agriculture', 'Cross', 'Education', 'Food Security', 'Health', 'Livelihoods', 'Logistics', 'Nutrition', 'Protection', 'Shelter', 'WASH']\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_LEN=200\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=sentence-transformers/paraphrase-mpnet-base-v2\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_TRACKING_URI=http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --classes ['Agriculture', 'Cross', 'Education', 'Food Security', 'Health', 'Livelihoods', 'Logistics', 'Nutrition', 'Protection', 'Shelter', 'WASH'] --epochs 1 --eval_batch_size 16 --experiment_name pl_test --max_len 200 --model_name sentence-transformers/paraphrase-mpnet-base-v2 --tracking_uri http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/ --train_batch_size 16\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m#015Validation sanity check: 0it [00:00, ?it/s]#015Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s][2021-07-06 10:17:29.572 algo-1:55 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.640 algo-1:55 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.640 algo-1:55 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.640 algo-1:55 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.642 algo-1:55 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.642 algo-1:55 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.666 algo-1:55 INFO hook.py:591] name:model.l1.embeddings.word_embeddings.weight count_params:23444736\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.666 algo-1:55 INFO hook.py:591] name:model.l1.embeddings.position_embeddings.weight count_params:394752\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.666 algo-1:55 INFO hook.py:591] name:model.l1.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.666 algo-1:55 INFO hook.py:591] name:model.l1.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.666 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.0.attention.attn.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.666 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.0.attention.attn.q.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.666 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.0.attention.attn.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.667 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.0.attention.attn.k.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.667 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.0.attention.attn.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.667 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.0.attention.attn.v.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.667 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.0.attention.attn.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.667 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.0.attention.attn.o.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.667 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.0.attention.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.667 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.0.attention.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.667 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.667 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.667 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.667 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.668 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.668 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.668 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.1.attention.attn.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.668 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.1.attention.attn.q.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.668 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.1.attention.attn.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.668 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.1.attention.attn.k.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.668 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.1.attention.attn.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.668 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.1.attention.attn.v.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.668 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.1.attention.attn.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.668 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.1.attention.attn.o.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.668 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.1.attention.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.669 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.1.attention.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.669 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.669 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.669 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.669 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.669 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.669 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.669 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.2.attention.attn.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.669 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.2.attention.attn.q.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.669 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.2.attention.attn.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.670 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.2.attention.attn.k.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.670 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.2.attention.attn.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.670 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.2.attention.attn.v.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.670 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.2.attention.attn.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.670 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.2.attention.attn.o.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.670 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.2.attention.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.670 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.2.attention.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.670 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.670 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.670 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.670 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.671 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.671 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.671 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.3.attention.attn.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.671 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.3.attention.attn.q.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.671 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.3.attention.attn.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.671 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.3.attention.attn.k.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.671 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.3.attention.attn.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.671 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.3.attention.attn.v.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.671 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.3.attention.attn.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.671 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.3.attention.attn.o.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.671 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.3.attention.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.672 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.3.attention.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.672 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.672 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.672 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.672 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.672 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.672 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.672 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.4.attention.attn.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.672 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.4.attention.attn.q.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.672 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.4.attention.attn.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.672 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.4.attention.attn.k.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.673 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.4.attention.attn.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.673 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.4.attention.attn.v.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.673 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.4.attention.attn.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.673 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.4.attention.attn.o.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.673 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.4.attention.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.673 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.4.attention.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.673 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.673 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.673 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.673 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.673 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.674 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.674 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.5.attention.attn.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.674 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.5.attention.attn.q.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.674 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.5.attention.attn.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.674 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.5.attention.attn.k.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.674 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.5.attention.attn.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.674 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.5.attention.attn.v.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.674 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.5.attention.attn.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.674 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.5.attention.attn.o.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.674 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.5.attention.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.674 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.5.attention.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.674 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.675 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.675 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.675 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.675 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.675 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.675 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.6.attention.attn.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.675 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.6.attention.attn.q.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.675 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.6.attention.attn.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.675 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.6.attention.attn.k.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.675 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.6.attention.attn.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.675 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.6.attention.attn.v.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.676 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.6.attention.attn.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.676 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.6.attention.attn.o.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.676 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.6.attention.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.676 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.6.attention.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.676 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.676 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.676 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.676 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.676 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.676 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.676 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.7.attention.attn.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.676 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.7.attention.attn.q.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.676 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.7.attention.attn.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.677 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.7.attention.attn.k.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.677 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.7.attention.attn.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.677 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.7.attention.attn.v.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.677 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.7.attention.attn.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.677 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.7.attention.attn.o.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.677 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.7.attention.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.677 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.7.attention.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.677 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.677 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.677 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.677 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.677 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.677 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.678 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.8.attention.attn.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.678 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.8.attention.attn.q.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.678 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.8.attention.attn.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.678 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.8.attention.attn.k.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.678 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.8.attention.attn.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.678 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.8.attention.attn.v.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.678 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.8.attention.attn.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.678 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.8.attention.attn.o.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.678 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.8.attention.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.678 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.8.attention.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.678 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.679 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.679 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.679 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.679 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.679 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.679 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.9.attention.attn.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.679 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.9.attention.attn.q.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.679 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.9.attention.attn.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.679 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.9.attention.attn.k.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.679 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.9.attention.attn.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.680 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.9.attention.attn.v.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.680 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.9.attention.attn.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.680 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.9.attention.attn.o.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.680 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.9.attention.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.680 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.9.attention.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.680 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.680 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.680 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.680 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.680 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.681 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.681 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.10.attention.attn.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.681 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.10.attention.attn.q.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.681 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.10.attention.attn.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.681 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.10.attention.attn.k.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.681 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.10.attention.attn.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.681 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.10.attention.attn.v.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.681 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.10.attention.attn.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.681 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.10.attention.attn.o.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.681 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.10.attention.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.681 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.10.attention.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.682 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.682 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.682 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.682 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.682 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.682 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.682 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.11.attention.attn.q.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.682 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.11.attention.attn.q.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.682 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.11.attention.attn.k.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.682 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.11.attention.attn.k.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.682 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.11.attention.attn.v.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.683 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.11.attention.attn.v.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.683 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.11.attention.attn.o.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.683 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.11.attention.attn.o.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.683 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.11.attention.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.683 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.11.attention.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.683 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.683 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.683 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.683 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.683 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.683 algo-1:55 INFO hook.py:591] name:model.l1.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.684 algo-1:55 INFO hook.py:591] name:model.l1.encoder.relative_attention_bias.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.684 algo-1:55 INFO hook.py:591] name:model.l1.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.684 algo-1:55 INFO hook.py:591] name:model.l1.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.684 algo-1:55 INFO hook.py:591] name:model.l3.weight count_params:8448\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.684 algo-1:55 INFO hook.py:591] name:model.l3.bias count_params:11\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.684 algo-1:55 INFO hook.py:593] Total Trainable Params: 109494923\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.684 algo-1:55 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-07-06 10:17:29.687 algo-1:55 INFO hook.py:488] Hook is writing from the hook with pid: 55\n",
      "\u001b[0m\n",
      "\u001b[34m#015Validation sanity check:  50%|     | 1/2 [00:03<00:03,  3.17s/it]#015                                                                      #015#015Training: 0it [00:00, ?it/s]#015Training:   0%|          | 0/14 [00:00<?, ?it/s]#015Epoch 0:   0%|          | 0/14 [00:00<?, ?it/s] #015Epoch 0:   7%|         | 1/14 [00:00<00:03,  4.03it/s]#015Epoch 0:   7%|         | 1/14 [00:00<00:03,  4.02it/s, loss=0.691, v_num=0, val_f1_epoch=0.469, val_loss_epoch=0.680]#015Epoch 0:  14%|        | 2/14 [00:00<00:02,  4.44it/s, loss=0.691, v_num=0, val_f1_epoch=0.469, val_loss_epoch=0.680]#015Epoch 0:  14%|        | 2/14 [00:00<00:02,  4.43it/s, loss=0.675, v_num=0, val_f1_epoch=0.469, val_loss_epoch=0.680, train_f1=0.380]#015Epoch 0:  21%|       | 3/14 [00:00<00:02,  4.61it/s, loss=0.675, v_num=0, val_f1_epoch=0.469, val_loss_epoch=0.680, train_f1=0.380]#015Epoch 0:  21%|       | 3/14 [00:00<00:02,  4.61it/s, loss=0.664, v_num=0, val_f1_epoch=0.469, val_loss_epoch=0.680, train_f1=0.535]#015Epoch 0:  29%|       | 4/14 [00:00<00:02,  4.71it/s, loss=0.664, v_num=0, val_f1_epoch=0.469, val_loss_epoch=0.680, train_f1=0.535]#015Epoch 0:  29%|       | 4/14 [00:00<00:02,  4.71it/s, loss=0.653, v_num=0, val_f1_epoch=0.469, val_loss_epoch=0.680, train_f1=0.483]#015Epoch 0:  36%|      | 5/14 [00:01<00:01,  4.77it/s, loss=0.653, v_num=0, val_f1_epoch=0.469, val_loss_epoch=0.680, train_f1=0.483]#015Epoch 0:  36%|      | 5/14 [00:01<00:01,  4.76it/s, loss=0.644, v_num=0, val_f1_epoch=0.469, val_loss_epoch=0.680, train_f1=0.517]#015Epoch 0:  43%|     | 6/14 [00:01<00:01,  4.83it/s, loss=0.644, v_num=0, val_f1_epoch=0.469, val_loss_epoch=0.680, train_f1=0.517]#015Epoch 0:  43%|     | 6/14 [00:01<00:01,  4.83it/s, loss=0.637, v_num=0, val_f1_epoch=0.469, val_loss_epoch=0.680, train_f1=0.507]#015Epoch 0:  50%|     | 7/14 [00:01<00:01,  5.24it/s, loss=0.628, v_num=0, val_f1_epoch=0.469, val_loss_epoch=0.680, train_f1=0.503]#015Epoch 0:  57%|    | 8/14 [00:01<00:01,  5.95it/s, loss=0.628, v_num=0, val_f1_epoch=0.469, val_loss_epoch=0.680, train_f1=0.503]\u001b[0m\n",
      "\u001b[34m#015Validating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:   0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  29%|       | 2/7 [00:00<00:00, 14.21it/s]#033[A#015Epoch 0:  71%|  | 10/14 [00:01<00:00,  6.73it/s, loss=0.628, v_num=0, val_f1_epoch=0.469, val_loss_epoch=0.680, train_f1=0.503]\u001b[0m\n",
      "\u001b[34m#015Validating:  57%|    | 4/7 [00:00<00:00, 13.76it/s]#033[A#015Epoch 0:  86%| | 12/14 [00:01<00:00,  7.31it/s, loss=0.628, v_num=0, val_f1_epoch=0.469, val_loss_epoch=0.680, train_f1=0.503]\u001b[0m\n",
      "\u001b[34m#015Validating:  86%| | 6/7 [00:00<00:00, 13.76it/s]#033[A#015Epoch 0: 100%|| 14/14 [00:01<00:00,  7.83it/s, loss=0.628, v_num=0, val_f1_epoch=0.469, val_loss_epoch=0.680, train_f1=0.503]#015Epoch 0: 100%|| 14/14 [00:02<00:00,  6.88it/s, loss=0.628, v_num=0, val_f1_epoch=0.474, val_loss_epoch=0.568, train_f1=0.476, val_f1_step=0.488, val_loss_step=0.549]\u001b[0m\n",
      "\u001b[34m#015                                                         #033[A#015Epoch 0: 100%|| 14/14 [00:05<00:00,  2.76it/s, loss=0.628, v_num=0, val_f1_epoch=0.474, val_loss_epoch=0.568, train_f1=0.476, val_f1_step=0.488, val_loss_step=0.549]\u001b[0m\n",
      "\u001b[34mINFO:root:reading data\u001b[0m\n",
      "\u001b[34mINFO:root:building training and testing datasets\u001b[0m\n",
      "\u001b[34mINFO:filelock:Lock 139887761565904 acquired on /root/.cache/huggingface/transformers/96eb1c084a276166de559d1ad85205eb6ba5dd929bb90f41af67e0d9d7a0c6fd.bbbef2c70c87c42bb5499099ec8bd707180a1fa6a72cbd40f6f23649d8bd6d8c.lock\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/594 [00:00<?, ?B/s]#015Downloading: 100%|| 594/594 [00:00<00:00, 1.01MB/s]\u001b[0m\n",
      "\u001b[34mINFO:filelock:Lock 139887761565904 released on /root/.cache/huggingface/transformers/96eb1c084a276166de559d1ad85205eb6ba5dd929bb90f41af67e0d9d7a0c6fd.bbbef2c70c87c42bb5499099ec8bd707180a1fa6a72cbd40f6f23649d8bd6d8c.lock\u001b[0m\n",
      "\u001b[34mINFO:filelock:Lock 139884616541968 acquired on /root/.cache/huggingface/transformers/d48c7a894f2f0438eb62fcb8884f11ff35ea7e1e44ca0274b6ae0a288fca285b.98b26f9c960899aa0e99c10a12750104e467743b3b460b79fa7d76907549319b.lock\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading: 100%|| 232k/232k [00:00<00:00, 46.5MB/s]\u001b[0m\n",
      "\u001b[34mINFO:filelock:Lock 139884616541968 released on /root/.cache/huggingface/transformers/d48c7a894f2f0438eb62fcb8884f11ff35ea7e1e44ca0274b6ae0a288fca285b.98b26f9c960899aa0e99c10a12750104e467743b3b460b79fa7d76907549319b.lock\u001b[0m\n",
      "\u001b[34mINFO:filelock:Lock 139884616542584 acquired on /root/.cache/huggingface/transformers/ae71b00a0e284b08264509b65ada716ffa2872f3063c4c939b59959e62d4ab75.4e84197f9f04666b541217ec3e1bd301b154ec9bfa8b968b3f9185b34bd73da6.lock\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]#015Downloading: 100%|| 466k/466k [00:00<00:00, 48.1MB/s]\u001b[0m\n",
      "\u001b[34mINFO:filelock:Lock 139884616542584 released on /root/.cache/huggingface/transformers/ae71b00a0e284b08264509b65ada716ffa2872f3063c4c939b59959e62d4ab75.4e84197f9f04666b541217ec3e1bd301b154ec9bfa8b968b3f9185b34bd73da6.lock\u001b[0m\n",
      "\u001b[34mINFO:filelock:Lock 139884616544096 acquired on /root/.cache/huggingface/transformers/e341629aba7afac8fd72ecdea6343b69a33eb5bb76ced6b288dbe1e666c35c47.18ebceb237d999d8f1cb15935e35b314f3e73dd6c4f65e119f4790fa226c9236.lock\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]#015Downloading: 100%|| 239/239 [00:00<00:00, 416kB/s]\u001b[0m\n",
      "\u001b[34mINFO:filelock:Lock 139884616544096 released on /root/.cache/huggingface/transformers/e341629aba7afac8fd72ecdea6343b69a33eb5bb76ced6b288dbe1e666c35c47.18ebceb237d999d8f1cb15935e35b314f3e73dd6c4f65e119f4790fa226c9236.lock\u001b[0m\n",
      "\u001b[34mINFO:filelock:Lock 139884616540288 acquired on /root/.cache/huggingface/transformers/8ceef1c800b05474d2b470c6dac1644ac368e78b997569934c1d9e48989fa09c.9777c1408575043925e0a5a63c1144e4c19e67cc033b9c85069a11e52ea4bc2d.lock\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/1.19k [00:00<?, ?B/s]#015Downloading: 100%|| 1.19k/1.19k [00:00<00:00, 1.99MB/s]\u001b[0m\n",
      "\u001b[34mINFO:filelock:Lock 139884616540288 released on /root/.cache/huggingface/transformers/8ceef1c800b05474d2b470c6dac1644ac368e78b997569934c1d9e48989fa09c.9777c1408575043925e0a5a63c1144e4c19e67cc033b9c85069a11e52ea4bc2d.lock\u001b[0m\n",
      "\u001b[34mINFO:root:training model\u001b[0m\n",
      "\u001b[34mGPU available: True, used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mINFO:filelock:Lock 139884606192496 acquired on /root/.cache/huggingface/transformers/992e9e88844a23031919226d183d2dec1973bb93482dd767713146d989b9972e.aabd9267f2a2af6ec34667d063ffbcacdfd7da949d6c2f6e145f113dacce0263.lock\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]#015Downloading:   1%|          | 5.06M/438M [00:00<00:08, 50.6MB/s]#015Downloading:   2%|         | 10.2M/438M [00:00<00:08, 51.0MB/s]#015Downloading:   4%|         | 15.5M/438M [00:00<00:08, 51.4MB/s]#015Downloading:   5%|         | 20.9M/438M [00:00<00:08, 52.1MB/s]#015Downloading:   6%|         | 26.3M/438M [00:00<00:07, 52.6MB/s]#015Downloading:   7%|         | 31.7M/438M [00:00<00:07, 53.2MB/s]#015Downloading:   9%|         | 37.3M/438M [00:00<00:07, 53.9MB/s]#015Downloading:  10%|         | 42.9M/438M [00:00<00:07, 54.5MB/s]#015Downloading:  11%|         | 48.2M/438M [00:00<00:07, 54.1MB/s]#015Downloading:  12%|        | 53.7M/438M [00:01<00:07, 54.4MB/s]#015Downloading:  14%|        | 59.3M/438M [00:01<00:06, 54.7MB/s]#015Downloading:  15%|        | 64.9M/438M [00:01<00:06, 55.2MB/s]#015Downloading:  16%|        | 70.6M/438M [00:01<00:06, 55.7MB/s]#015Downloading:  17%|        | 76.3M/438M [00:01<00:06, 56.1MB/s]#015Downloading:  19%|        | 82.0M/438M [00:01<00:06, 56.5MB/s]#015Downloading:  20%|        | 87.7M/438M [00:01<00:06, 54.5MB/s]#015Downloading:  21%|       | 93.1M/438M [00:01<00:06, 53.5MB/s]#015Downloading:  22%|       | 98.5M/438M [00:01<00:06, 52.0MB/s]#015Downloading:  24%|       | 104M/438M [00:01<00:06, 51.7MB/s] #015Downloading:  25%|       | 109M/438M [00:02<00:06, 51.3MB/s]#015Downloading:  26%|       | 114M/438M [00:02<00:06, 50.9MB/s]#015Downloading:  27%|       | 120M/438M [00:02<00:06, 52.9MB/s]#015Downloading:  29%|       | 126M/438M [00:02<00:05, 55.2MB/s]#015Downloading:  30%|       | 132M/438M [00:02<00:05, 56.7MB/s]#015Downloading:  31%|      | 138M/438M [00:02<00:05, 53.8MB/s]#015Downloading:  33%|      | 143M/438M [00:02<00:05, 54.3MB/s]#015Downloading:  34%|      | 149M/438M [00:02<00:05, 56.3MB/s]#015Downloading:  35%|      | 155M/438M [00:02<00:05, 55.6MB/s]#015Downloading:  37%|      | 161M/438M [00:02<00:04, 55.7MB/s]#015Downloading:  38%|      | 167M/438M [00:03<00:04, 57.2MB/s]#015Downloading:  39%|      | 173M/438M [00:03<00:04, 56.8MB/s]#015Downloading:  41%|      | 178M/438M [00:03<00:04, 56.8MB/s]#015Downloading:  42%|     | 184M/438M [00:03<00:04, 57.0MB/s]#015Downloading:  43%|     | 190M/438M [00:03<00:04, 58.3MB/s]#015Downloading:  45%|     | 196M/438M [00:03<00:04, 59.0MB/s]#015Downloading:  46%|     | 202M/438M [00:03<00:04, 58.3MB/s]#015Downloading:  47%|     | 208M/438M [00:03<00:03, 57.6MB/s]#015Downloading:  49%|     | 214M/438M [00:03<00:04, 54.7MB/s]#015Downloading:  50%|     | 220M/438M [00:03<00:03, 56.6MB/s]#015Downloading:  52%|    | 226M/438M [00:04<00:03, 53.5MB/s]#015Downloading:  53%|    | 231M/438M [00:04<00:03, 54.6MB/s]#015Downloading:  54%|    | 238M/438M [00:04<00:03, 56.4MB/s]#015Downloading:  56%|    | 243M/438M [00:04<00:03, 56.6MB/s]#015Downloading:  57%|    | 249M/438M [00:04<00:03, 56.8MB/s]#015Downloading:  58%|    | 255M/438M [00:04<00:03, 57.1MB/s]#015Downloading:  59%|    | 260M/438M [00:04<00:03, 57.1MB/s]#015Downloading:  61%|    | 266M/438M [00:04<00:03, 57.2MB/s]#015Downloading:  62%|   | 272M/438M [00:04<00:03, 55.3MB/s]#015Downloading:  63%|   | 278M/438M [00:05<00:02, 56.9MB/s]#015Downloading:  65%|   | 284M/438M [00:05<00:02, 56.8MB/s]#015Downloading:  66%|   | 290M/438M [00:05<00:02, 57.8MB/s]#015Downloading:  68%|   | 296M/438M [00:05<00:02, 59.1MB/s]#015Downloading:  69%|   | 302M/438M [00:05<00:02, 57.0MB/s]#015Downloading:  70%|   | 308M/438M [00:05<00:02, 57.0MB/s]#015Downloading:  72%|  | 313M/438M [00:05<00:02, 55.8MB/s]#015Downloading:  73%|  | 320M/438M [00:05<00:02, 57.8MB/s]#015Downloading:  74%|  | 326M/438M [00:05<00:01, 59.0MB/s]#015Downloading:  76%|  | 332M/438M [00:05<00:01, 58.9MB/s]#015Downloading:  77%|  | 338M/438M [00:06<00:01, 59.8MB/s]#015Downloading:  79%|  | 344M/438M [00:06<00:01, 60.4MB/s]#015Downloading:  80%|  | 351M/438M [00:06<00:01, 61.0MB/s]#015Downloading:  81%| | 357M/438M [00:06<00:01, 60.1MB/s]#015Downloading:  83%| | 363M/438M [00:06<00:01, 58.8MB/s]#015Downloading:  84%| | 369M/438M [00:06<00:01, 59.9MB/s]#015Downloading:  86%| | 375M/438M [00:06<00:01, 58.8MB/s]#015Downloading:  87%| | 381M/438M [00:06<00:00, 57.4MB/s]#015Downloading:  88%| | 387M/438M [00:06<00:00, 56.6MB/s]#015Downloading:  90%| | 392M/438M [00:07<00:00, 52.5MB/s]#015Downloading:  91%| | 398M/438M [00:07<00:00, 51.6MB/s]#015Downloading:  92%|| 404M/438M [00:07<00:00, 54.2MB/s]#015Downloading:  94%|| 410M/438M [00:07<00:00, 56.0MB/s]#015Downloading:  95%|| 416M/438M [00:07<00:00, 56.3MB/s]#015Downloading:  96%|| 421M/438M [00:07<00:00, 56.3MB/s]#015Downloading:  97%|| 427M/438M [00:07<00:00, 53.8MB/s]#015Downloading:  99%|| 432M/438M [00:07<00:00, 53.9MB/s]#015Downloading: 100%|| 438M/438M [00:07<00:00, 56.0MB/s]\u001b[0m\n",
      "\u001b[34mINFO:filelock:Lock 139884606192496 released on /root/.cache/huggingface/transformers/992e9e88844a23031919226d183d2dec1973bb93482dd767713146d989b9972e.aabd9267f2a2af6ec34667d063ffbcacdfd7da949d6c2f6e145f113dacce0263.lock\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[0m\n",
      "\u001b[34m2021-07-06 10:18:03,358 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "  | Name           | Type  | Params\u001b[0m\n",
      "\u001b[34m-----------------------------------------\u001b[0m\n",
      "\u001b[34m0 | model          | Model | 109 M \u001b[0m\n",
      "\u001b[34m1 | f1_score_train | F1    | 0     \u001b[0m\n",
      "\u001b[34m-----------------------------------------\u001b[0m\n",
      "\u001b[34m109 M     Trainable params\u001b[0m\n",
      "\u001b[34m0         Non-trainable params\u001b[0m\n",
      "\u001b[34m109 M     Total params\u001b[0m\n",
      "\u001b[34m437.980   Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mRegistered model 'pytorch-first-example' already exists. Creating a new version of this model...\u001b[0m\n",
      "\u001b[34m2021/07/06 10:18:02 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: pytorch-first-example, version 5\u001b[0m\n",
      "\u001b[34mCreated version '5' of model 'pytorch-first-example'.\n",
      "\u001b[0m\n",
      "\n",
      "2021-07-06 10:18:10 Uploading - Uploading generated training model\n",
      "2021-07-06 10:18:10 Completed - Training job completed\n",
      "Training seconds: 329\n",
      "Billable seconds: 329\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(fit_arguments, job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow.sagemaker\n",
    "\n",
    "# URL of the ECR-hosted Docker image the model should be deployed into\n",
    "image_uri = '961104659532.dkr.ecr.us-east-1.amazonaws.com/mlflow-pyfunc'\n",
    "endpoint_name = 'pytorch-trial'\n",
    "# The location, in URI format, of the MLflow model to deploy to SageMaker.\n",
    "model_uri = 's3://deep-mlflow-artifact/2/18ce50fe730646b6b80fbafdcd22aeb1/artifacts/sentence-transformers/paraphrase-mpnet-base-v2'\n",
    "\n",
    "mlflow.sagemaker.deploy(\n",
    "    mode='create',\n",
    "    app_name=endpoint_name,\n",
    "    model_uri=model_uri,\n",
    "    image_url=image_uri,\n",
    "    execution_role_arn=role_arn,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    region_name='us-east-1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlflow.pytorch import pickle_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../scripts/examples/sector-pl/')\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.load('/Users/stefano/Downloads/model.pth', pickle_module=pickle_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-06T11:17:10.680709Z",
     "iopub.status.busy": "2021-07-06T11:17:10.679083Z",
     "iopub.status.idle": "2021-07-06T11:19:10.422907Z",
     "shell.execute_reply": "2021-07-06T11:19:10.420103Z",
     "shell.execute_reply.started": "2021-07-06T11:17:10.680609Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-19-e5d16cbd2572>\", line 5, in <module>\n",
      "    loaded_model = mlflow.pytorch.load_model(logged_model)\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/mlflow/pytorch/__init__.py\", line 659, in load_model\n",
      "    local_model_path = _download_artifact_from_uri(artifact_uri=model_uri)\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/mlflow/tracking/artifact_utils.py\", line 79, in _download_artifact_from_uri\n",
      "    return get_artifact_repository(artifact_uri=root_uri).download_artifacts(\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/mlflow/store/artifact/artifact_repo.py\", line 181, in download_artifacts\n",
      "    return download_artifact_dir(\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/mlflow/store/artifact/artifact_repo.py\", line 148, in download_artifact_dir\n",
      "    download_artifact_dir(\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/mlflow/store/artifact/artifact_repo.py\", line 153, in download_artifact_dir\n",
      "    download_artifact(\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/mlflow/store/artifact/artifact_repo.py\", line 130, in download_artifact\n",
      "    self._download_file(\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/mlflow/store/artifact/s3_artifact_repo.py\", line 152, in _download_file\n",
      "    s3_client.download_file(bucket, s3_full_path, local_path)\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/boto3/s3/inject.py\", line 170, in download_file\n",
      "    return transfer.download_file(\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/boto3/s3/transfer.py\", line 307, in download_file\n",
      "    future.result()\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/s3transfer/futures.py\", line 109, in result\n",
      "    raise e\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/s3transfer/futures.py\", line 106, in result\n",
      "    return self._coordinator.result()\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/s3transfer/futures.py\", line 260, in result\n",
      "    self._done_event.wait(MAXINT)\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/threading.py\", line 574, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/threading.py\", line 312, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/inspect.py\", line 1503, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 182, in findsource\n",
      "    lines = linecache.getlines(file, globals_dict)\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/linecache.py\", line 46, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"/Users/stefano/miniconda3/envs/deep/lib/python3.9/tokenize.py\", line 392, in open\n",
      "    buffer = _builtin_open(filename, 'rb')\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-e5d16cbd2572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load model as a PyFuncModel.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogged_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/mlflow/pytorch/__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_uri, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m     \u001b[0mlocal_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_download_artifact_from_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/mlflow/tracking/artifact_utils.py\u001b[0m in \u001b[0;36m_download_artifact_from_uri\u001b[0;34m(artifact_uri, output_path)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     return get_artifact_repository(artifact_uri=root_uri).download_artifacts(\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0martifact_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/mlflow/store/artifact/artifact_repo.py\u001b[0m in \u001b[0;36mdownload_artifacts\u001b[0;34m(self, artifact_path, dst_path)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             return download_artifact_dir(\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0msrc_artifact_dir_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_local_dir_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/mlflow/store/artifact/artifact_repo.py\u001b[0m in \u001b[0;36mdownload_artifact_dir\u001b[0;34m(src_artifact_dir_path, dst_local_dir_path)\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfile_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                         download_artifact_dir(\n\u001b[0m\u001b[1;32m    149\u001b[0m                             \u001b[0msrc_artifact_dir_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/mlflow/store/artifact/artifact_repo.py\u001b[0m in \u001b[0;36mdownload_artifact_dir\u001b[0;34m(src_artifact_dir_path, dst_local_dir_path)\u001b[0m\n\u001b[1;32m    152\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                         download_artifact(\n\u001b[0m\u001b[1;32m    154\u001b[0m                             \u001b[0msrc_artifact_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_local_dir_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdst_local_dir_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/mlflow/store/artifact/artifact_repo.py\u001b[0m in \u001b[0;36mdownload_artifact\u001b[0;34m(src_artifact_path, dst_local_dir_path)\u001b[0m\n\u001b[1;32m    129\u001b[0m             )\n\u001b[0;32m--> 130\u001b[0;31m             self._download_file(\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0mremote_file_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_artifact_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_destination_file_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/mlflow/store/artifact/s3_artifact_repo.py\u001b[0m in \u001b[0;36m_download_file\u001b[0;34m(self, remote_file_path, local_path)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0ms3_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_s3_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0ms3_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms3_full_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mS3Transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransfer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         return transfer.download_file(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/boto3/s3/transfer.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;31m# This is for backwards compatibility where when retries are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# out of this and propogate the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;31m# years...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAXINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2060\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2061\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2064\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "logged_model = 's3://deep-mlflow-artifact/2/9f216acf38d54ff6b185441a0f80e8b7/artifacts/model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pytorch.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_model.predict(pd.DataFrame({'data': []}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
