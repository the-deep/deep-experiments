{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b06090",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:42:51.103587Z",
     "start_time": "2022-01-17T20:42:50.143438Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import functools\n",
    "from ast import literal_eval\n",
    "from operator import itemgetter\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from num2words import num2words\n",
    "                                                                \n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2452aca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:42:51.495081Z",
     "start_time": "2022-01-17T20:42:51.492503Z"
    }
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d7f07f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:42:55.140399Z",
     "start_time": "2022-01-17T20:42:55.133283Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    if doc != doc:\n",
    "        return \"\"\n",
    "    doc = doc.lower()\n",
    "    # remove preceeding dates\n",
    "    #doc = re.sub(\"^\\[.+\\]\", \" \", doc).strip()\n",
    "    #doc = re.sub(\"^\\(.+\\)\", \" \", doc).strip()\n",
    "    # spaces btw numbers and words\n",
    "    doc = re.sub('(\\d+(\\W\\d+)?)', r' \\1 ', doc).strip()\n",
    "    doc = re.sub(\"[‐‑–—―─_]\", \"-\", doc)\n",
    "    doc = re.sub(\"(\\w)\\- (\\w)\", r\"\\1\\2\", doc)\n",
    "    doc = re.sub(\n",
    "        \"[\" + re.escape(\n",
    "            '_@^~.()[],\"“’…<❖‐»—─|•&{≥➢\\ue0e4\\uf0d8\\uf0fc●°#\\u200b>`?�€■!‘%;̧\\'›«”:≤―\\uf0b7$}*´=‑▪\\xad❑·–'\n",
    "        ) + \"]\", \" \", doc)\n",
    "    #remove some puncs\n",
    "    doc = re.sub('\\s+', \" \", doc).strip()\n",
    "    return doc\n",
    "##\n",
    "def preprocess_and_tokenize(doc, n=1):\n",
    "    doc = preprocess(doc)\n",
    "    # tokenize\n",
    "    words = word_tokenize(doc)\n",
    "    if n == 1:\n",
    "        return set(words)\n",
    "    return set(ngrams(words, n))\n",
    "##\n",
    "def tokenize(words, n=1):\n",
    "    if n == 1:\n",
    "        return set(words)\n",
    "    return set(ngrams(words, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d4f0c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:42:58.509398Z",
     "start_time": "2022-01-17T20:42:56.309870Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/train_v0.7.1.csv\",\n",
    "                       usecols=[\n",
    "                           'entry_id', 'excerpt', 'specific_needs_groups',\n",
    "                           'lang', \"translation_en\",\n",
    "                           \"translation_fr\", \"translation_es\"\n",
    "                       ])\n",
    "df_val = pd.read_csv(\"../data/val_v0.7.1.csv\",\n",
    "                     usecols=[\n",
    "                         'entry_id', 'excerpt', 'specific_needs_groups',\n",
    "                         'lang', \"translation_en\", \"translation_fr\",\n",
    "                         \"translation_es\"\n",
    "                     ])\n",
    "df_test = pd.read_csv(\"../data/test_v0.7.1.csv\",\n",
    "                      usecols=[\n",
    "                          'entry_id', 'excerpt', 'specific_needs_groups',\n",
    "                          'lang', \"translation_en\", \"translation_fr\",\n",
    "                          \"translation_es\"\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d999dc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:02.148109Z",
     "start_time": "2022-01-17T20:43:01.197265Z"
    }
   },
   "outputs": [],
   "source": [
    "col = 'specific_needs_groups'\n",
    "for df in [df_train, df_val, df_test]:\n",
    "    df[col] = df[col].apply(lambda x: [\n",
    "        e for e in list(sorted(list(set(literal_eval(x)))))\n",
    "        #if e not in ['None', 'NOT_MAPPED']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e727b1ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:12.260990Z",
     "start_time": "2022-01-17T20:43:12.178948Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_en = df_train.copy()\n",
    "df_train_en.loc[df_train_en[\"lang\"].ne(\"en\"),\n",
    "                \"excerpt\"] = df_train_en.loc[df_train_en[\"lang\"].ne(\"en\"),\n",
    "                                             \"translation_en\"]\n",
    "##\n",
    "df_val_en = df_val.copy()\n",
    "df_val_en.loc[df_val_en[\"lang\"].ne(\"en\"),\n",
    "                \"excerpt\"] = df_val_en.loc[df_val_en[\"lang\"].ne(\"en\"),\n",
    "                                             \"translation_en\"]\n",
    "##\n",
    "df_test_en = df_test.copy()\n",
    "df_test_en.loc[df_test_en[\"lang\"].ne(\"en\"),\n",
    "                \"excerpt\"] = df_test_en.loc[df_test_en[\"lang\"].ne(\"en\"),\n",
    "                                             \"translation_en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808b77dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:13.925746Z",
     "start_time": "2022-01-17T20:43:13.923603Z"
    }
   },
   "outputs": [],
   "source": [
    "def unique_values(df, col):\n",
    "    vals = Counter()\n",
    "    for val in df[col]:\n",
    "        vals.update(val)\n",
    "    return vals.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d0f412e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:15.257908Z",
     "start_time": "2022-01-17T20:43:15.183270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('Pregnant or Lactating Women', 1667),\n",
       "  ('Indigenous people', 1341),\n",
       "  ('Persons with Disability', 1289),\n",
       "  ('Minorities', 757),\n",
       "  ('GBV survivors', 742),\n",
       "  ('Unaccompanied or Separated Children', 638),\n",
       "  ('Chronically Ill', 615),\n",
       "  ('Female Head of Household', 573),\n",
       "  ('LGBTQI+', 411),\n",
       "  ('Single Women (including Widows)', 157),\n",
       "  ('Child Head of Household', 139),\n",
       "  ('Elderly Head of Household', 93)],)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values(df_train_en, \"specific_needs_groups\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "787dd7eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:17.201012Z",
     "start_time": "2022-01-17T20:43:17.192815Z"
    }
   },
   "outputs": [],
   "source": [
    "plw = 'Pregnant or Lactating Women'\n",
    "ip = 'Indigenous people'\n",
    "pwd = 'Persons with Disability'\n",
    "minorities = 'Minorities'\n",
    "gbv = 'GBV survivors'\n",
    "unaccompanied_child = 'Unaccompanied or Separated Children'\n",
    "chronically_ill = 'Chronically Ill'\n",
    "fhh = 'Female Head of Household'\n",
    "lgbt = 'LGBTQI+'\n",
    "single_women = 'Single Women (including Widows)'\n",
    "chh = 'Child Head of Household'\n",
    "ehh = 'Elderly Head of Household'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "588e82fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:17.870719Z",
     "start_time": "2022-01-17T20:43:17.815835Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_en_positive = df_train_en[df_train_en['specific_needs_groups'].apply(\n",
    "    lambda x: x != [])].copy()\n",
    "##\n",
    "df_train_en_plw = df_train_en_positive[df_train_en_positive['specific_needs_groups'].apply(\n",
    "    lambda x: plw in x)]\n",
    "df_train_en_ip = df_train_en_positive[df_train_en_positive['specific_needs_groups'].apply(\n",
    "    lambda x: ip in x)]\n",
    "df_train_en_pwd = df_train_en_positive[df_train_en_positive['specific_needs_groups'].apply(\n",
    "    lambda x: pwd in x)]\n",
    "df_train_en_minorities = df_train_en_positive[df_train_en_positive['specific_needs_groups'].apply(\n",
    "    lambda x: minorities in x)]\n",
    "df_train_en_gbv = df_train_en_positive[df_train_en_positive['specific_needs_groups'].apply(\n",
    "    lambda x: gbv in x)]\n",
    "df_train_en_unaccompanied_child = df_train_en_positive[df_train_en_positive['specific_needs_groups'].apply(\n",
    "    lambda x: unaccompanied_child in x)]\n",
    "df_train_en_chronically_ill = df_train_en_positive[df_train_en_positive['specific_needs_groups'].apply(\n",
    "    lambda x: chronically_ill in x)]\n",
    "df_train_en_fhh = df_train_en_positive[df_train_en_positive['specific_needs_groups'].apply(\n",
    "    lambda x: fhh in x)]\n",
    "df_train_en_lgbt = df_train_en_positive[df_train_en_positive['specific_needs_groups'].apply(\n",
    "    lambda x: lgbt in x)]\n",
    "df_train_en_single_women = df_train_en_positive[df_train_en_positive['specific_needs_groups'].apply(\n",
    "    lambda x: single_women in x)]\n",
    "df_train_en_chh = df_train_en_positive[df_train_en_positive['specific_needs_groups'].apply(\n",
    "    lambda x: chh in x)]\n",
    "df_train_en_ehh = df_train_en_positive[df_train_en_positive['specific_needs_groups'].apply(\n",
    "    lambda x: ehh in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef459b15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:19.221697Z",
     "start_time": "2022-01-17T20:43:19.180218Z"
    }
   },
   "outputs": [],
   "source": [
    "class KeywordExtractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        docs_bg_corpus,\n",
    "        docs_classes,\n",
    "        lang=\"en\",\n",
    "        n_grams=2,\n",
    "        num_to_words=False,\n",
    "        stop_words=None,\n",
    "    ):\n",
    "        if isinstance(stop_words, Sequence) and not isinstance(stop_words, set):\n",
    "            self.stop_words = set(stop_words)\n",
    "        elif stop_words is None:\n",
    "            self.stop_words = set()\n",
    "        self.n_grams = n_grams\n",
    "        self.num_to_words = num_to_words\n",
    "        self.docs_bg_corpus = docs_bg_corpus\n",
    "        self.class_name_to_idx = dict(\n",
    "            zip(list(docs_classes.keys()), range(len(docs_classes))))\n",
    "        self.docs_classes = list(docs_classes.values())\n",
    "        self.lang = lang\n",
    "        ## for preprocessing - should be moved to a util class/func\n",
    "        self.num_normalizer = dict(\n",
    "            zip(\"⁰¹²³⁴⁵⁶⁷⁸⁹\", [str(i) for i in range(10)]))\n",
    "        self.num_normalizer.update(\n",
    "            dict(zip(\"₀₁₂₃₄₅₆₇₈₉\", [str(i) for i in range(10)])))\n",
    "        if lang == \"en\":\n",
    "            self.num_normalizer.update({\"⅓\": \"one-third\", \"¼\": \"one-fourth\"})\n",
    "        elif lang == \"fr\":\n",
    "            self.num_normalizer.update({\"⅓\": \"un-tiers\", \"¼\": \"un-quart\"})\n",
    "        elif lang == \"es\":\n",
    "            self.num_normalizer.update({\"⅓\": \"un-tercio\", \"¼\": \"un-tercio\"})\n",
    "        ##\n",
    "        self.word_to_freq_bg_corpus = self.extract_word_counts(docs_bg_corpus)\n",
    "        # split word to freq dict into multiple dicts according to ngram len\n",
    "        self.ngram_to_freq_bg_corpus = [dict() for _ in range(n_grams)]\n",
    "        for kw, count in self.word_to_freq_bg_corpus.items():\n",
    "            if isinstance(kw, str):\n",
    "                self.ngram_to_freq_bg_corpus[0][kw] = count\n",
    "            else:\n",
    "                self.ngram_to_freq_bg_corpus[len(kw) - 1][kw] = count\n",
    "        self.bg_corpus_sizes = [\n",
    "            sum(self.ngram_to_freq_bg_corpus[i].values())\n",
    "            for i in range(n_grams)\n",
    "        ]\n",
    "        ##\n",
    "        self.word_to_freq_classes = [\n",
    "            self.extract_word_counts(corpus) for corpus in self.docs_classes\n",
    "        ]\n",
    "        # split word to freq dict of each class into multiple dicts\n",
    "        # len(ngram_word_to_freq_classes) = c\n",
    "        # len(ngram_word_to_freq_classes[x]) = n\n",
    "        self.ngram_word_to_freq_classes = [[dict() for _ in range(n_grams)]\n",
    "                                           for _ in self.class_name_to_idx]\n",
    "        for c, word_to_freq_cls in enumerate(self.word_to_freq_classes):\n",
    "            for kw, count in word_to_freq_cls.items():\n",
    "                n = 0 if isinstance(kw, str) else len(kw) - 1\n",
    "                self.ngram_word_to_freq_classes[c][n][kw] = count\n",
    "        self.ngram_corpora_sizes = [[\n",
    "            sum(word_to_freq[i].values()) for i in range(n_grams)\n",
    "        ] for word_to_freq in self.ngram_word_to_freq_classes]\n",
    "\n",
    "        self.ngram_corpora_sizes = [[\n",
    "            sum(word_to_freq[i].values()) for i in range(n_grams)\n",
    "        ] for word_to_freq in self.ngram_word_to_freq_classes]\n",
    "        ##\n",
    "        # calc likelihoods for each ngram length in each class separately\n",
    "        # a list of lists of dicts\n",
    "        # each represents a class\n",
    "        # each class is represented by n dicts\n",
    "        # each dict is {\"ngram in class\": likelihood}\n",
    "        self.ngram_likelihoods = [[] for _ in self.class_name_to_idx]\n",
    "        for c, cls_name in enumerate(self.class_name_to_idx):\n",
    "            for n in range(self.n_grams):\n",
    "                w_to_f = self.ngram_word_to_freq_classes[c][n]\n",
    "                corpus_size = self.ngram_corpora_sizes[c][n]\n",
    "                self.ngram_likelihoods[c].append(\n",
    "                    self.calc_likelihoods(w_to_f, corpus_size))\n",
    "        ##\n",
    "        # calc potts scores for each ngram in each class\n",
    "        add_two_dict = lambda a, b: {\n",
    "            **a,\n",
    "            **b,\n",
    "            **{k: a[k] + b[k]\n",
    "               for k in a.keys() & b}\n",
    "        }\n",
    "        self.ngram_potts_scores = [[] for _ in self.class_name_to_idx]\n",
    "        self.ngram_z_score_of_the_log_odds_ratios = [\n",
    "            [] for _ in self.class_name_to_idx\n",
    "        ]\n",
    "        for c, cls_name in enumerate(self.class_name_to_idx):\n",
    "            for n in range(self.n_grams):\n",
    "                lh = self.ngram_likelihoods[c][n]\n",
    "                other_lhs = [\n",
    "                    self.ngram_likelihoods[c_other][n]\n",
    "                    for c_other, _ in enumerate(self.class_name_to_idx)\n",
    "                    if c != c_other\n",
    "                ]\n",
    "                other_lhs = functools.reduce(add_two_dict, other_lhs)\n",
    "                self.ngram_potts_scores[c].append(\n",
    "                    self.calc_potts_scores(lh, other_lhs))\n",
    "\n",
    "                word_to_freq = self.ngram_word_to_freq_classes[c][n]\n",
    "                corpus_size = self.ngram_corpora_sizes[c][n]\n",
    "                word_to_freq_others = [\n",
    "                    self.ngram_word_to_freq_classes[c_other][n]\n",
    "                    for c_other, _ in enumerate(self.class_name_to_idx)\n",
    "                    if c != c_other\n",
    "                ]\n",
    "                word_to_freq_others = functools.reduce(add_two_dict,\n",
    "                                                       word_to_freq_others)\n",
    "                corpus_size_others = sum([\n",
    "                    self.ngram_corpora_sizes[c_other][n]\n",
    "                    for c_other, _ in enumerate(self.class_name_to_idx)\n",
    "                    if c != c_other\n",
    "                ])\n",
    "\n",
    "                self.ngram_z_score_of_the_log_odds_ratios[c].append(\n",
    "                    self.calc_prior_modified_log_odds_ratio(\n",
    "                        word_to_freq, corpus_size, word_to_freq_others,\n",
    "                        corpus_size_others, self.ngram_to_freq_bg_corpus[n],\n",
    "                        self.bg_corpus_sizes[n]))\n",
    "\n",
    "    def preprocess_and_tokenize(self, doc):\n",
    "        if doc != doc:\n",
    "            return \"\"\n",
    "        # remove preceeding dates\n",
    "        doc = re.sub(\"^\\[.+\\]\", \" \", doc).strip()\n",
    "        doc = re.sub(\"^\\(.+\\)\", \" \", doc).strip()\n",
    "        # spaces btw numbers and words\n",
    "        doc = re.sub('(\\d+(\\W\\d+)?)', r' \\1 ', doc).strip()\n",
    "        doc = re.sub(\"[‐‑–—―─_]\", \"-\", doc)\n",
    "        doc = re.sub(\n",
    "            \"[\" + re.escape(\n",
    "                '_@^~.()[],\"“’…<❖‐»—─|•&{≥➢\\ue0e4\\uf0d8\\uf0fc●°#\\u200b>`?�€■!‘%;̧\\'›«”:≤―\\uf0b7$}*+´=‑▪\\xad❑·–'\n",
    "            ) + \"]\", \" \", doc)\n",
    "        #remove some puncs\n",
    "        doc = re.sub('\\s+', \" \", doc)\n",
    "\n",
    "        # tokenize\n",
    "        words = word_tokenize(doc)\n",
    "        # lower and remove non-words\n",
    "        words = [word.lower() for word in words if word not in self.stop_words]\n",
    "        words = [self.num_normalizer.get(token, token) for token in words]\n",
    "        if self.num_to_words:\n",
    "            words = [\n",
    "                num2words(token, lang=self.lang)\n",
    "                if token.isnumeric() else token for token in words\n",
    "            ]\n",
    "        kw_kp = words.copy()\n",
    "        for n in range(2, self.n_grams + 1):\n",
    "            kw_kp.extend(list(ngrams(words, n)))\n",
    "        return kw_kp\n",
    "\n",
    "    def calc_potts_scores(self, word_to_likelihood_main,\n",
    "                          word_to_likelihood_other):\n",
    "        potts_scores = dict()\n",
    "        for word in word_to_likelihood_main.keys():\n",
    "            potts_scores[word] = word_to_likelihood_main[word] / (\n",
    "                word_to_likelihood_main[word] +\n",
    "                word_to_likelihood_other.get(word, 0))\n",
    "        return potts_scores\n",
    "\n",
    "    def calc_likelihoods(self, word_to_freq, corpus_size):\n",
    "        likelihoods = dict()\n",
    "        for word, count in word_to_freq.items():\n",
    "            likelihoods[word] = count / corpus_size\n",
    "        return likelihoods\n",
    "\n",
    "    def extract_word_counts(self, docs):\n",
    "        word_to_freq = defaultdict(int)\n",
    "        for doc in docs:\n",
    "            words = self.preprocess_and_tokenize(doc)\n",
    "            for word in words:\n",
    "                #if word in stopwords, then do not add it\n",
    "                word_to_freq[word] += 1\n",
    "\n",
    "        return word_to_freq\n",
    "\n",
    "    def calc_prior_modified_log_odds_ratio(self, word_to_freq_c1,\n",
    "                                           corpus_size_c1, word_to_freq_c2,\n",
    "                                           corpus_size_c2, word_to_freq_all,\n",
    "                                           corpus_size_all):\n",
    "\n",
    "        prior_modified_log_odds_ratio_c1 = dict()\n",
    "        variance_of_the_log_odds_ratio = dict()\n",
    "        z_score_of_the_log_odds_ratio_c1 = dict()\n",
    "        ##\n",
    "        for word in word_to_freq_c1.keys():\n",
    "            numerator_1 = word_to_freq_c1[word] + word_to_freq_all[word]\n",
    "            denomerator_1 = corpus_size_c1 + corpus_size_all - (numerator_1)\n",
    "            ratio_1 = np.log(numerator_1 / denomerator_1)\n",
    "            ##\n",
    "            numerator_2 = word_to_freq_c2.get(word, 0) + word_to_freq_all[word]\n",
    "            denomerator_2 = corpus_size_c2 + corpus_size_all - (numerator_2)\n",
    "            ratio_2 = np.log(numerator_2 / denomerator_2)\n",
    "            ##\n",
    "            prior_modified_log_odds_ratio_c1[word] = ratio_1 - ratio_2\n",
    "            ##\n",
    "            variance_of_the_log_odds_ratio[word] = (1 / numerator_1) + (\n",
    "                1 / numerator_2)\n",
    "            ##\n",
    "            z_score_of_the_log_odds_ratio_c1[\n",
    "                word] = prior_modified_log_odds_ratio_c1[word] / np.sqrt(\n",
    "                    variance_of_the_log_odds_ratio[word])\n",
    "        return z_score_of_the_log_odds_ratio_c1\n",
    "\n",
    "    def get_kws(self, cls_name, n):\n",
    "        cls_idx = self.class_name_to_idx[cls_name]\n",
    "        kw_dict = self.ngram_z_score_of_the_log_odds_ratios[cls_idx][n - 1]\n",
    "        return list(\n",
    "            sorted([(word, score) for word, score in kw_dict.items()],\n",
    "                   key=itemgetter(1),\n",
    "                   reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5866b69f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:22.634163Z",
     "start_time": "2022-01-17T20:43:22.630229Z"
    }
   },
   "outputs": [],
   "source": [
    "n_grams = 7\n",
    "stop_words_en = None\n",
    "# stop_words_en = stopwords.words(\"english\")\n",
    "kwe_en = KeywordExtractor(\n",
    "    df_train_en_positive['excerpt'].tolist(),\n",
    "    {\n",
    "        plw:df_train_en_plw[\"excerpt\"].tolist(),\n",
    "        ip:df_train_en_ip[\"excerpt\"].tolist(),\n",
    "        pwd:df_train_en_pwd[\"excerpt\"].tolist(),\n",
    "        minorities:df_train_en_minorities[\"excerpt\"].tolist(),\n",
    "        gbv:df_train_en_gbv[\"excerpt\"].tolist(),\n",
    "        unaccompanied_child:df_train_en_unaccompanied_child[\"excerpt\"].tolist(),\n",
    "        chronically_ill:df_train_en_chronically_ill[\"excerpt\"].tolist(),\n",
    "        fhh:df_train_en_fhh[\"excerpt\"].tolist(),\n",
    "        lgbt:df_train_en_lgbt[\"excerpt\"].tolist(),\n",
    "        single_women:df_train_en_single_women[\"excerpt\"].tolist(),\n",
    "        chh:df_train_en_chh[\"excerpt\"].tolist(),\n",
    "        ehh:df_train_en_ehh[\"excerpt\"].tolist(),\n",
    "    },\n",
    "    n_grams=n_grams,\n",
    "    stop_words=stop_words_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d59204db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:30.036815Z",
     "start_time": "2022-01-17T20:43:30.014187Z"
    }
   },
   "outputs": [],
   "source": [
    "kw_en_plw = [\n",
    "    'abortion', 'abortions', 'ahi', 'ante-natal', 'anteparto', 'breastfall',\n",
    "    'breastfed', 'breastfeed', 'breastfeeding', 'breastfeeds', 'childbirth',\n",
    "    'childbirths', 'eclampsia', 'fetal', 'fetus', 'gestant', 'gestants',\n",
    "    'gestation', 'gestational', 'gestations', 'gs/plw', 'hcw', 'impregnated',\n",
    "    'infanting', 'intrapartum', 'ipt', 'lactate', 'lactates', 'lactating',\n",
    "    'lactating/pregnant', 'lactation', 'lactations', 'miscarriage',\n",
    "    'miscarriages', 'mothers/plw', #'neo-natal', 'neonatal', 'neo-natals',\n",
    "    #'neonatals', 'neonatology', 'newborn', 'newborns', 'new-born', 'new-borns',\n",
    "    'non-breastfall', 'non-breastfeeding', 'obsteric', 'obstestrium',\n",
    "    'obstetric', 'obstetrician', 'obstetricians', 'obstetrics', 'parturients',\n",
    "    'perinatal', 'perinatals', 'placental', 'placentarios', 'plw', 'plw/gs',\n",
    "    'plw/mothers', 'plwg', 'plwgs', 'plws', 'post-birth', 'post-natal',\n",
    "    'post-partum', 'postnatal', 'postpartum', 'pre-natal', 'preeclampsia',\n",
    "    'pregnancies', 'pregnancy', 'pregnancy-related', 'pregnant',\n",
    "    'pregnant/lactating', 'pregnants', 'prematurity-immature', 'prenatal', \n",
    "]\n",
    "##\n",
    "negative_kw_plw = []\n",
    "##\n",
    "plw_patterns = []\n",
    "##\n",
    "def is_plw(row):\n",
    "    for kw in negative_kw_plw:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return False\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return False\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return False\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return False\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return False\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return False\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return False\n",
    "    for p in plw_patterns:\n",
    "        if re.search(p, row[\"excerpt_pp\"]): return True\n",
    "    for kw in kw_en_plw:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return True\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return True\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return True\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return True\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return True\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return True\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return True\n",
    "    return False\n",
    "#list(sorted(set(kw_en_plw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f32a4fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:31.280540Z",
     "start_time": "2022-01-17T20:43:31.259206Z"
    }
   },
   "outputs": [],
   "source": [
    "kw_en_ip = [\n",
    "   'indigenous', 'habitat', 'habitats', 'aboriginals', # 'inhabitants', 'inhabitant', \n",
    "    ('aboriginal', 'people'), ('aboriginal', 'peoples'), ('aboriginal', 'persons'), \n",
    "    ('aboriginal', 'community'), ('aboriginal', 'communities'), ('urban', 'communities'), \n",
    "    ('urban', 'community'), ('rural', 'communities'), ('rural', 'community'), \n",
    "    ('recipient', 'community'), ('recipient', 'communities'), \n",
    "]\n",
    "##\n",
    "negative_kw_ip = []\n",
    "##\n",
    "ip_patterns = []\n",
    "##\n",
    "def is_ip(row):\n",
    "    for kw in negative_kw_ip:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return False\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return False\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return False\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return False\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return False\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return False\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return False\n",
    "    for p in ip_patterns:\n",
    "        if re.search(p, row[\"excerpt_pp\"]): return True\n",
    "    for kw in kw_en_ip:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return True\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return True\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return True\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return True\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return True\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return True\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9beb4a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:46:47.780346Z",
     "start_time": "2022-01-17T20:46:47.744241Z"
    }
   },
   "outputs": [],
   "source": [
    "kw_en_pwd = [\n",
    "    #'disabilities',\n",
    "    'disabilites',\n",
    "    #'disability',\n",
    "    #'disabled',\n",
    "    'handicap',\n",
    "    'pwds',\n",
    "    'pwd',\n",
    "    #'impaired',\n",
    "    'wheelchair',\n",
    "    'wheelchairs',\n",
    "    'disability-related',\n",
    "    'deaf',\n",
    "    'prosthesis',\n",
    "    'rehabilitative',\n",
    "    'prosthetic',\n",
    "    'prostheses',\n",
    "    'deaf-deaf',\n",
    "    'atrophy',\n",
    "    'disability-confidence',\n",
    "    'guide-dogs',\n",
    "    'power-wheelchairs',\n",
    "    'pwg',\n",
    "    \"pbs\",#Personnes à Besoins Spécifiques\n",
    "    'psns', #persons with specific needs (PSNs)\n",
    "    \"schizophrenia\",\n",
    "    'deafness', \n",
    "    ('people', 'with', 'reduced', 'mobility'),\n",
    "    ('persons', 'with', 'reduced', 'mobility'),\n",
    "    ('children', 'with', 'reduced', 'mobility'),\n",
    "    ('men', 'with', 'reduced', 'mobility'),\n",
    "    ('women', 'with', 'reduced', 'mobility'),\n",
    "    ('elderly', 'with', 'reduced', 'mobility'),\n",
    "    ('people', 'with', 'reduced', 'or', 'elderly', 'mobility'),\n",
    "    \n",
    "    ('persons', 'with', 'specific', 'needs'),\n",
    "    ('people', 'with', 'specific', 'needs'),\n",
    "    ('peoples', 'with', 'specific', 'needs'),\n",
    "    ('children', 'with', 'specific', 'needs'),\n",
    "    ('girls', 'with', 'specific', 'needs'),\n",
    "    ('boys', 'with', 'specific', 'needs'),\n",
    "    ('women', 'with', 'specific', 'needs'),\n",
    "    ('men', 'with', 'specific', 'needs'),\n",
    "    ('elderly', 'with', 'specific', 'needs'),\n",
    "    ('persons', 'with', 'special', 'needs'),\n",
    "    ('people', 'with', 'special', 'needs'),\n",
    "    ('peoples', 'with', 'special', 'needs'),\n",
    "    ('children', 'with', 'special', 'needs'),\n",
    "    ('girls', 'with', 'special', 'needs'),\n",
    "    ('boys', 'with', 'special', 'needs'),\n",
    "    ('women', 'with', 'special', 'needs'),\n",
    "    ('men', 'with', 'special', 'needs'),\n",
    "    ('elderly', 'with', 'special', 'needs'),\n",
    "    ('persons', 'with', 'difficulties', 'in', 'functioning'),\n",
    "    ('people', 'with', 'difficulties', 'in', 'functioning'),\n",
    "    ('peoples', 'with', 'difficulties', 'in', 'functioning'),\n",
    "    ('children', 'with', 'difficulties', 'in', 'functioning'),\n",
    "    ('women', 'with', 'difficulties', 'in', 'functioning'),\n",
    "    ('men', 'with', 'difficulties', 'in', 'functioning'),\n",
    "    ('elderly', 'with', 'difficulties', 'in', 'functioning'),\n",
    "    \"dementia\",\n",
    "    \n",
    "    ('persons', 'with', 'certain', 'impairments'),\n",
    "    ('people', 'with', 'certain', 'impairments'),\n",
    "    ('peoples', 'with', 'certain', 'impairments'),\n",
    "    ('children', 'with', 'certain', 'impairments'),\n",
    "    ('women', 'with', 'certain', 'impairments'),\n",
    "    ('men', 'with', 'certain', 'impairments'),\n",
    "    ('elderly', 'with', 'certain', 'impairments'),\n",
    "    ('physical', 'impairments'),\n",
    "    ('physical', 'impairment'),\n",
    "    ('visual', 'impairments'),\n",
    "    ('visual', 'impairment'),\n",
    "    ('sensory', 'impairments'),\n",
    "    ('sensory', 'impairment'),\n",
    "    ('hearing', 'impairments'),\n",
    "    ('hearing', 'impairment'),\n",
    "    ('physical', 'impairments'),\n",
    "    ('physical', 'impairment'),\n",
    "    ('psychic', 'impairments'),\n",
    "    ('psychic', 'impairment'),\n",
    "    ('psychiatric', 'impairments'),\n",
    "    ('psychiatric', 'impairment'),\n",
    "    ('psychological', 'impairments'),\n",
    "    ('psychological', 'impairment'),\n",
    "    ('mobility', 'impairments'),\n",
    "    ('mobility', 'impairment'),\n",
    "    ('persons', 'with', 'physical'),\n",
    "    ('people', 'with', 'physical'),\n",
    "    ('peoples', 'with', 'physical'),\n",
    "    ('children', 'with', 'physical'),\n",
    "    ('girls', 'with', 'physical'),\n",
    "    ('boys', 'with', 'physical'),\n",
    "    ('youths', 'with', 'physical'),\n",
    "    ('women', 'with', 'physical'),\n",
    "    ('men', 'with', 'physical'),\n",
    "    ('elderly', 'with', 'physical'),\n",
    "    ('persons', 'with', 'psychic'),\n",
    "    ('people', 'with', 'psychic'),\n",
    "    ('peoples', 'with', 'psychic'),\n",
    "    ('children', 'with', 'psychic'),\n",
    "    ('girls', 'with', 'psychic'),\n",
    "    ('boys', 'with', 'psychic'),\n",
    "    ('youths', 'with', 'psychic'),\n",
    "    ('women', 'with', 'psychic'),\n",
    "    ('men', 'with', 'psychic'),\n",
    "    ('elderly', 'with', 'psychic'),\n",
    "    ('living', 'with', 'specific', 'needs'),\n",
    "    ('reduced', 'mobility'),\n",
    "    ('impaired', 'mobility'),\n",
    "    ('had', 'any', 'specific', 'need'),\n",
    "    ('physical', 'difficulties'),\n",
    "    ('physical', 'difficulty'),\n",
    "    ('difficulty', 'walking'),\n",
    "    ('people', 'have', 'specific', 'needs'),\n",
    "    ('people', 'having', 'specific', 'needs'),\n",
    "    ('difficulties', 'of', 'mobility'),\n",
    "    ('difficulty', 'of', 'mobility'),\n",
    "    ('barely', 'moves', 'alone'),\n",
    "    ('persons', 'with', 'disabilities'),\n",
    "    ('people', 'with', 'disabilities'),\n",
    "    ('peoples', 'with', 'disabilities'),\n",
    "    ('children', 'with', 'disabilities'),\n",
    "    ('girls', 'with', 'disabilities'),\n",
    "    ('boys', 'with', 'disabilities'),\n",
    "    ('women', 'with', 'disabilities'),\n",
    "    ('men', 'with', 'disabilities'),\n",
    "    ('elderly', 'with', 'disabilities'),\n",
    "    ('persons', 'with', 'disabilities-'),\n",
    "    ('people', 'with', 'disabilities-'),\n",
    "    ('peoples', 'with', 'disabilities-'),\n",
    "    ('children', 'with', 'disabilities-'),\n",
    "    ('girls', 'with', 'disabilities-'),\n",
    "    ('boys', 'with', 'disabilities-'),\n",
    "    ('women', 'with', 'disabilities-'),\n",
    "    ('men', 'with', 'disabilities-'),\n",
    "    ('elderly', 'with', 'disabilities-'),\n",
    "    ('persons', 'with', 'disabilities—'),\n",
    "    ('people', 'with', 'disabilities—'),\n",
    "    ('peoples', 'with', 'disabilities—'),\n",
    "    ('children', 'with', 'disabilities—'),\n",
    "    ('girls', 'with', 'disabilities—'),\n",
    "    ('boys', 'with', 'disabilities—'),\n",
    "    ('women', 'with', 'disabilities—'),\n",
    "    ('men', 'with', 'disabilities—'),\n",
    "    ('elderly', 'with', 'disabilities—'),\n",
    "    ('persons', 'with', 'disability'),\n",
    "    ('people', 'with', 'disability'),\n",
    "    ('peoples', 'with', 'disability'),\n",
    "    ('children', 'with', 'disability'),\n",
    "    ('girls', 'with', 'disability'),\n",
    "    ('boys', 'with', 'disability'),\n",
    "    ('women', 'with', 'disability'),\n",
    "    ('men', 'with', 'disability'),\n",
    "    ('elderly', 'with', 'disability'),\n",
    "    ('persons', 'with', 'disability-'),\n",
    "    ('people', 'with', 'disability-'),\n",
    "    ('peoples', 'with', 'disability-'),\n",
    "    ('children', 'with', 'disability-'),\n",
    "    ('girls', 'with', 'disability-'),\n",
    "    ('boys', 'with', 'disability-'),\n",
    "    ('women', 'with', 'disability-'),\n",
    "    ('men', 'with', 'disability-'),\n",
    "    ('elderly', 'with', 'disability-'),\n",
    "    ('persons', 'with', 'disability—'),\n",
    "    ('people', 'with', 'disability—'),\n",
    "    ('peoples', 'with', 'disability—'),\n",
    "    ('children', 'with', 'disability—'),\n",
    "    ('girls', 'with', 'disability—'),\n",
    "    ('boys', 'with', 'disability—'),\n",
    "    ('women', 'with', 'disability—'),\n",
    "    ('men', 'with', 'disability—'),\n",
    "    ('elderly', 'with', 'disability—'),\n",
    "    ('disabled', 'persons'),\n",
    "    ('disabled', 'people'),\n",
    "    ('disabled', 'peoples'),\n",
    "    ('disabled', 'children'),\n",
    "    ('disabled', 'girls'),\n",
    "    ('disabled', 'boys'),\n",
    "    ('disabled', 'women'),\n",
    "    ('disabled', 'men'),\n",
    "    ('disabled', 'elderly'),\n",
    "    ('persons', 'in', 'disability'),\n",
    "    ('people', 'in', 'disability'),\n",
    "    ('persons', 'in', 'disabilities'),\n",
    "    ('people', 'in', 'disabilities'),\n",
    "    ('living', 'with', 'a', 'disability'),\n",
    "    ('living', 'with', 'disability'),\n",
    "    ('living', 'with', 'disabilities'),\n",
    "]\n",
    "\n",
    "\n",
    "#disability --> neg when: payment, sales\n",
    "#disabled\n",
    "#     impaired the labor\n",
    "#\n",
    "##\n",
    "def lf_pwd_pos_1(row):\n",
    "    if len({'disability', 'disabilities', 'disabled', 'impaired'}\n",
    "           & row['tokenized_excerpt']) and (len(\n",
    "               {'labor', 'labuor', 'sales', 'market', 'payment', 'payments'}\n",
    "               & row['tokenized_excerpt'])):\n",
    "        return -1\n",
    "    elif len({'disability', 'disabilities', 'disabled', 'impaired'}\n",
    "             & row['tokenized_excerpt']) and (not len(\n",
    "                 {'labor', 'labuor', 'sales', 'market', 'payment', 'payments'}\n",
    "                 & row['tokenized_excerpt'])):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "##\n",
    "def lf_pwd_pos_2(row):\n",
    "    if row[\"lang\"] == \"fr\" and (\n",
    "        ('women', 'and', 'girls', 'stupids') in row[\"fourgram_excerpt\"] or\n",
    "        ('of', 'a', 'certain', 'vulnerability') in row[\"fourgram_excerpt\"] or\n",
    "        ('with', 'other', 'vulnerabilities') in row[\"trigram_excerpt\"]):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "##\n",
    "\n",
    "##\n",
    "negative_kw_pwd = []\n",
    "##\n",
    "pwd_patterns = []\n",
    "\n",
    "\n",
    "##\n",
    "def is_pwd(row):\n",
    "    for kw in negative_kw_pwd:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return False\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return False\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return False\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return False\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return False\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return False\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return False\n",
    "    for p in pwd_patterns:\n",
    "        if re.search(p, row[\"excerpt_pp\"]): return True\n",
    "    for lf in [lf_pwd_pos_1, lf_pwd_pos_2]:\n",
    "        if lf(row) == 1:\n",
    "            return True\n",
    "    for kw in kw_en_pwd:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return True\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return True\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return True\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return True\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return True\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return True\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1834957d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:32.410293Z",
     "start_time": "2022-01-17T20:43:32.377178Z"
    }
   },
   "outputs": [],
   "source": [
    "kw_en_gbv = [\n",
    "    'gbv',\n",
    "    'gbvs',\n",
    "    'raping',\n",
    "    'rape',\n",
    "    'rapes',\n",
    "    'raped',\n",
    "    'rapist',\n",
    "    'rapists',\n",
    "    'sgbv',\n",
    "    'sgbvs',\n",
    "    'vbg',\n",
    "    'vbgs',\n",
    "    'vsbg',\n",
    "    'vbgs',\n",
    "    'gbbs',\n",
    "    'gbv-covid-',\n",
    "    'gbvsc-nwsw-cameroon',\n",
    "    'sgtvbg',\n",
    "    'sgt',\n",
    "    'gbvv',\n",
    "    'gbvims',\n",
    "    'gbv-related',\n",
    "    'gbv-specific',\n",
    "    'sgbv/covid',\n",
    "    'protection/gbv', \n",
    "    'gender-based',\n",
    "    'fgm',\n",
    "    'fgms',\n",
    "    'fpcs',\n",
    "    ('sexually', 'harassed'),\n",
    "    ('survival', 'sex'),\n",
    "    'vawg',\n",
    "    'bushwives',\n",
    "    'bushwife',\n",
    "    ('sexual', 'slaves'),\n",
    "    ('sexual', 'slave'),\n",
    "    ('attacks', 'on', 'integrity'),\n",
    "    ('infringement', 'of', 'integrity'),\n",
    "    ('infringements', 'of', 'the', 'integrity'),\n",
    "    'vsbgs',\n",
    "    ('domestic', 'abuse'),\n",
    "    ('intimate', 'violence'),\n",
    "    ('inappropriate', 'intimate', 'behaviours'),\n",
    "    ('females', 'facing', 'attacks'),\n",
    "    ('girls', 'facing', 'attacks'),\n",
    "    ('women', 'facing', 'attacks'),\n",
    "    ('survivors', 'of', 'violence'),\n",
    "    ('sexual', 'and', 'emotional', 'violence'),\n",
    "    ('genital', 'mutilation'),\n",
    "    ('sexual', 'assignments'),\n",
    "    ('sexual', 'operating'),  # fr\n",
    "    'vawg',  #VAWG # Violence Against Women and Girls\n",
    "    'vawgs',  #VAWG # Violence Against Women and Girls\n",
    "    'gbvie',  #GBViE # GBV in Emergencies (GBViE)\n",
    "    'psea',  #Protection against sexual exploitation and abuse (PSEA)\n",
    "    ('sexual', 'violence'),\n",
    "    ('genderbased', 'violence'),\n",
    "    ('gender-based', 'violence'),\n",
    "    ('sexual', 'abuse'),\n",
    "    ('genderbased', 'abuse'),\n",
    "    ('gender-based', 'abuse'),\n",
    "    ('sexual', 'assault'),\n",
    "    ('genderbased', 'assault'),\n",
    "    ('gender-based', 'assault'),\n",
    "    ('sexual', 'assault'),\n",
    "    ('sexual', 'abuse'),\n",
    "    ('sexual', 'exploitation'),\n",
    "    ('sexual', 'harassment'),\n",
    "    ('sexual', 'gender-based'),\n",
    "    ('sexual', 'assaults'),\n",
    "    ('sexual', 'exploitation'),\n",
    "    ('integrity', 'abuse'),\n",
    "    ('integrity', 'abuses'),\n",
    "    ('integrity', 'attack'),\n",
    "    ('integrity', 'attacks'),\n",
    "    ('taking', 'over', 'the', 'integrity'),\n",
    "    ('victims', 'of', 'infringement'),\n",
    "    ('integrity', 'damage'),\n",
    "    ('bacha', 'bazi'),\n",
    "    ('teen', 'pregnancies'),\n",
    "    ('forced', 'marriage'),\n",
    "    ('forced', 'marriages'),\n",
    "    ('domestic', 'violence'),\n",
    "    ('victims', 'of', 'sexual'),\n",
    "    ('intimate', 'partner', 'violence'),\n",
    "    ('violence', 'against', 'women'),\n",
    "    ('violence', 'against', 'girls'),\n",
    "    ('abuse', 'and', 'sexual'),\n",
    "    ('forced', 'into', 'child', 'marriage'),\n",
    "    ('sexual', 'violence'),\n",
    "    ('sexual', 'assault'),\n",
    "    ('sexual', 'exploitation'),\n",
    "    ('sexual', 'abuse'),\n",
    "    ('sexual', 'maltreatment'),\n",
    "    ('emotional', 'maltreatment'),\n",
    "    ('gender', 'based', 'violence'),\n",
    "    ('female', 'genital', 'mutilation'),\n",
    "    ('female', 'genital', 'mutilations'),\n",
    "]\n",
    "\n",
    "\n",
    "def lf_pos_gbv_1(row):\n",
    "    if len({'women', 'woman', 'girls', 'girl', 'female', 'females'}\n",
    "           & row[\"tokenized_excerpt\"]) and len({\n",
    "               'violence', 'exploitation', 'exploited', 'assault', 'abuse',\n",
    "               'abuses', 'maltreatment', 'maltreatments', 'survivors',\n",
    "               'survivor', 'harassment', 'harassments'\n",
    "               'discrimination', 'discriminations', 'trafficking',\n",
    "               'aggressions', 'aggression'\n",
    "           } & row[\"tokenized_excerpt\"]):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def lf_pos_gbv_2(row):\n",
    "    if len({'sex', 'sexual', 'sexually'}\n",
    "           & row[\"tokenized_excerpt\"]) and len({\n",
    "               'violence', 'exploitation', 'exploited', 'assault', 'abuse',\n",
    "               'abuses', 'maltreatment', 'maltreatments', 'survivors',\n",
    "               'survivor', 'harassment', 'harassments', 'aggressions',\n",
    "               'aggression'\n",
    "           }\n",
    "                                               & row[\"tokenized_excerpt\"]):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def lf_pos_gbv_3(row):\n",
    "    if len({'survivor', 'survivors'}\n",
    "           & row[\"tokenized_excerpt\"]) and len({\n",
    "               'violence', 'exploitation', 'exploited', 'assault', 'abuse',\n",
    "               'abuses', 'maltreatment', 'maltreatments', 'harassment',\n",
    "               'harassments', 'trafficking', 'aggressions', 'aggression'\n",
    "           }\n",
    "                                               & row[\"tokenized_excerpt\"]):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def lf_pos_gbv_4(row):\n",
    "    # gbv-*\n",
    "    if re.search(r\"\\bgbv\\-?.+?\\b\", row['excerpt_pp']):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "##\n",
    "negative_kw_gbv = []\n",
    "##\n",
    "gbv_patterns = []\n",
    "\n",
    "\n",
    "##\n",
    "def is_gbv(row):\n",
    "    for kw in negative_kw_gbv:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return False\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return False\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return False\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return False\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return False\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return False\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return False\n",
    "    for p in gbv_patterns:\n",
    "        if re.search(p, row[\"excerpt_pp\"]): return True\n",
    "    for lf in [lf_pos_gbv_1, lf_pos_gbv_2, lf_pos_gbv_3, lf_pos_gbv_4]:\n",
    "        if lf(row) == 1:\n",
    "            return True\n",
    "    for kw in kw_en_gbv:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return True\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return True\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return True\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return True\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return True\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return True\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4756edf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:33.127423Z",
     "start_time": "2022-01-17T20:43:33.091476Z"
    }
   },
   "outputs": [],
   "source": [
    "kw_en_unaccompanied_child = [\n",
    "    ('unaccompanied', 'nna'),\n",
    "    ('nna', 'unaccompanied'),\n",
    "    ('nnas', 'unaccompanied'),\n",
    "    'orphans',\n",
    "    ('orphan', 'children'),\n",
    "    ('orphaned', 'children'),\n",
    "    ('separated', 'nna'),\n",
    "    ('separated', 'nnas'),\n",
    "    ('nna', 'separated'),\n",
    "    ('nnas', 'separated'),\n",
    "    ('separate', 'nna'),\n",
    "    ('separate', 'nnas'),\n",
    "    ('nna', 'separate'),\n",
    "    ('nnas', 'separate'),\n",
    "    ('undecreamed', 'minors'),\n",
    "    ('children', 'alone'),\n",
    "    ('unaccompanied', 'children'),\n",
    "    ('unaccochildrenmpanied', 'unaccompanied'),\n",
    "    ('non-accompanied', 'children'),\n",
    "    ('unaccompanied', 'minors'),\n",
    "    ('non-accompanied', 'minors'),\n",
    "    ('minors', 'unaccompanied'),\n",
    "    ('separated', 'children'),\n",
    "    ('children', 'separated'),\n",
    "    ('separate', 'children'),\n",
    "    ('children', 'separate'),\n",
    "    ('separated', 'minors'),\n",
    "    ('minors', 'separated'),\n",
    "    ('separate', 'minors'),\n",
    "    ('minors', 'separate'),\n",
    "    ('separation', 'of', 'children'),\n",
    "    ('street', 'children'),\n",
    "    ('abandoning', 'children'),\n",
    "    ('abandoned', 'children'),\n",
    "    ('deprived', 'of', 'their', 'childhood'),\n",
    "    ('deprived', 'of', 'their', 'adolescence'),\n",
    "    ('wish', 'to', 'find', 'their', 'families'),\n",
    "    'uasc',  # unaccomapnied or asylum-seeking children\n",
    "]\n",
    "\n",
    "\n",
    "def lf_pos_unaccompanied_child_1(row):\n",
    "    if len({\n",
    "            'children', 'child', 'girls', 'boys', 'adolescents', 'nna',\n",
    "            'minor', 'minors', 'teen', 'teens', 'nna', 'nnas'\n",
    "    }\n",
    "           & row[\"tokenized_excerpt\"]) and len({\n",
    "               'separated', 'unaccompanied', 'non-accompanied',\n",
    "               'not-accompanied', 'unaccompanied/separated',\n",
    "               'separated/unaccompanied', 'household/unaccompanied',\n",
    "               'unaccompanied/household', 'orphan', 'orphans', 'orphaned'\n",
    "           } & row[\"tokenized_excerpt\"]):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def lf_pos_unaccompanied_child_2(row):\n",
    "    if len({\n",
    "            'children', 'child', 'girls', 'boys', 'adolescents', 'nna',\n",
    "            'minor', 'minors', 'teen', 'teens', 'nna', 'nnas'\n",
    "    }\n",
    "           & row[\"tokenized_excerpt\"]) and len({\n",
    "               ('no', 'accompaniment'),\n",
    "               ('no', 'accompanied'),\n",
    "               ('arrived', 'alone'),\n",
    "               ('family', 'separation'),\n",
    "               ('family', 'separations'),\n",
    "               ('outside', 'family'),\n",
    "               ('not', 'accompanied'),\n",
    "               ('live', 'alone'),\n",
    "               ('without', 'parents'),\n",
    "           } & row[\"bigram_excerpt\"]):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def lf_pos_unaccompanied_child_3(row):\n",
    "    if len({\n",
    "            'children', 'child', 'girls', 'boys', 'adolescents', 'nna',\n",
    "            'minor', 'minors', 'teen', 'teens', 'nna', 'nnas'\n",
    "    }\n",
    "           & row[\"tokenized_excerpt\"]) and len({\n",
    "               ('without', 'the', 'company'),\n",
    "               ('without', 'their', 'parents'),\n",
    "               ('without', 'their', 'accompanying'),\n",
    "               ('without', 'a', 'company'),\n",
    "               ('outside', 'the', 'family'),\n",
    "               ('loss', 'of', 'parents'),\n",
    "               ('loss', 'of', 'guardians'),\n",
    "               ('outside', 'of', 'family'),\n",
    "               ('lost', 'their', 'family'),\n",
    "               ('lost', 'their', 'parents'),\n",
    "               ('without', 'a', 'parent'),\n",
    "               ('without', 'a', 'guardian'),\n",
    "               ('live', 'with', 'none'),\n",
    "           } & row[\"trigram_excerpt\"]):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def lf_pos_unaccompanied_child_4(row):\n",
    "    if len({\n",
    "            'children', 'child', 'girls', 'boys', 'adolescents', 'nna',\n",
    "            'minor', 'minors', 'teen', 'teens', 'nna', 'nnas'\n",
    "    }\n",
    "           & row[\"tokenized_excerpt\"]) and len({\n",
    "               ('without', 'their', 'legal', 'learners'),\n",
    "               ('without', 'their', 'legal', 'learner'),\n",
    "               ('without', 'their', 'legal', 'counseling'),\n",
    "               ('separated', 'from', 'both', 'parents'),\n",
    "               ('not', 'live', 'with', 'relatives'),\n",
    "               ('separated', 'from', 'their', 'families'),\n",
    "           } & row[\"fourgram_excerpt\"]):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def lf_pos_unaccompanied_child_5(row):\n",
    "    if len({\n",
    "            'children', 'child', 'girls', 'boys', 'adolescents', 'nna',\n",
    "            'minor', 'minors', 'teen', 'teens', 'nna', 'nnas'\n",
    "    }\n",
    "           & row[\"tokenized_excerpt\"]) and is_stateless(row):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def lf_pos_unaccompanied_child_6(row):  # to be used for now\n",
    "    if len({\n",
    "            'children', 'child', 'girls', 'boys', 'adolescents', 'nna',\n",
    "            'minor', 'minors', 'teen', 'teens', 'nna', 'nnas'\n",
    "    }\n",
    "           & row[\"tokenized_excerpt\"]) and len({\n",
    "               ('armed', 'groups'),\n",
    "               ('armed', 'group'),\n",
    "               ('armed', 'forces'),\n",
    "               ('armed', 'force'),\n",
    "           } & row[\"bigram_excerpt\"]) and len(\n",
    "               {'recruitment', 'recruitments', 'recruited'}\n",
    "               & row[\"tokenized_excerpt\"]):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "##\n",
    "negative_kw_unaccompanied_child = []\n",
    "##\n",
    "unaccompanied_child_patterns = []\n",
    "\n",
    "\n",
    "##\n",
    "def is_unaccompanied_child(row):\n",
    "    for kw in negative_kw_unaccompanied_child:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return False\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return False\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return False\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return False\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return False\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return False\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return False\n",
    "    for p in unaccompanied_child_patterns:\n",
    "        if re.search(p, row[\"excerpt_pp\"]): return True\n",
    "    for lf in [\n",
    "            lf_pos_unaccompanied_child_1,\n",
    "            lf_pos_unaccompanied_child_2,\n",
    "            lf_pos_unaccompanied_child_3,\n",
    "            lf_pos_unaccompanied_child_4,\n",
    "            #lf_pos_unaccompanied_child_5,\n",
    "    ]:\n",
    "        if lf(row) == 1:\n",
    "            return True\n",
    "    for kw in kw_en_unaccompanied_child:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return True\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return True\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return True\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return True\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return True\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return True\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d32985f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:33.962830Z",
     "start_time": "2022-01-17T20:43:33.915344Z"
    }
   },
   "outputs": [],
   "source": [
    "kw_en_chronically_ill = [\n",
    "    'hiv',\n",
    "    'hypertension',\n",
    "    'cancer',\n",
    "    'aids',\n",
    "    'dpvih',\n",
    "    'asthma',\n",
    "    'dialysis',\n",
    "    'tuberculosis',\n",
    "    'tb',\n",
    "    'diabetic',\n",
    "    'dialyzed',\n",
    "    'ncds',\n",
    "    'arthritis',\n",
    "    'plhiv',\n",
    "    #'paralyzed',\n",
    "    'immunodeficiency',\n",
    "    'immunodeficient',\n",
    "    'hypertensive',\n",
    "    'hepatitis',\n",
    "    'chemotherapy',\n",
    "    'parkinson',\n",
    "    'hiv-aids',\n",
    "    'hiv/aids',\n",
    "    'schizophrenia',\n",
    "    'nephropathy',\n",
    "    'neoplasm',\n",
    "    'rheumatism',\n",
    "    'osteoporosis',\n",
    "    'rheumatic',\n",
    "    'snyder-robinson',\n",
    "    'leukemia',\n",
    "    'immunodeficiencies',\n",
    "    'pvvih',\n",
    "    'pvvs',\n",
    "    'transplantation',\n",
    "    'transplant',\n",
    "    'transplants',\n",
    "    'transplanted',\n",
    "    'poliomyelitis',\n",
    "    'marasmus',\n",
    "    'tbc',\n",
    "    ('prostatic', 'hyperplasia'),\n",
    "    ('prostate', 'hyperplasia'),\n",
    "    ('snyder', 'robinson'),\n",
    "    ('robinson', 'syndrome'),\n",
    "    ('chronic', 'psychiatric', 'disorders'),\n",
    "    ('chronic', 'psychiatric', 'disorder'),\n",
    "    ('chronic', 'psychological', 'disorders'),\n",
    "    ('chronic', 'psychological', 'disorder'),\n",
    "    ('chronic', 'physical', 'disorders'),\n",
    "    ('chronic', 'physical', 'disorder'),\n",
    "    ('permanent', 'psychiatric', 'disorders'),\n",
    "    ('permanent', 'psychiatric', 'disorder'),\n",
    "    ('permanent', 'psychological', 'disorders'),\n",
    "    ('permanent', 'psychological', 'disorder'),\n",
    "    ('permanent', 'physical', 'disorders'),\n",
    "    ('permanent', 'physical', 'disorder'),\n",
    "    ('chronic', 'diseases'),\n",
    "    ('chronic', 'disease'),\n",
    "    ('chronic', 'illnesses'),\n",
    "    ('chronic', 'illness'),\n",
    "    ('permanent', 'diseases'),\n",
    "    ('permanent', 'disease'),\n",
    "    ('permanent', 'illnesses'),\n",
    "    ('permanent', 'illness'),\n",
    "    ('chronic', 'sick'),\n",
    "    ('chronic', 'ill'),\n",
    "    ('chronically', 'sick'),\n",
    "    ('chronically', 'ill'),\n",
    "    ('chronic', 'sickness'),\n",
    "    ('chronic', 'sicknesses'),\n",
    "    ('permanent', 'vulnerabilities'),\n",
    "    ('permanent', 'vulnerability'),\n",
    "    ('chronic', 'malnutrition'),\n",
    "    ('chronic', 'conditions'),\n",
    "    ('chronic', 'health', 'conditions'),\n",
    "    ('chronic', 'health', 'issues'),\n",
    "    ('chronic', 'medical', 'health', 'condition'),\n",
    "    ('chronic', 'medical', 'health', 'conditions'),\n",
    "    ('chronic', 'medical', 'condition'),\n",
    "    ('chronic', 'medical', 'conditions'),\n",
    "    ('chronic', 'patients'),\n",
    "    'paraling',\n",
    "    'diabetics',\n",
    "    'diabetologist',\n",
    "    'obese',\n",
    "    'diabetes',\n",
    "    'infarction',\n",
    "    'hiv/aids',\n",
    "    'unaids',\n",
    "    'obesity',\n",
    "    ('require', 'daily', 'medicine'),\n",
    "    ('people', 'require', 'medications', 'daily'),\n",
    "    ('retrooviral', 'treatment'),\n",
    "    ('retroviral', 'therapy'),\n",
    "    ('antiretroviral', 'treatment'),\n",
    "    ('gastrointestinal', 'diseases'),\n",
    "    ('acute', 'respiratory', 'diseases'),\n",
    "    ('gestational', 'syphilis'),\n",
    "    ('congenital', 'syphilis'),\n",
    "    ('maternal', 'morbidity'),\n",
    "    ('long-term', 'treatment'),\n",
    "    ('daily', 'medication'),\n",
    "    ('acute', 'malnutrition'),\n",
    "    ('medical', 'or', 'chronic', 'condition'),\n",
    "    'acv',\n",
    "    ('daily', 'medical', 'attention'),\n",
    "    ('medications', 'daily'),\n",
    "    ('daily', 'medications'),\n",
    "    # 'art',\n",
    "    ## infectious and serious illnesses\n",
    "    ('advanced', 'disease'),\n",
    "    ('pre-existing', 'diseases'),\n",
    "    'contagion',\n",
    "    ('infectious', 'diseases'),\n",
    "    ('contagious', 'diseases'),\n",
    "    ('critical', 'medical', 'conditions'),\n",
    "    ('critical', 'medical', 'condition'),\n",
    "    ('serious', 'medical', 'condition'),\n",
    "    ('serious', 'medical', 'conditions'),\n",
    "    'dengue',\n",
    "    'malaria',\n",
    "    ('conditions', 'that', 'increase', 'the', 'risks', 'of', 'mortality'),\n",
    "    ('illness', 'which', 'lasted', '3', 'months', 'or', 'longer'),\n",
    "    'hernias',\n",
    "    ('joint', 'pain'),\n",
    "    ('affected', 'medical', 'problems'),\n",
    "    ('stroke', 'of', 'ischemic', 'type'),\n",
    "    'cholera',\n",
    "    ('persons', 'with', 'mental', 'health', 'problems'),\n",
    "    ('persons', 'with', 'mental', 'health', 'problem'),\n",
    "    ('persons', 'with', 'mental', 'health', 'disorders'),\n",
    "    ('persons', 'with', 'mental', 'health', 'disorder'),\n",
    "    ('children', 'with', 'mental', 'health', 'problems'),\n",
    "    ('children', 'with', 'mental', 'health', 'problem'),\n",
    "    ('children', 'with', 'mental', 'health', 'disorders'),\n",
    "    ('children', 'with', 'mental', 'health', 'disorder'),\n",
    "    ('girls', 'with', 'mental', 'health', 'problems'),\n",
    "    ('girls', 'with', 'mental', 'health', 'problem'),\n",
    "    ('girls', 'with', 'mental', 'health', 'disorders'),\n",
    "    ('girls', 'with', 'mental', 'health', 'disorder'),\n",
    "    ('boys', 'with', 'mental', 'health', 'problems'),\n",
    "    ('boys', 'with', 'mental', 'health', 'problem'),\n",
    "    ('boys', 'with', 'mental', 'health', 'disorders'),\n",
    "    ('boys', 'with', 'mental', 'health', 'disorder'),\n",
    "    ('youths', 'with', 'mental', 'health', 'problems'),\n",
    "    ('youths', 'with', 'mental', 'health', 'problem'),\n",
    "    ('youths', 'with', 'mental', 'health', 'disorders'),\n",
    "    ('youths', 'with', 'mental', 'health', 'disorder'),\n",
    "    ('women', 'with', 'mental', 'health', 'problems'),\n",
    "    ('women', 'with', 'mental', 'health', 'problem'),\n",
    "    ('women', 'with', 'mental', 'health', 'disorders'),\n",
    "    ('women', 'with', 'mental', 'health', 'disorder'),\n",
    "    ('men', 'with', 'mental', 'health', 'problems'),\n",
    "    ('men', 'with', 'mental', 'health', 'problem'),\n",
    "    ('men', 'with', 'mental', 'health', 'disorders'),\n",
    "    ('men', 'with', 'mental', 'health', 'disorder'),\n",
    "    ('elderly', 'with', 'mental', 'health', 'problems'),\n",
    "    ('elderly', 'with', 'mental', 'health', 'problem'),\n",
    "    ('elderly', 'with', 'mental', 'health', 'disorders'),\n",
    "    ('elderly', 'with', 'mental', 'health', 'disorder'),\n",
    "    ('people', 'with', 'mental', 'health', 'problems'),\n",
    "    ('people', 'with', 'mental', 'health', 'problem'),\n",
    "    ('people', 'with', 'mental', 'health', 'disorders'),\n",
    "    ('people', 'with', 'mental', 'health', 'disorder'),\n",
    "    ('peoples', 'with', 'mental', 'health', 'problems'),\n",
    "    ('peoples', 'with', 'mental', 'health', 'problem'),\n",
    "    ('peoples', 'with', 'mental', 'health', 'disorders'),\n",
    "    ('peoples', 'with', 'mental', 'health', 'disorder'),\n",
    "    ('chronic', 'psychiatric', 'disorders'),\n",
    "    ('chronic', 'psychological', 'disorders'),\n",
    "    ('suffering', 'from', 'mental', 'disorders'),\n",
    "    ('suffering', 'from', 'mental', 'illnesses'),\n",
    "    ('suffering', 'from', 'mental', 'illness'),\n",
    "    ('chronic', 'psychiatric', 'disorders'),\n",
    "    ('chronic', 'psychiatric', 'disorder'),\n",
    "]\n",
    "\n",
    "\n",
    "def lf_chronically_ill_pos_1(row):\n",
    "    if ('serious', 'chronic', 'conditions') in row[\"trigram_excerpt\"] or (\n",
    "        ('chronic', 'illness') in row[\"bigram_excerpt\"] or\n",
    "        ('chronic', 'disorders') in row[\"bigram_excerpt\"] or\n",
    "        ('transplanted', 'persons') in row[\"bigram_excerpt\"]):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def lf_chronically_ill_pos_2(row):\n",
    "    if row[\"lang\"] == \"fr\" and (\n",
    "        ('mental', 'patients') in row[\"bigram_excerpt\"] or\n",
    "        ('mental', 'sick', 'women', 'and', 'girls') in row[\"fivegram_excerpt\"]\n",
    "            or ('mental', 'women', 'and', 'girls') in row[\"fourgram_excerpt\"]):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def lf_chronically_ill_pos_3(row):\n",
    "    if row[\"lang\"] == \"es\" and (\n",
    "        ('serious', 'chronic', 'conditions') in row[\"trigram_excerpt\"] or\n",
    "        ('chronic', 'illness') in row[\"bigram_excerpt\"] or\n",
    "        ('chronic', 'disorders') in row[\"bigram_excerpt\"] or\n",
    "        ('transplanted', 'persons') in row[\"bigram_excerpt\"]):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "##\n",
    "negative_kw_chronically_ill = []\n",
    "##\n",
    "chronically_ill_patterns = []\n",
    "\n",
    "\n",
    "##\n",
    "def is_chronically_ill(row):\n",
    "    for kw in negative_kw_chronically_ill:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return False\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return False\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return False\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return False\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return False\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return False\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return False\n",
    "    for p in chronically_ill_patterns:\n",
    "        if re.search(p, row[\"excerpt_pp\"]): return True\n",
    "    for lf in [\n",
    "            lf_chronically_ill_pos_1,\n",
    "            lf_chronically_ill_pos_2,\n",
    "            lf_chronically_ill_pos_3,\n",
    "    ]:\n",
    "        if lf(row) == 1:\n",
    "            return True\n",
    "    for kw in kw_en_chronically_ill:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return True\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return True\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return True\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return True\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return True\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return True\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9b9a4cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:34.733087Z",
     "start_time": "2022-01-17T20:43:34.702801Z"
    }
   },
   "outputs": [],
   "source": [
    "kw_en_fhh = ['fhh', 'fhhs', ('women', 's', 'household', 'women')]\n",
    "\n",
    "\n",
    "def lf_fhh_pos_1(row):\n",
    "    if len({\n",
    "        ('female', 'headed'),\n",
    "        ('female', 'led'),\n",
    "        ('women', 'led'),\n",
    "        ('women', 'headed'),\n",
    "        ('woman', 'led'),\n",
    "        ('woman', 'headed'),\n",
    "        ('female-', 'headed'),\n",
    "        ('female-', 'led'),\n",
    "        ('women-', 'led'),\n",
    "        ('women-', 'headed'),\n",
    "        ('woman-', 'led'),\n",
    "        ('woman-', 'headed'),\n",
    "        ('women', 'run'),\n",
    "        ('woman', 'run'),\n",
    "        ('female', 'run'),\n",
    "        ('females', 'run'),\n",
    "    }\n",
    "           & row['bigram_excerpt']) and len({'households', 'household'}\n",
    "                                            & row['tokenized_excerpt']):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def lf_fhh_pos_2(row):\n",
    "    if len({\n",
    "            'female-headed', 'female-led', 'women-led', 'women-headed',\n",
    "            'woman-headed', 'femaleheaded', 'femaleled', 'womenled',\n",
    "            'womenheaded', 'womanheaded', 'womenrun', 'womanrun', 'femalerun',\n",
    "            'femalesrun'\n",
    "    }\n",
    "           & row['tokenized_excerpt']) and len({'households', 'household'}\n",
    "                                               & row['tokenized_excerpt']):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def lf_fhh_pos_3(row):\n",
    "    if len({'female', 'females', 'woman', 'women'}\n",
    "           & row['tokenized_excerpt']) and len(\n",
    "               {\n",
    "                   'headed',\n",
    "                   'head',\n",
    "                   'heads',\n",
    "                   'headquarters',\n",
    "                   'headquarter',\n",
    "                   'led',\n",
    "                   'supplier',\n",
    "                   'suppliers',\n",
    "                   'chefts',\n",
    "                   'chiefs',\n",
    "                   'chief',\n",
    "                   'cheffes',\n",
    "                   'cheffes',\n",
    "                   'breadwinner',\n",
    "                   'breadwinners',\n",
    "                   'headache',\n",
    "               }\n",
    "               & row['tokenized_excerpt']\n",
    "           ) and len({\n",
    "               'households', 'household', 'family', 'families', 'home', 'homes'\n",
    "           }\n",
    "                     & row['tokenized_excerpt']):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def lf_fhh_pos_4(row):\n",
    "    if fhh in row[\"specific_needs_groups\"]:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "##\n",
    "negative_kw_fhh = []\n",
    "##\n",
    "fhh_patterns = []\n",
    "\n",
    "\n",
    "##\n",
    "def is_fhh(row):\n",
    "    for kw in negative_kw_fhh:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return False\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return False\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return False\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return False\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return False\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return False\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return False\n",
    "    for p in fhh_patterns:\n",
    "        if re.search(p, row[\"excerpt_pp\"]): return True\n",
    "    for lf in [lf_fhh_pos_1, lf_fhh_pos_2, lf_fhh_pos_3, lf_fhh_pos_4]:\n",
    "        if lf(row) == 1:\n",
    "            return True\n",
    "    for kw in kw_en_fhh:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return True\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return True\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return True\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return True\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return True\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return True\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c04b1a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:35.503306Z",
     "start_time": "2022-01-17T20:43:35.471667Z"
    }
   },
   "outputs": [],
   "source": [
    "kw_en_single_women = [\n",
    "    'widows',\n",
    "    'widow',\n",
    "    'widowed',\n",
    "    'cpf',\n",
    "    'widowhood',\n",
    "    'wrc',\n",
    "    'widowers',\n",
    "    'widower',\n",
    "    'unmarried',\n",
    "    'single-parent',\n",
    "    'singleparent',\n",
    "    'single-mother',\n",
    "    'single-mothers',\n",
    "    'singlemother',\n",
    "    'singlemothers',\n",
    "    ('single', 'women'),\n",
    "    ('single', 'mothers'),\n",
    "    ('single', 'mother'),\n",
    "    ('single', 'woman'),\n",
    "    ('single', 'female'),\n",
    "    ('single', 'females'),\n",
    "    ('women', 'traveling', 'alone'),\n",
    "    ('woman', 'traveling', 'alone'),\n",
    "    ('unmarried', 'women'),\n",
    "    ('unmarried', 'women'),\n",
    "    ('unmarried', 'females'),\n",
    "    ('unmarried', 'female'),\n",
    "    ('females', 'traveling', 'alone'),\n",
    "    ('female', 'traveling', 'alone'),\n",
    "    ('women', 'living', 'alone'),\n",
    "    ('woman', 'living', 'alone'),\n",
    "    ('females', 'living', 'alone'),\n",
    "    ('female', 'living', 'alone'),\n",
    "    ('divorced', 'or', 'separated'),\n",
    "    ('women-only', 'places'),\n",
    "    ('womenonly', 'places'),\n",
    "    ('women', 'only', 'places'),\n",
    "    ('female-only', 'places'),\n",
    "    ('femaleonly', 'places'),\n",
    "    ('female', 'only', 'places'),\n",
    "    ('females-only', 'places'),\n",
    "    ('femalesonly', 'places'),\n",
    "    ('females', 'only', 'places'),\n",
    "]\n",
    "\n",
    "\n",
    "def lf_single_women_1(row):\n",
    "    if single_women in row['specific_needs_groups']:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "##\n",
    "negative_kw_single_women = []\n",
    "##\n",
    "single_women_patterns = []\n",
    "\n",
    "\n",
    "##\n",
    "def is_single_women(row):\n",
    "    for kw in negative_kw_single_women:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return False\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return False\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return False\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return False\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return False\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return False\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return False\n",
    "    for p in single_women_patterns:\n",
    "        if re.search(p, row[\"excerpt_pp\"]): return True\n",
    "    for lf in [lf_single_women_1]:\n",
    "        if lf(row) == 1:\n",
    "            return True\n",
    "    for kw in kw_en_single_women:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return True\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return True\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return True\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return True\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return True\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return True\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb6d3b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:36.145843Z",
     "start_time": "2022-01-17T20:43:36.125464Z"
    }
   },
   "outputs": [],
   "source": [
    "# when the labels of a tag are good enough, I usually preserve the original labels to\n",
    "# avoid lowering the precision caused by noisy keywords I extract\n",
    "kw_en_lgbt = []\n",
    "\n",
    "\n",
    "def lf_lgbt_1(row):\n",
    "    if lgbt in row['specific_needs_groups']:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "##\n",
    "negative_kw_lgbt = []\n",
    "##\n",
    "lgbt_patterns = []\n",
    "\n",
    "\n",
    "##\n",
    "def is_lgbt(row):\n",
    "    for kw in negative_kw_lgbt:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return False\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return False\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return False\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return False\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return False\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return False\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return False\n",
    "    for p in lgbt_patterns:\n",
    "        if re.search(p, row[\"excerpt_pp\"]): return True\n",
    "    for lf in [lf_lgbt_1]:\n",
    "        if lf(row) == 1:\n",
    "            return True\n",
    "    for kw in kw_en_lgbt:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return True\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return True\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return True\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return True\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return True\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return True\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2b4a58e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:36.925949Z",
     "start_time": "2022-01-17T20:43:36.894527Z"
    }
   },
   "outputs": [],
   "source": [
    "kw_en_chh = [\n",
    "    'chh',\n",
    "    'chhs'\n",
    "    'child-headed',\n",
    "    'childheaded',\n",
    "    'children-headed',\n",
    "    'childrenheaded',\n",
    "    ('children', 'heads'),\n",
    "    ('child', 'heads'),\n",
    "    ('children', 'headed'),\n",
    "    ('child', 'household'),\n",
    "    ('child', 'hh'),\n",
    "    ('children', 'hh'),\n",
    "    ('child', 'headed'),\n",
    "    ('children', 'headed'),\n",
    "    ('child', 'headed'),\n",
    "    ('children', 'headed'),\n",
    "]\n",
    "\n",
    "\n",
    "def lf_chh_pos_1(row):\n",
    "    if len({\n",
    "        ('child', 'headed'),\n",
    "        ('child-', 'headed'),\n",
    "        ('child', 'led'),\n",
    "        ('child-', 'led'),\n",
    "        ('child', 'run'),\n",
    "        ('child-', 'run'),\n",
    "        ('children', 'headed'),\n",
    "        ('children-', 'headed'),\n",
    "        ('children', 'led'),\n",
    "        ('children-', 'led'),\n",
    "        ('children', 'run'),\n",
    "        ('children-', 'run'),\n",
    "        ('adolescent', 'headed'),\n",
    "        ('adolescent-', 'headed'),\n",
    "        ('adolescent', 'led'),\n",
    "        ('adolescent-', 'led'),\n",
    "        ('adolescent', 'run'),\n",
    "        ('adolescent-', 'run'),\n",
    "        ('adolescents', 'headed'),\n",
    "        ('adolescents-', 'headed'),\n",
    "        ('adolescents', 'led'),\n",
    "        ('adolescents-', 'led'),\n",
    "        ('adolescents', 'run'),\n",
    "        ('adolescents-', 'run'),\n",
    "    }\n",
    "           & row['bigram_excerpt']) and len({'households', 'household', 'family', 'families', 'home', 'homes'}\n",
    "                                            & row['tokenized_excerpt']):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def lf_chh_pos_2(row):\n",
    "    if len({\n",
    "            'child-headed', 'child-led', 'child-run', 'childheaded',\n",
    "            'childled', 'childrun', 'adolescent-headed', 'adolescent-led',\n",
    "            'adolescent-run', 'adolescentheaded', 'adolescentled',\n",
    "            'adolescentrun', 'adolescents-headed', 'adolescents-led',\n",
    "            'adolescents-run', 'adolescentsheaded', 'adolescentsled',\n",
    "            'adolescentsrun', 'children-headed', 'children-led',\n",
    "            'children-run', 'childrenheaded', 'childrenled', 'childrenrun'\n",
    "    }\n",
    "           & row['tokenized_excerpt']) and len({'households', 'household', 'family', 'families', 'home', 'homes'}\n",
    "                                               & row['tokenized_excerpt']):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def lf_chh_pos_3(row):\n",
    "    if len({'child', 'children', 'adolescent', 'adolescents'}\n",
    "           & row['tokenized_excerpt']) and len(\n",
    "               {\n",
    "                   'headed',\n",
    "                   'head',\n",
    "                   'heads',\n",
    "                   'headquarters',\n",
    "                   'headquarter',\n",
    "                   'led',\n",
    "                   'supplier',\n",
    "                   'suppliers',\n",
    "                   'chefts',\n",
    "                   'chiefs',\n",
    "                   'chief',\n",
    "                   'cheffes',\n",
    "                   'cheffes',\n",
    "                   'breadwinner',\n",
    "                   'breadwinners',\n",
    "                   'headache',\n",
    "               }\n",
    "               & row['tokenized_excerpt']\n",
    "           ) and len({\n",
    "               'households', 'household', 'family', 'families', 'home', 'homes'\n",
    "           }\n",
    "                     & row['tokenized_excerpt']):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def lf_chh_pos_4(row):\n",
    "    if chh in row['specific_needs_groups']:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "##\n",
    "negative_kw_chh = []\n",
    "##\n",
    "chh_patterns = []\n",
    "\n",
    "\n",
    "##\n",
    "def is_chh(row):\n",
    "    for kw in negative_kw_chh:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return False\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return False\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return False\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return False\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return False\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return False\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return False\n",
    "    for p in chh_patterns:\n",
    "        if re.search(p, row[\"excerpt_pp\"]): return True\n",
    "    for lf in [lf_chh_pos_1, lf_chh_pos_2, lf_chh_pos_4]:\n",
    "        if lf(row) == 1:\n",
    "            return True\n",
    "    for kw in kw_en_chh:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return True\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return True\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return True\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return True\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return True\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return True\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f8942d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:38.297225Z",
     "start_time": "2022-01-17T20:43:38.271121Z"
    }
   },
   "outputs": [],
   "source": [
    "kw_en_ehh = [\n",
    "    'ehh',\n",
    "    'ehhs',\n",
    "    'elderly-headed',\n",
    "    'elderlyheaded',\n",
    "    'retiree-headed',\n",
    "    'retireeheaded',\n",
    "    'retirees-headed',\n",
    "    'retireesheaded',\n",
    "    'pensioner-headed',\n",
    "    'pensionerheaded',\n",
    "    'pensioners-headed',\n",
    "    'pensionersheaded',\n",
    "    'elderly-led',\n",
    "    'elderlyled',\n",
    "    'retiree-led',\n",
    "    'retireeled',\n",
    "    'retirees-led',\n",
    "    'retireesled',\n",
    "    'pensioner-led',\n",
    "    'pensionerled',\n",
    "    'pensioners-led',\n",
    "    'pensionersled',\n",
    "    'elderly-run',\n",
    "    'elderlyrun',\n",
    "    'retiree-run',\n",
    "    'retireerun',\n",
    "    'retirees-run',\n",
    "    'retireesrun',\n",
    "    'pensioner-run',\n",
    "    'pensionerrun',\n",
    "    'pensioners-run',\n",
    "    'pensionersrun',\n",
    "    ('elderly', 'headed'),\n",
    "    ('retiree', 'headed'),\n",
    "    ('retirees', 'headed'),\n",
    "    ('pensioner', 'headed'),\n",
    "    ('pensioners', 'headed'),\n",
    "    ('elderly', 'led'),\n",
    "    ('retiree', 'led'),\n",
    "    ('retirees', 'led'),\n",
    "    ('pensioner', 'led'),\n",
    "    ('pensioners', 'led'),\n",
    "    ('elderly', 'run'),\n",
    "    ('retiree', 'run'),\n",
    "    ('retirees', 'run'),\n",
    "    ('pensioner', 'run'),\n",
    "    ('pensioners', 'run'),\n",
    "]\n",
    "\n",
    "\n",
    "def lf_ehh_pos_1(row):\n",
    "    if len({'elderly', 'retiree', 'retirees', 'pensioner', 'pensioners'}\n",
    "           & row['tokenized_excerpt']) and len(\n",
    "               {\n",
    "                   'headed',\n",
    "                   'head',\n",
    "                   'heads',\n",
    "                   'headquarters',\n",
    "                   'headquarter',\n",
    "                   'led',\n",
    "                   'run',\n",
    "                   'supplier',\n",
    "                   'suppliers',\n",
    "                   'chefts',\n",
    "                   'chiefs',\n",
    "                   'chief',\n",
    "                   'cheffes',\n",
    "                   'cheffes',\n",
    "                   'breadwinner',\n",
    "                   'breadwinners',\n",
    "                   'headache',\n",
    "               }\n",
    "               & row['tokenized_excerpt']\n",
    "           ) and len({\n",
    "               'households', 'household', 'family', 'families', 'home', 'homes'\n",
    "           }\n",
    "                     & row['tokenized_excerpt']):\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def lf_ehh_pos_2(row):\n",
    "    if ehh in row['specific_needs_groups']:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "##\n",
    "negative_kw_ehh = []\n",
    "##\n",
    "ehh_patterns = []\n",
    "\n",
    "\n",
    "##\n",
    "def is_ehh(row):\n",
    "    for kw in negative_kw_ehh:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return False\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return False\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return False\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return False\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return False\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return False\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return False\n",
    "    for p in ehh_patterns:\n",
    "        if re.search(p, row[\"excerpt_pp\"]): return True\n",
    "    for lf in [lf_ehh_pos_2]:\n",
    "        if lf(row) == 1:\n",
    "            return True\n",
    "    for kw in kw_en_ehh:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return True\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return True\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return True\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return True\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return True\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return True\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb008d12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:39.196529Z",
     "start_time": "2022-01-17T20:43:39.188303Z"
    }
   },
   "outputs": [],
   "source": [
    "kw_en_minorities = [\n",
    "    'ex-combatants',\n",
    "    'excombatants',\n",
    "    'afro-descendant',\n",
    "    'afrodescendant',\n",
    "    'haitians',\n",
    "    'haitian',\n",
    "    #'ethnic',\n",
    "    'afro-descendants',\n",
    "    'afrodescendants',\n",
    "    'peasant',\n",
    "    'peasants',\n",
    "    'guyanese',\n",
    "    'boudouma',\n",
    "    'bahamian',\n",
    "    'kanembou',\n",
    "    'racism',\n",
    "    'bojayá',\n",
    "    'bojaya',\n",
    "    'minority',\n",
    "    'minorities',\n",
    "    'bahamians',\n",
    "    'afroodescending',\n",
    "    'afrodescending',\n",
    "    'afro-descending',\n",
    "    #'black',\n",
    "    #'ethnicity',\n",
    "    #'victimizing',\n",
    "    ('ethnic', 'communities'),\n",
    "    ('ethnic', 'groups'),\n",
    "    ('religious', 'groups'),\n",
    "    'afro-colombians',\n",
    "    'afrocolombians',\n",
    "    'afro-colombian',\n",
    "    'afrocolombian',\n",
    "    'guaviare',\n",
    "    'meta-guaviare',\n",
    "    'metaguaviare',\n",
    "    'bilwi',\n",
    "    'nariño',\n",
    "    'narino',\n",
    "    'raizals',\n",
    "    'raizal',\n",
    "    'gaitanistas',\n",
    "    ('ethnic', 'organizational', 'processes'),\n",
    "    'gao',\n",
    "    ('ethnic', 'territories'),\n",
    "    'wounaan',\n",
    "    ('mesopotamia', 'communities'),\n",
    "    ('opogadó', 'bocas'),\n",
    "    ('opogado', 'bocas'),\n",
    "    'bojaya',\n",
    "]\n",
    "\n",
    "\n",
    "def lf_minorities_pos_1(row):\n",
    "    if minorities in row['specific_needs_groups']:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "\n",
    "##\n",
    "negative_kw_minorities = []\n",
    "##\n",
    "minorities_patterns = []\n",
    "\n",
    "##\n",
    "def is_minorities(row):\n",
    "    for kw in negative_kw_minorities:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return False\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return False\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return False\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return False\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return False\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return False\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return False\n",
    "    for p in minorities_patterns:\n",
    "        if re.search(p, row[\"excerpt_pp\"]): return True\n",
    "    for lf in [lf_minorities_pos_1]:\n",
    "        if lf(row) == 1:\n",
    "            return True\n",
    "    for kw in kw_en_minorities:\n",
    "        if isinstance(kw, str) and kw in row['tokenized_excerpt']: return True\n",
    "        elif len(kw) == 2 and kw in row['bigram_excerpt']: return True\n",
    "        elif len(kw) == 3 and kw in row['trigram_excerpt']: return True\n",
    "        elif len(kw) == 4 and kw in row['fourgram_excerpt']: return True\n",
    "        elif len(kw) == 5 and kw in row['fivegram_excerpt']: return True\n",
    "        elif len(kw) == 6 and kw in row['sixgram_excerpt']: return True\n",
    "        elif len(kw) == 7 and kw in row['sevengram_excerpt']: return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42edd898",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:51.281808Z",
     "start_time": "2022-01-17T20:43:51.260627Z"
    }
   },
   "outputs": [],
   "source": [
    "def specific_needs_groups(row):\n",
    "    ret = []\n",
    "    if is_plw(row):\n",
    "        ret.append(plw)\n",
    "    if is_ip(row):\n",
    "        ret.append(ip)\n",
    "    if is_pwd(row):\n",
    "        ret.append(pwd)\n",
    "    if is_minorities(row):\n",
    "        ret.append(minorities)\n",
    "    if is_gbv(row):\n",
    "        ret.append(gbv)\n",
    "    if is_unaccompanied_child(row):\n",
    "        ret.append(unaccompanied_child)\n",
    "    if is_chronically_ill(row):\n",
    "        ret.append(chronically_ill)\n",
    "    if is_fhh(row):\n",
    "        ret.append(fhh)\n",
    "    if is_lgbt(row):\n",
    "        ret.append(lgbt)\n",
    "    if is_single_women(row):\n",
    "        ret.append(single_women)\n",
    "    if is_chh(row):\n",
    "        ret.append(chh)\n",
    "    if is_ehh(row):\n",
    "        ret.append(ehh)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a0d80f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:52.843634Z",
     "start_time": "2022-01-17T20:43:52.832493Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    if doc != doc:\n",
    "        return \"\"\n",
    "    doc = doc.lower()\n",
    "    # remove preceeding dates\n",
    "    #doc = re.sub(\"^\\[.+\\]\", \" \", doc).strip()\n",
    "    #doc = re.sub(\"^\\(.+\\)\", \" \", doc).strip()\n",
    "    # spaces btw numbers and words\n",
    "    doc = re.sub('(\\d+(\\W\\d+)?)', r' \\1 ', doc).strip()\n",
    "    doc = re.sub(\"[‐‑–—―─_]\", \"-\", doc)\n",
    "    doc = re.sub(\"(\\w)\\- (\\w)\", r\"\\1\\2\", doc)\n",
    "    # NOTE: I have added \"-\"\n",
    "    doc = re.sub(\n",
    "        \"[\" + re.escape(\n",
    "            '-_@^~.()[],\"“’…<❖‐»—─|•&{≥➢\\ue0e4\\uf0d8\\uf0fc●°#\\u200b>`?�€■!‘%;̧\\'›«”:≤―\\uf0b7$}*´=‑▪\\xad❑·–'\n",
    "        ) + \"]\", \" \", doc)\n",
    "    #remove some puncs\n",
    "    doc = re.sub('\\s+', \" \", doc).strip()\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6141be2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:53.523522Z",
     "start_time": "2022-01-17T20:43:53.519117Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(doc, n=1):\n",
    "    doc = preprocess(doc)\n",
    "    # tokenize\n",
    "    words = word_tokenize(doc)\n",
    "    if n == 1:\n",
    "        return words\n",
    "    return set(ngrams(words, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0864217",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:43:54.888883Z",
     "start_time": "2022-01-17T20:43:54.882423Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(words, n=1):\n",
    "    if n == 1:\n",
    "        return set(words)\n",
    "    return set(ngrams(words, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "979dd907",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:44:54.366488Z",
     "start_time": "2022-01-17T20:43:56.514637Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f98d5df02e4a86a92cc7eb5f20a647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6be80205e342eb8afdc8789f0ce406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed9198b31094d61970e61ae83944963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3b2a9723a84534990092f174776192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715982f2d6b9493095fbffe3208d2e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c194d111856348f1bf094a3ba899d2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcf7e8761ba48a98c3bd05f2de7d3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b1657458e94a8ba707fb1618ebc810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057c3b4865754726ad5eb67c30009558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess\n",
    "df_train_en[\"excerpt_pp\"] = df_train_en[\"excerpt\"].progress_apply(preprocess)\n",
    "# tokenize and cacl ngrams\n",
    "df_train_en[\"tokenized_excerpt\"] = df_train_en[\"excerpt\"].progress_apply(preprocess_and_tokenize)\n",
    "df_train_en[\"bigram_excerpt\"] = df_train_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 2))\n",
    "df_train_en[\"trigram_excerpt\"] = df_train_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 3))\n",
    "df_train_en[\"fourgram_excerpt\"] = df_train_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 4))\n",
    "df_train_en[\"fivegram_excerpt\"] = df_train_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 5))\n",
    "df_train_en[\"sixgram_excerpt\"] = df_train_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 6))\n",
    "df_train_en[\"sevengram_excerpt\"] = df_train_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 7))\n",
    "df_train_en[\"tokenized_excerpt\"] = df_train_en[\"tokenized_excerpt\"].progress_apply(lambda x: set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "caf893e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:45:03.904363Z",
     "start_time": "2022-01-17T20:44:55.231931Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d350f8a0564e492cb0bc90162a1056c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e936adbc5fbc4b7288a64944dbfa6200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130c7a23f46b4226b3bcd362bc02adf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435c47a7f84d44f4a26daa0a58f8d58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346fca104b4342f7a3b9de0441761da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd1d032ba82407fb0f68acd265b12d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3951e2ee8d847e39bcda6f423886d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92e9c9db0b64b75b35d12c534c4244f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc39cffd5c64d2fa5c8e6730b492813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess\n",
    "df_val_en[\"excerpt_pp\"] = df_val_en[\"excerpt\"].progress_apply(preprocess)\n",
    "# tokenize and cacl ngrams\n",
    "df_val_en[\"tokenized_excerpt\"] = df_val_en[\"excerpt\"].progress_apply(preprocess_and_tokenize)\n",
    "df_val_en[\"bigram_excerpt\"] = df_val_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 2))\n",
    "df_val_en[\"trigram_excerpt\"] = df_val_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 3))\n",
    "df_val_en[\"fourgram_excerpt\"] = df_val_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 4))\n",
    "df_val_en[\"fivegram_excerpt\"] = df_val_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 5))\n",
    "df_val_en[\"sixgram_excerpt\"] = df_val_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 6))\n",
    "df_val_en[\"sevengram_excerpt\"] = df_val_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 7))\n",
    "df_val_en[\"tokenized_excerpt\"] = df_val_en[\"tokenized_excerpt\"].progress_apply(lambda x: set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc577297",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:45:12.628600Z",
     "start_time": "2022-01-17T20:45:05.196283Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7cd11749787479abee89b010685b8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767da9432c8f4073ae9ae42818a11af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b35ac7f7734b02b0e1a34188e7c5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0d4dc160504c86911dc369d4e374a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1edf36387a9461aa5f8e3e28ffa2460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512418ca6e2b48d5a9ca7745a3df8c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b747c7cc9a1d43ff90f48d0e79b70b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daaf4529eb364705bb6d159c022482ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d8ff09a9f9433f8122a46b00b1823b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess\n",
    "df_test_en[\"excerpt_pp\"] = df_test_en[\"excerpt\"].progress_apply(preprocess)\n",
    "# tokenize and cacl ngrams\n",
    "df_test_en[\"tokenized_excerpt\"] = df_test_en[\"excerpt\"].progress_apply(preprocess_and_tokenize)\n",
    "df_test_en[\"bigram_excerpt\"] = df_test_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 2))\n",
    "df_test_en[\"trigram_excerpt\"] = df_test_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 3))\n",
    "df_test_en[\"fourgram_excerpt\"] = df_test_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 4))\n",
    "df_test_en[\"fivegram_excerpt\"] = df_test_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 5))\n",
    "df_test_en[\"sixgram_excerpt\"] = df_test_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 6))\n",
    "df_test_en[\"sevengram_excerpt\"] = df_test_en[\"tokenized_excerpt\"].progress_apply(lambda x: tokenize(x, 7))\n",
    "df_test_en[\"tokenized_excerpt\"] = df_test_en[\"tokenized_excerpt\"].progress_apply(lambda x: set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08dd696c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:52:51.139797Z",
     "start_time": "2022-01-17T20:47:02.882649Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3501c3b60784b58a9b02a5b6aa98d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/126323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a44b06714444be5abf52cd33785c277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d556da6dbfd44cfa92a96f25d6bf9db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train_en[\"specific_needs_groups_kw\"] = df_train_en.progress_apply(specific_needs_groups, axis=1)\n",
    "df_val_en[\"specific_needs_groups_kw\"] = df_val_en.progress_apply(specific_needs_groups, axis=1)\n",
    "df_test_en[\"specific_needs_groups_kw\"] = df_test_en.progress_apply(specific_needs_groups, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1289b709",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T20:52:53.142662Z",
     "start_time": "2022-01-17T20:52:52.887871Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_en.to_csv(\"train_0.7.1_keyword_specific_needs_groups.csv\",\n",
    "                  columns=['entry_id', \"specific_needs_groups_kw\"],\n",
    "                  index=False)\n",
    "##\n",
    "df_val_en.to_csv(\"val_0.7.1_keyword_specific_needs_groups.csv\",\n",
    "                  columns=['entry_id', \"specific_needs_groups_kw\"],\n",
    "                  index=False)\n",
    "##\n",
    "df_test_en.to_csv(\"test_0.7.1_keyword_specific_needs_groups.csv\",\n",
    "                  columns=['entry_id', \"specific_needs_groups_kw\"],\n",
    "                  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece053b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VPI: gender based violence\n",
    "#NSAGs: non-state armed groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514c77da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd0f487e277ea6a75fd1c7c341a1deb40c7861148cbc006695943c5304af00fedbe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
