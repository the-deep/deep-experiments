{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d43b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install all depedencies\n",
    "# no GPU is needed to run this notebook\n",
    "\n",
    "!pip install -r ./src/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38dddb0",
   "metadata": {},
   "source": [
    "### Authors: \n",
    "- **DEEPL team in Data Friendly Space**\n",
    "    - Fekih Selim: NLP \n",
    "    - Nicol√≤ Tamagnone: Data Scientist, ISI foundation\n",
    "    - Oguz Kaan Yuksel  \n",
    "<br>\n",
    "\n",
    "- **Supervised by**:\n",
    "    - Ewan Oglethorpe: Executive Director, Data Friendly Space\n",
    "    - Patrice Chataigner: Analysis & Assessment Expert\n",
    "    - Navid Rekabsaz: Assistant Professor, Johannes Kepler University\n",
    "    - Ximena Contla: DEEP Product Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2e27b6",
   "metadata": {},
   "source": [
    "### DEEP Assisted Tagging Tool\n",
    "\n",
    "\n",
    "- In this notebook we propose an example of the main model architecture for the assisted tagging tool deployed in [**The DEEP**](https://thedeep.io/) platform.\n",
    "\n",
    "- The DEEP:\n",
    "    - collaborative platform for qualitative data analysis supporting humanitarian analytical teams to produce actionable insights. \n",
    "    - significantly contributed to improving the humanitarian data ecosystem\n",
    "    - today one of the largest repository of annotated humanitarian response documents: 50k+ sources/leads and 400k+ entries, used for 300+ projects by 3.5k+ registered users in 60+ countries.  \n",
    "<br>\n",
    "- DEEP is largely benefitting from  Natural Language Processing (NLP) and Deep Learning (DL) to aid and support the manual tagging process and give the humanitarian community more time to produce analyses and take rapid action to save more lives.\n",
    "\n",
    "- Up to now, all the information uploaded to the platform has been annotated by hand by experts in the humanitarian sector. \n",
    "\n",
    "- **main objective of the NLP system**:\n",
    "improve and speed up the analysis of the texts by providing live recommendations to taggers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f72c6",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "1) **Data**: Data specifications and challenges  \n",
    "2) **Models**: Implemented methods and their impact on results  \n",
    "3) **Results**: Results discussion and challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533325ec",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Let's go into the details of the model now, starting from the data.\n",
    "\n",
    "To combine entries from different projects and various analytical frameworks (set of labels), we defined a generic analytical framework and we transformed our labels accordingly. Our generic analytical framework has 8 different multi-label categories, totalling 86 different labels, covering all areas of a detailed humanitarian analysis.\n",
    "\n",
    "Our proposed dataset contains 8 categories overall:\n",
    "- 3 **primary tags**: sectors, pillars/subpillars_2d, pillars/subpillars_1d\n",
    "- 5 **secondary tags**: affected_groups, demographic_groups, specific_needs_groups, severity, geolocation\n",
    "\n",
    "In this notebook we focus only on a subset of above categories, **primary tags**.\n",
    "Primary tags contain 75 labels under different subcategories named as follows: \n",
    "- **Sectors** with 11 labels,\n",
    "- **2D Pillars** with  6 labels,\n",
    "- **2D Sub-pillars** with  18 labels,\n",
    "- **1D Pillars** with  7 labels,\n",
    "- **1D Sub-pillars** with  33 labels\n",
    "\n",
    "Our objective is to correctly classify the `sectors`, `2D Sub-pillars`, `1D Sub-pillars`\n",
    "\n",
    "\n",
    "Let's see how they are divided:\n",
    "\n",
    "<div> <center><img src=\"./img/plot1.png\" alt=\"Drawing\" style=\"width: 1500px;\"/></center></div> \n",
    "\n",
    "The difference between 1D and 2D Pillars (and respective Sub-pillars), as we can see in the previous image, lies in the fact that the 2D subcategory presents an additional level of hierarchy, given by the Sectors. Example:\n",
    "\n",
    "<div> <center><img src=\"./img/ex.png\" alt=\"Drawing\" style=\"width: 1400px;\"/></center></div> \n",
    "\n",
    "- Apart from Sectors, each subcategory has an annotation hierarchy, from Pillar to Sub-pillar (1D and 2D) .\n",
    "- Each text excerpt can be annotated with multiple labels, thus making the problem a multi-label text classification.\n",
    "- Total of 157_948 entries to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8736f021",
   "metadata": {},
   "source": [
    "**dataset languages**\n",
    "<div> <center><img src=\"./img/piechart_lang.png\" alt=\"Drawing\" style=\"width: 600px;\"/></center></div> \n",
    "\n",
    "$\\Rightarrow$ Textual entries are in 3 different languages: **English (mainly)**, **Spanish** and **French**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9049cd9",
   "metadata": {},
   "source": [
    "**Tags (groundtruth) distribution**\n",
    "<div> <center><img src=\"./img/tags_proba.png\" alt=\"Drawing\" style=\"width: 600px;\"/></center></div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30946c58",
   "metadata": {},
   "source": [
    "**Main Conclusions about Data**: \n",
    "* Multilingual Data\n",
    "* many tags to be handled\n",
    "* Imbalanced repartition of tags with some tags nearly absent\n",
    "\n",
    "**Discussion: How can we improve our data?**\n",
    "<div> <center><img src=\"./img/kraken_read.png\" alt=\"Drawing\" style=\"width: 600px;\"/></center></div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b2bc0",
   "metadata": {},
   "source": [
    "**Implemented Ways to augment data**:\n",
    "1) Basic Data augmentation techniques (random swapping, random deletion etc.)  \n",
    "2) Targetted Data Augmentation (augment text containing number-related tags by changing numbers) \n",
    "3) Data augmentation with translation  \n",
    "\n",
    "\n",
    "$\\rightarrow$ Option 3 improved results, while options 1 and 2 did not show any improvement in results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d712e645",
   "metadata": {},
   "source": [
    "#### Get Data\n",
    "- In this notebook, we only import a small portion of the total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09b8e807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10420 samples in this test notebook\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.dowload_dataset import download_file_from_google_drive\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# download public data from google drive\n",
    "\n",
    "#TRAIN_VAL_ID = \"1W9-IN0DgYYsiwrWi9Aefyc1V87_q_LWd\"\n",
    "TEST_ID = \"11NQCwixCW2ZiFlSp1QNen2uiS2gThu2B\"\n",
    "\n",
    "# set files path for training and testing set\n",
    "#TRAIN_VAL_NAME = \"train_val_dataset.csv\"\n",
    "TEST_NAME = \"test_dataset.csv\"\n",
    "\n",
    "# downloading\n",
    "#download_file_from_google_drive(id=TRAIN_VAL_ID, destination=TRAIN_VAL_NAME)\n",
    "download_file_from_google_drive(id=TEST_ID, destination=TEST_NAME)\n",
    "\n",
    "\n",
    "important_columns = ['entry_id', 'project_id', 'lead_id', 'analysis_framework_id', 'excerpt', 'sectors', 'subpillars_1d', 'subpillars_2d', 'lang']\n",
    "\n",
    "dataset = pd.read_csv(TEST_NAME)[important_columns]\n",
    "nb_samples = dataset.shape[0]\n",
    "print(f'We have {nb_samples} samples in this test notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c056a24b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb8ad0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "classification_columns = ['sectors', 'subpillars_1d', 'subpillars_2d']\n",
    "for col in classification_columns:\n",
    "    dataset[col] = dataset[col].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa0a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57cb2c3c",
   "metadata": {},
   "source": [
    "- For simpplification, keep only the 50 most present tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e88961e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "dataset[\"target\"] = dataset.apply(\n",
    "        lambda x: x.sectors + x.subpillars_1d + x.subpillars_2d, axis=1\n",
    "    )\n",
    "\n",
    "most_frequent_tags = list(dict(Counter(flatten(dataset['target'])).most_common(50)).keys())\n",
    "\n",
    "dataset[\"target\"] = dataset[\"target\"].apply(\n",
    "    lambda x: [tag for tag in x if tag in most_frequent_tags]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5504f1d",
   "metadata": {},
   "source": [
    "**Data Splitting**:\n",
    "- We create a unified training and testing data to be able to have a clear understanding of models performance.\n",
    "- We use stratified splitting, which keeps the same labels proportions in both the train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f6c0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train set shape contains: 8236 entries\n",
      "The validation set shape contains 1291 entries\n",
      "The test set shape contains 893 entries\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def custom_stratified_train_test_split(df: pd.DataFrame, ratios: dict[str, float]):\n",
    "    \"\"\"\n",
    "    custom function for stratified train test splitting\n",
    "    1) take unique sub-tags (example: ['Health'])\n",
    "    2) For each unique subtag:\n",
    "        i) take all indexes that have that specific subtag\n",
    "        ii) split them randomly to train, val and test sets\n",
    "    \"\"\"\n",
    "\n",
    "    train_ids = []\n",
    "    val_ids = []\n",
    "    test_ids = []\n",
    "\n",
    "    ratio_val_to_test = ratios['val'] / (1 - ratios['train'])\n",
    "    positive_df = df.copy()\n",
    "    positive_df[\"target\"] = positive_df.target.apply(str)\n",
    "    ids = positive_df.groupby(\"target\")[\"entry_id\"].agg(list).values\n",
    "    unique_ids = [list(np.unique(list_tmp)) for list_tmp in ids]\n",
    "\n",
    "    for ids_entry in unique_ids:\n",
    "\n",
    "        train_ids_entry = random.sample(\n",
    "            ids_entry, int(len(ids_entry) * ratios[\"train\"]) + 1\n",
    "        )\n",
    "\n",
    "        val_test_ids_entry = list(set(ids_entry) - set(train_ids_entry))\n",
    "\n",
    "        val_ids_entry = random.sample(\n",
    "            val_test_ids_entry, int(len(val_test_ids_entry) * ratio_val_to_test) \n",
    "        )\n",
    "        test_ids_entry = list(set(val_test_ids_entry) - set(val_ids_entry))\n",
    "\n",
    "        train_ids.append(train_ids_entry)\n",
    "        val_ids.append(val_ids_entry)\n",
    "        test_ids.append(test_ids_entry)\n",
    "\n",
    "    train_df = df[df.entry_id.isin(flatten(train_ids))]\n",
    "    val_df = df[df.entry_id.isin(flatten(val_ids))]\n",
    "    test_df = df[df.entry_id.isin(flatten(test_ids))]\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "ratios = {'train': 0.7, 'val': 0.2, 'test': 0.1}\n",
    "train_df, val_df, test_df = custom_stratified_train_test_split(dataset, ratios)\n",
    "\n",
    "print(f'The train set shape contains: {train_df.shape[0]} entries')\n",
    "print(f'The validation set shape contains {val_df.shape[0]} entries')\n",
    "print(f'The test set shape contains {test_df.shape[0]} entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb3857",
   "metadata": {},
   "source": [
    "- verify that we have the same number of labels in all sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47690b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 50)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "labels_train = list(set(flatten(train_df.target)))\n",
    "labels_val = list(set(flatten(val_df.target)))\n",
    "labels_test = list(set(flatten(test_df.target)))\n",
    "\n",
    "len(labels_train), len(labels_val), len(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a175993",
   "metadata": {},
   "source": [
    "- Create new column to be trained on: `subpillars`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a24ae161",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"subpillars\"] = dataset.apply(\n",
    "        lambda x: x.subpillars_1d + x.subpillars_2d, axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6938186f",
   "metadata": {},
   "source": [
    "### **Dummy Baseline**\n",
    "- We begin by implemeneting a simple baseline model.\n",
    "- We choose to work with a [fasttext](https://fasttext.cc/docs/en/python-module.html) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f810928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4ca2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prepare_fasttext_data import prepare_data\n",
    "import fasttext\n",
    "\n",
    "#prepare data to train fasttext models\n",
    "prepare_data(train_df, \"target\", \"fasttextdata.train\")\n",
    "prepare_data(val_df, \"target\", \"fasttextdata.val\")\n",
    "prepare_data(test_df, \"target\", \"fasttextdata.test\")\n",
    "\n",
    "#train model\n",
    "model = fasttext.train_supervised(input=\"./fast_data/fasttextdata.train\",\n",
    "                                  autotuneValidationFile=\"./fast_data/fasttextdata.val\",\n",
    "                                  thread=1,\n",
    "                                  loss=\"ova\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd8675bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(filename, model, thres = 0.5):\n",
    "    \"\"\"\n",
    "    function used to get predictions using fasttext models\n",
    "    \"\"\"\n",
    "    tot = []\n",
    "    test = open(filename, \"r\").read().split(\"\\n\")\n",
    "    for s in test:\n",
    "        labels = [c for c in s.split() if \"__label__\" in c]\n",
    "        ss = \" \".join([c for c in s.split() if \"__label__\" not in c]).strip()\n",
    "        pred = model.predict(ss, k=-1, threshold=thres)\n",
    "        lab = [c.replace(\"__label__\",\"\").replace(\"*\", \" \") for c in pred[0] if not \"NEGATIVE\" in c]\n",
    "        tot.append(lab)\n",
    "    return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3beaf715",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = get_pred(\"./fast_data/fasttextdata.test\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34981528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>hamming_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>covid-19-&gt;deaths</th>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.4190</td>\n",
       "      <td>0.48100</td>\n",
       "      <td>0.03100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capacities &amp; response-&gt;national response</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>0.23100</td>\n",
       "      <td>0.04500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humanitarian access-&gt;physical constraints</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.00700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capacities &amp; response-&gt;international response</th>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.4570</td>\n",
       "      <td>0.58200</td>\n",
       "      <td>0.05200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humanitarian access-&gt;relief to population</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid-19-&gt;testing</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.57100</td>\n",
       "      <td>0.01000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context-&gt;economy</th>\n",
       "      <td>0.8890</td>\n",
       "      <td>0.3480</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.01800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid-19-&gt;restriction measures</th>\n",
       "      <td>0.7890</td>\n",
       "      <td>0.3660</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.03400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>information and communication-&gt;knowledge and info gaps (hum)</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>displacement-&gt;type/numbers/movements</th>\n",
       "      <td>0.8280</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.69600</td>\n",
       "      <td>0.02400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nutrition</th>\n",
       "      <td>0.8670</td>\n",
       "      <td>0.4640</td>\n",
       "      <td>0.60500</td>\n",
       "      <td>0.01900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capacities &amp; response-&gt;number of people reached/response gaps</th>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.49200</td>\n",
       "      <td>0.03700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid-19-&gt;cases</th>\n",
       "      <td>0.7400</td>\n",
       "      <td>0.5970</td>\n",
       "      <td>0.66100</td>\n",
       "      <td>0.04300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>priority interventions-&gt;expressed by humanitarian staff</th>\n",
       "      <td>0.7780</td>\n",
       "      <td>0.2690</td>\n",
       "      <td>0.40000</td>\n",
       "      <td>0.02400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context-&gt;legal &amp; policy</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wash</th>\n",
       "      <td>0.9290</td>\n",
       "      <td>0.6720</td>\n",
       "      <td>0.78000</td>\n",
       "      <td>0.02500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid-19-&gt;vaccination</th>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.4290</td>\n",
       "      <td>0.58100</td>\n",
       "      <td>0.01500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>0.7690</td>\n",
       "      <td>0.6900</td>\n",
       "      <td>0.72700</td>\n",
       "      <td>0.01700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humanitarian conditions-&gt;living standards</th>\n",
       "      <td>0.6450</td>\n",
       "      <td>0.4550</td>\n",
       "      <td>0.53400</td>\n",
       "      <td>0.13900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context-&gt;environment</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>displacement-&gt;local integration</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context-&gt;security &amp; stability</th>\n",
       "      <td>0.6190</td>\n",
       "      <td>0.3420</td>\n",
       "      <td>0.44100</td>\n",
       "      <td>0.03700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context-&gt;politics</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistics</th>\n",
       "      <td>0.6670</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.41400</td>\n",
       "      <td>0.01900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>casualties-&gt;dead</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>0.36800</td>\n",
       "      <td>0.02700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shelter</th>\n",
       "      <td>0.9230</td>\n",
       "      <td>0.4620</td>\n",
       "      <td>0.61500</td>\n",
       "      <td>0.01700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humanitarian conditions-&gt;coping mechanisms</th>\n",
       "      <td>0.7690</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.60600</td>\n",
       "      <td>0.01500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humanitarian conditions-&gt;number of people in need</th>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.4290</td>\n",
       "      <td>0.54500</td>\n",
       "      <td>0.00600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context-&gt;socio cultural</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shock/event-&gt;type and characteristics</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impact-&gt;driver/aggravating factors</th>\n",
       "      <td>0.5920</td>\n",
       "      <td>0.3870</td>\n",
       "      <td>0.46800</td>\n",
       "      <td>0.07400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>displacement-&gt;push factors</th>\n",
       "      <td>0.8890</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.64000</td>\n",
       "      <td>0.01000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shock/event-&gt;underlying/aggravating factors</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>priority needs-&gt;expressed by population</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>0.80000</td>\n",
       "      <td>0.00300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>priority interventions-&gt;expressed by population</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impact-&gt;impact on systems, services and networks</th>\n",
       "      <td>0.6710</td>\n",
       "      <td>0.4360</td>\n",
       "      <td>0.52800</td>\n",
       "      <td>0.10200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agriculture</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>0.42100</td>\n",
       "      <td>0.01200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cross</th>\n",
       "      <td>0.4230</td>\n",
       "      <td>0.2240</td>\n",
       "      <td>0.29300</td>\n",
       "      <td>0.05900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food security</th>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.6520</td>\n",
       "      <td>0.69800</td>\n",
       "      <td>0.04400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humanitarian conditions-&gt;physical and mental well being</th>\n",
       "      <td>0.7310</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>0.58700</td>\n",
       "      <td>0.07700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>priority needs-&gt;expressed by humanitarian staff</th>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.16700</td>\n",
       "      <td>0.01100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at risk-&gt;risk and vulnerabilities</th>\n",
       "      <td>0.6110</td>\n",
       "      <td>0.2440</td>\n",
       "      <td>0.34900</td>\n",
       "      <td>0.04600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>livelihoods</th>\n",
       "      <td>0.8820</td>\n",
       "      <td>0.4920</td>\n",
       "      <td>0.63200</td>\n",
       "      <td>0.03900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impact-&gt;impact on people</th>\n",
       "      <td>0.4780</td>\n",
       "      <td>0.2930</td>\n",
       "      <td>0.36400</td>\n",
       "      <td>0.08600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid-19-&gt;contact tracing</th>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>0.72700</td>\n",
       "      <td>0.00300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context-&gt;demography</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>0.7380</td>\n",
       "      <td>0.6560</td>\n",
       "      <td>0.69500</td>\n",
       "      <td>0.14700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impact-&gt;number of people affected</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>protection</th>\n",
       "      <td>0.7080</td>\n",
       "      <td>0.4740</td>\n",
       "      <td>0.56800</td>\n",
       "      <td>0.07800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shock/event-&gt;hazard &amp; threats</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_all_tags</th>\n",
       "      <td>0.5508</td>\n",
       "      <td>0.3149</td>\n",
       "      <td>0.39034</td>\n",
       "      <td>0.03042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    precision  recall  \\\n",
       "covid-19->deaths                                       0.5650  0.4190   \n",
       "capacities & response->national response               0.4000  0.1620   \n",
       "humanitarian access->physical constraints              1.0000  0.1430   \n",
       "capacities & response->international response          0.8000  0.4570   \n",
       "humanitarian access->relief to population              0.0000  0.0000   \n",
       "covid-19->testing                                      1.0000  0.4000   \n",
       "context->economy                                       0.8890  0.3480   \n",
       "covid-19->restriction measures                         0.7890  0.3660   \n",
       "information and communication->knowledge and in...     0.0000  0.0000   \n",
       "displacement->type/numbers/movements                   0.8280  0.6000   \n",
       "nutrition                                              0.8670  0.4640   \n",
       "capacities & response->number of people reached...     0.6400  0.4000   \n",
       "covid-19->cases                                        0.7400  0.5970   \n",
       "priority interventions->expressed by humanitari...     0.7780  0.2690   \n",
       "context->legal & policy                                0.0000  0.0000   \n",
       "wash                                                   0.9290  0.6720   \n",
       "covid-19->vaccination                                  0.9000  0.4290   \n",
       "education                                              0.7690  0.6900   \n",
       "humanitarian conditions->living standards              0.6450  0.4550   \n",
       "context->environment                                   0.0000  0.0000   \n",
       "displacement->local integration                        0.0000  0.0000   \n",
       "context->security & stability                          0.6190  0.3420   \n",
       "context->politics                                      0.0000  0.0000   \n",
       "logistics                                              0.6670  0.3000   \n",
       "casualties->dead                                       0.5000  0.2920   \n",
       "shelter                                                0.9230  0.4620   \n",
       "humanitarian conditions->coping mechanisms             0.7690  0.5000   \n",
       "humanitarian conditions->number of people in need      0.7500  0.4290   \n",
       "context->socio cultural                                0.0000  0.0000   \n",
       "shock/event->type and characteristics                  0.0000  0.0000   \n",
       "impact->driver/aggravating factors                     0.5920  0.3870   \n",
       "displacement->push factors                             0.8890  0.5000   \n",
       "shock/event->underlying/aggravating factors            0.0000  0.0000   \n",
       "priority needs->expressed by population                1.0000  0.6670   \n",
       "priority interventions->expressed by population        0.0000  0.0000   \n",
       "impact->impact on systems, services and networks       0.6710  0.4360   \n",
       "agriculture                                            1.0000  0.2670   \n",
       "cross                                                  0.4230  0.2240   \n",
       "food security                                          0.7500  0.6520   \n",
       "humanitarian conditions->physical and mental we...     0.7310  0.4900   \n",
       "priority needs->expressed by humanitarian staff        0.5000  0.1000   \n",
       "at risk->risk and vulnerabilities                      0.6110  0.2440   \n",
       "livelihoods                                            0.8820  0.4920   \n",
       "impact->impact on people                               0.4780  0.2930   \n",
       "covid-19->contact tracing                              0.8000  0.6670   \n",
       "context->demography                                    0.0000  0.0000   \n",
       "health                                                 0.7380  0.6560   \n",
       "impact->number of people affected                      0.0000  0.0000   \n",
       "protection                                             0.7080  0.4740   \n",
       "shock/event->hazard & threats                          0.0000  0.0000   \n",
       "mean_all_tags                                          0.5508  0.3149   \n",
       "\n",
       "                                                    f1_score  hamming_loss  \n",
       "covid-19->deaths                                     0.48100       0.03100  \n",
       "capacities & response->national response             0.23100       0.04500  \n",
       "humanitarian access->physical constraints            0.25000       0.00700  \n",
       "capacities & response->international response        0.58200       0.05200  \n",
       "humanitarian access->relief to population            0.00000       0.00600  \n",
       "covid-19->testing                                    0.57100       0.01000  \n",
       "context->economy                                     0.50000       0.01800  \n",
       "covid-19->restriction measures                       0.50000       0.03400  \n",
       "information and communication->knowledge and in...   0.00000       0.00300  \n",
       "displacement->type/numbers/movements                 0.69600       0.02400  \n",
       "nutrition                                            0.60500       0.01900  \n",
       "capacities & response->number of people reached...   0.49200       0.03700  \n",
       "covid-19->cases                                      0.66100       0.04300  \n",
       "priority interventions->expressed by humanitari...   0.40000       0.02400  \n",
       "context->legal & policy                              0.00000       0.00700  \n",
       "wash                                                 0.78000       0.02500  \n",
       "covid-19->vaccination                                0.58100       0.01500  \n",
       "education                                            0.72700       0.01700  \n",
       "humanitarian conditions->living standards            0.53400       0.13900  \n",
       "context->environment                                 0.00000       0.00600  \n",
       "displacement->local integration                      0.00000       0.00100  \n",
       "context->security & stability                        0.44100       0.03700  \n",
       "context->politics                                    0.00000       0.00900  \n",
       "logistics                                            0.41400       0.01900  \n",
       "casualties->dead                                     0.36800       0.02700  \n",
       "shelter                                              0.61500       0.01700  \n",
       "humanitarian conditions->coping mechanisms           0.60600       0.01500  \n",
       "humanitarian conditions->number of people in need    0.54500       0.00600  \n",
       "context->socio cultural                              0.00000       0.00400  \n",
       "shock/event->type and characteristics                0.00000       0.00400  \n",
       "impact->driver/aggravating factors                   0.46800       0.07400  \n",
       "displacement->push factors                           0.64000       0.01000  \n",
       "shock/event->underlying/aggravating factors          0.00000       0.00300  \n",
       "priority needs->expressed by population              0.80000       0.00300  \n",
       "priority interventions->expressed by population      0.00000       0.00400  \n",
       "impact->impact on systems, services and networks     0.52800       0.10200  \n",
       "agriculture                                          0.42100       0.01200  \n",
       "cross                                                0.29300       0.05900  \n",
       "food security                                        0.69800       0.04400  \n",
       "humanitarian conditions->physical and mental we...   0.58700       0.07700  \n",
       "priority needs->expressed by humanitarian staff      0.16700       0.01100  \n",
       "at risk->risk and vulnerabilities                    0.34900       0.04600  \n",
       "livelihoods                                          0.63200       0.03900  \n",
       "impact->impact on people                             0.36400       0.08600  \n",
       "covid-19->contact tracing                            0.72700       0.00300  \n",
       "context->demography                                  0.00000       0.00800  \n",
       "health                                               0.69500       0.14700  \n",
       "impact->number of people affected                    0.00000       0.00600  \n",
       "protection                                           0.56800       0.07800  \n",
       "shock/event->hazard & threats                        0.00000       0.00800  \n",
       "mean_all_tags                                        0.39034       0.03042  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functions to get metrics \n",
    "from src.metrics import compare_preds, assess_performance\n",
    "\n",
    "groundtruth = [[c.lower() for c in a] for a in test_df.target]\n",
    "subtags = [c.lower() for c in labels_test]\n",
    "\n",
    "results_df = assess_performance (predictions, groundtruth, subtags, 'all_tags')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ca37390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number tags with a low performance (f1 score <0.3): 17\n",
      "Number tags with a medium performance (0.3 < f1 score < 0.6): 20\n",
      "Number tags with a very low performance (f1 score <0.3): 13\n"
     ]
    }
   ],
   "source": [
    "results_df = results_df.iloc[:-1]\n",
    "n_low_tags = sum(results_df.f1_score<0.3)\n",
    "\n",
    "print(f'Number tags with a low performance (f1 score <0.3): {n_low_tags}')\n",
    "print(f'Number tags with a medium performance (0.3 < f1 score < 0.6): {sum(results_df.f1_score<0.6) - n_low_tags}')\n",
    "print(f'Number tags with a good performance (f1 score <0.3): {sum(results_df.f1_score>0.6)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff0b478",
   "metadata": {},
   "source": [
    "**Baseline results interpertation**:\n",
    "- Models perform heterogeneously depending on tags "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd1436",
   "metadata": {},
   "source": [
    "### **Model**\n",
    "\n",
    "**Discussion: what are the specifications the model should satisfy?**\n",
    "<div> <center><img src=\"./img/kraken_search.png\" alt=\"Drawing\" style=\"width: 600px;\"/></center></div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71469685",
   "metadata": {},
   "source": [
    "#### *I. Model specifications*:\n",
    "The model developed is based on pre-trained transformer architecture. The transformer had to fulfill some criteria:\n",
    "- **multilingual** : it needs to work for different languages\n",
    "- **good performance** : in order for it to be useful, the model needs to be performant\n",
    "- **fast predictions** : the main goal of the modelling is to give live predictions to taggers while they are working on tagging. Speed is critical in this case and the faster the model the better.\n",
    "- **ability to handle important number of imbalanced tags** : The model needs to be able to adapt to data specificities: many tags which are not represented the same way.\n",
    "- **one endpoint only for deployment**: in order to optimize costs, we want to have one endpoint only for all models and predictions. To do this, we create one custom class containing models and deploy it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e9e29",
   "metadata": {},
   "source": [
    "#### *II. model training*\n",
    "**Discussion**\n",
    "- **best models to deal with multilabled data?** \n",
    "- **backbone specifications?**\n",
    "- **loss function?**\n",
    "- **other methods to optimize results?**\n",
    "\n",
    "<div> <center><img src=\"./img/kraken_skate.png\" alt=\"Drawing\" style=\"width: 600px;\"/></center></div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f90590",
   "metadata": {},
   "source": [
    "\n",
    "- **backbone**: \n",
    "    - We use [HuggingFace](https://huggingface.co) for this task\n",
    "    - Transformer: \n",
    "        - [**microsoft/xtremedistil-l6-h256-uncased**](https://huggingface.co/microsoft/xtremedistil-l6-h256-uncased) as a backbone.\n",
    "        - Multi-stage Distillation + Deep Self-Attention Distillation of multilingual pretrained transformres\n",
    "        - 6 layers, 256 hidden size, 12 attention heads -> 13M parameters\n",
    "        - low time and memory complexity compared to other pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d87005",
   "metadata": {},
   "source": [
    "- **loss function**:\n",
    "    - We use the [focal loss](https://arxiv.org/abs/1708.02002) function in order to diminush the imbalancement problem.   \n",
    "<br>\n",
    "- **trained models**: In this notebook we overall train two independent models: `sectors`, one for `subpillars (1D and 2D)`. \n",
    "    - *`subpillars`*: we use tree-like multi-task learning models, fine-tuning the last hidden state of the transformer differently for each subtask. The 5 first hidden layers from the backbone are common to all the tasks and the last hidden layer is specific to each task.  \n",
    "        - Each subtask corresponds to one different pillar $\\rightarrow$ We have 13 different sub-tasks for a total of 51 different tags.\n",
    "        <!-- - `secondary tags` we don't have a hierarchy, so the submodel is only a multi-task architecture $\\rightarrow$ We have 5 different tasks -->\n",
    "        - *benefits*:\n",
    "            - Share the encoding information, obtained from our transformer backbone, and then train different heads separately for different tasks\n",
    "            - Further anneal the problems due to data imbalance\n",
    "            - Deal with a big number of different tags in single models\n",
    "    <div> <center><img src=\"./img/subpillars_architecture.png\" alt=\"Drawing\" style=\"width: 900px;\"/></center></div> \n",
    "    <br>\n",
    "    \n",
    "    - *`sectors` model training*: \n",
    "        - output layer: MLP-like standard architecture.\n",
    "    <div> <center><img src=\"./img/sectors_architecture.png\" alt=\"Drawing\" style=\"width: 300px;\"/></center></div> \n",
    "    \n",
    "<!-- We have 13 different subtasks for the subpillars model (Humanitarian Conditions, At Risk, Displacement, Covid-19, Humanitarian Access, Impact, Information And Communication, Shock/Event, Capacities & Response, Context, Casualties, Priority Interventions, Priority Needs) each of which then has its own final labels, which we want to predict. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76da087",
   "metadata": {},
   "source": [
    "- **After training: decision boundary tuning for each tag**: \n",
    "    1) After training the models on the training set we generate logit predictions on the validation set.  \n",
    "    2) and to optimize our results, we hypertune the threshold for each label to optimize the f1 score on the validation set. Therefore, each different label has a different threshold. This keeps the models from overtagging and helps adapt to the data imbalanceness problem.  \n",
    "- Example:  \n",
    "```\n",
    "thresholds = {'Agriculture': 0.6, 'Protection: 0.68, 'Covid-19->Deaths': 0.35, 'Humanitarian Conditions->Number of People In need': 0.28 .....}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bf8808",
   "metadata": {},
   "source": [
    "- **model training and deployment**: \n",
    "    - For the training, we use\n",
    "        - AWS GPUs\n",
    "        - Mlflow for monitoring the models' performance. We use the [`pyfunc flavor`](https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html) for inference\n",
    "        - AWS endpoints for deployment\n",
    "    - We want ot reduce the deployment's cost as much as possible. \n",
    "    - Overall, we train three separate models. However, in order to use only one endpoint for deployment, we create and deploy a custom data structure which regroups the three models.\n",
    "\n",
    "<div> <center><img src=\"./img/model.png\" alt=\"Drawing\" style=\"width: 600px;\"/></center></div> \n",
    "<!-- ![image info](./img/model.png) -->\n",
    "\n",
    "\n",
    "Now let's get started with the serious stuff ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22d78166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use pytorch and pytorch-lightning as main frameworks\n",
    "\n",
    "import torch\n",
    "import copy, os\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "from typing import Optional, List\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import metrics\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from transformers import AdamW\n",
    "\n",
    "# importing some utilities methods\n",
    "from src.utils import *\n",
    "# loss\n",
    "from src.loss import FocalLoss\n",
    "# encoder embeddings pooling module\n",
    "from src.pooling import Pooling\n",
    "# pytorch Dataset custom dataset builder\n",
    "from src.data import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b5be607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the classifier architecture\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        ids_each_level,\n",
    "        dropout_rate=0.3,\n",
    "        output_length=384,\n",
    "        dim_hidden_layer: int = 256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ids_each_level = ids_each_level\n",
    "        self.l0 = AutoModel.from_pretrained(model_name_or_path)\n",
    "        self.pool = Pooling(word_embedding_dimension=output_length, pooling_mode=\"cls\")\n",
    "\n",
    "        self.LayerNorm_backbone = torch.nn.LayerNorm(output_length)\n",
    "        self.LayerNorm_specific_hidden = torch.nn.LayerNorm(dim_hidden_layer)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.specific_hidden_layer = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(output_length, dim_hidden_layer) for _ in ids_each_level\n",
    "        ])\n",
    "\n",
    "        self.output_layer = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(dim_hidden_layer, len(id_one_level))\n",
    "            for id_one_level in ids_each_level\n",
    "        ])\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Multi-task forward.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        output = self.l0(\n",
    "            inputs[\"ids\"],\n",
    "            attention_mask=inputs[\"mask\"],\n",
    "        )\n",
    "        output = self.pool(\n",
    "            {\n",
    "                \"token_embeddings\": output.last_hidden_state,\n",
    "                \"attention_mask\": inputs[\"mask\"],\n",
    "            }\n",
    "        )[\"sentence_embedding\"]\n",
    "\n",
    "        last_hidden_states = [output.clone() for _ in self.ids_each_level]\n",
    "\n",
    "        heads = []\n",
    "        for i in range(len(self.ids_each_level)):\n",
    "            # specific hidden layer\n",
    "            output_tmp = F.selu(last_hidden_states[i])\n",
    "            output_tmp = self.dropout(output_tmp)\n",
    "            output_tmp = self.LayerNorm_specific_hidden(output_tmp)\n",
    "\n",
    "            # output layer\n",
    "            output_tmp = self.output_layer[i](output_tmp)\n",
    "            heads.append(output_tmp)\n",
    "\n",
    "        return heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d53d39d",
   "metadata": {},
   "source": [
    "- We use [Pytorch lightning](https://www.pytorchlightning.ai/) for implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f482890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch-lightining model class\n",
    "# as loss we use a BCE focal loss (details in ./src/loss.py)\n",
    "\n",
    "class Transformer(pl.LightningModule):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        train_params,\n",
    "        val_params,\n",
    "        tokenizer,\n",
    "        column_name,\n",
    "        multiclass,\n",
    "        learning_rate: float = 1e-5,\n",
    "        adam_epsilon: float = 1e-7,\n",
    "        warmup_steps: int = 500,\n",
    "        weight_decay: float = 0.1,\n",
    "        train_batch_size: int = 32,\n",
    "        eval_batch_size: int = 32,\n",
    "        dropout_rate: float = 0.3,\n",
    "        max_len: int = 512,\n",
    "        output_length: int = 384,\n",
    "        training_device: str = \"cuda\",\n",
    "        keep_neg_examples: bool = False,\n",
    "        dim_hidden_layer: int = 256,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.output_length = output_length\n",
    "        self.column_name = column_name\n",
    "        self.save_hyperparameters()\n",
    "        self.targets = train_dataset[\"target\"]\n",
    "        self.tagname_to_tagid = tagname_to_id(train_dataset[\"target\"])\n",
    "        self.num_labels = len(self.tagname_to_tagid)\n",
    "        self.get_first_level_ids()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.model = Model(\n",
    "            model_name_or_path,\n",
    "            self.ids_each_level,\n",
    "            dropout_rate,\n",
    "            self.output_length,\n",
    "            dim_hidden_layer,\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "        self.val_params = val_params\n",
    "\n",
    "        self.training_device = training_device\n",
    "\n",
    "        self.multiclass = multiclass\n",
    "        self.keep_neg_examples = keep_neg_examples\n",
    "\n",
    "        self.training_loader = self.get_loaders(\n",
    "            train_dataset, train_params, self.tagname_to_tagid, self.tokenizer, max_len\n",
    "        )\n",
    "        self.val_loader = self.get_loaders(\n",
    "            val_dataset, val_params, self.tagname_to_tagid, self.tokenizer, max_len\n",
    "        )\n",
    "        self.Focal_loss = FocalLoss()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output = self.model(inputs)\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(batch)\n",
    "        train_loss = self.get_loss(outputs, batch[\"targets\"])\n",
    "\n",
    "        self.log(\n",
    "            \"train_loss\", train_loss.item(), prog_bar=True, on_step=False, on_epoch=True\n",
    "        )\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(batch)\n",
    "        val_loss = self.get_loss(outputs, batch[\"targets\"])\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            val_loss,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=False,\n",
    "        )\n",
    "\n",
    "        return {\"val_loss\": val_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "        optimizer = AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "            eps=self.hparams.adam_epsilon,\n",
    "        )\n",
    "\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer, \"min\", 0.5, patience=self.hparams.max_epochs // 6\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.training_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_loader\n",
    "\n",
    "    def get_loaders(\n",
    "        self, dataset, params, tagname_to_tagid, tokenizer, max_len: int = 128\n",
    "    ):\n",
    "\n",
    "        set = CustomDataset(dataset, tagname_to_tagid, tokenizer, max_len)\n",
    "        loader = DataLoader(set, **params, pin_memory=True)\n",
    "        return loader\n",
    "\n",
    "    def get_loss(self, outputs, targets, only_pos: bool = False):\n",
    "        \"\"\"\n",
    "        INPUTS:\n",
    "            - outputs: raw outputs of model: List[List[float]]\n",
    "            - targets: groundtruth: List[int]\n",
    "            - only_pos: if True: do not do backpropagation for negative samples \n",
    "            (Negative samples are rows which do not contain any subatg)\n",
    "        OUTPUTS:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(self.ids_each_level) == 1:\n",
    "            return self.Focal_loss(outputs[0], targets)\n",
    "        else:\n",
    "            tot_loss = 0\n",
    "            for i_th_level in range(len(self.ids_each_level)):\n",
    "                ids_one_level = self.ids_each_level[i_th_level]\n",
    "                outputs_i_th_level = outputs[i_th_level]\n",
    "                targets_one_level = targets[:, ids_one_level]\n",
    "                \n",
    "                # main objective: for each level, if row contains only zeros, not to do backpropagation\n",
    "\n",
    "                if only_pos:\n",
    "                    mask_ids_neg_example = [\n",
    "                        not bool(int(torch.sum(one_row)))\n",
    "                        for one_row in targets_one_level\n",
    "                    ]\n",
    "                    outputs_i_th_level[mask_ids_neg_example, :] = 1e-8\n",
    "\n",
    "                tot_loss += self.Focal_loss(outputs_i_th_level, targets_one_level)\n",
    "\n",
    "            return tot_loss\n",
    "\n",
    "    def get_first_level_ids(self):\n",
    "        \"\"\"\n",
    "        function used to get different tasks and levels of classification\n",
    "            - What characterizes multitask data from non multitask data is the presence of the arrow '->'\n",
    "            Therefore, use it as characterization for multitask or not.\n",
    "            - When having a multitask scenario, we return a list of lists for ids\n",
    "            Example: We have 3 different tasks, each containing 2 subtasks\n",
    "            The output would be [[0, 1], [2, 3], [4, 5]]\n",
    "            This will be used later in models to get ids of each specific task.\n",
    "        \"\"\"\n",
    "        all_names = list(self.tagname_to_tagid.keys())\n",
    "        if np.all([\"->\" in name for name in all_names]):\n",
    "            first_level_names = list(\n",
    "                np.unique([name.split(\"->\")[0] for name in all_names])\n",
    "            )\n",
    "            self.ids_each_level = [\n",
    "                [i for i in range(len(all_names)) if name in all_names[i]]\n",
    "                for name in first_level_names\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            self.ids_each_level = [[i for i in range(len(all_names))]]\n",
    "\n",
    "    def custom_predict(\n",
    "        self, validation_dataset, testing=False, hypertuning_threshold: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        1) get raw predictions\n",
    "        2) postprocess them to output an output compatible with what we want in the inference\n",
    "        \"\"\"\n",
    "\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        if testing:\n",
    "            self.val_params[\"num_workers\"] = 0\n",
    "\n",
    "        validation_loader = self.get_loaders(\n",
    "            validation_dataset,\n",
    "            self.val_params,\n",
    "            self.tagname_to_tagid,\n",
    "            self.tokenizer,\n",
    "            self.max_len,\n",
    "        )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            testing_device = \"cuda\"\n",
    "        else:\n",
    "            testing_device = \"cpu\"\n",
    "\n",
    "        self.to(testing_device)\n",
    "        self.eval()\n",
    "        self.freeze()\n",
    "        y_true = []\n",
    "        logit_predictions = []\n",
    "        indexes = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(\n",
    "                validation_loader,\n",
    "                total=len(validation_loader.dataset) // validation_loader.batch_size,\n",
    "            ):\n",
    "\n",
    "                if not testing:\n",
    "                    y_true.append(batch[\"targets\"].detach())\n",
    "                    indexes.append(batch[\"entry_id\"].detach())\n",
    "\n",
    "                logits = self(\n",
    "                    {\n",
    "                        \"ids\": batch[\"ids\"].to(testing_device),\n",
    "                        \"mask\": batch[\"mask\"].to(testing_device),\n",
    "                        \"token_type_ids\": batch[\"token_type_ids\"].to(testing_device),\n",
    "                    }\n",
    "                )\n",
    "                logits = torch.cat(logits, dim=1)  # have a matrix like in the beginning\n",
    "                logits_to_array = np.array([np.array(t) for t in logits.cpu()])\n",
    "                logit_predictions.append(logits_to_array)\n",
    "\n",
    "        logit_predictions = np.concatenate(logit_predictions)\n",
    "        logit_predictions = sigmoid(logit_predictions)\n",
    "\n",
    "        target_list = list(self.tagname_to_tagid.keys())\n",
    "        probabilities_dict = []\n",
    "        # postprocess predictions\n",
    "        for i in range(logit_predictions.shape[0]):\n",
    "\n",
    "            # Return predictions\n",
    "            row_logits = logit_predictions[i, :]\n",
    "\n",
    "            # Return probabilities\n",
    "            probabilities_item_dict = {}\n",
    "            for j in range(logit_predictions.shape[1]):\n",
    "                if hypertuning_threshold:\n",
    "                    probabilities_item_dict[target_list[j]] = row_logits[j]\n",
    "                else:\n",
    "                    probabilities_item_dict[target_list[j]] = (\n",
    "                        row_logits[j] / self.optimal_thresholds[target_list[j]]\n",
    "                    )\n",
    "\n",
    "            probabilities_dict.append(probabilities_item_dict)\n",
    "\n",
    "        if not testing:\n",
    "            y_true = np.concatenate(y_true)\n",
    "            indexes = np.concatenate(indexes)\n",
    "            return indexes, logit_predictions, y_true, probabilities_dict\n",
    "\n",
    "        else:\n",
    "            return probabilities_dict\n",
    "\n",
    "    def hypertune_threshold(self, beta_f1: float = 0.8):\n",
    "        \"\"\"\n",
    "        having the probabilities, loop over a list of thresholds to see which one:\n",
    "        1) yields the best results\n",
    "        2) without being an aberrant value (we use a runinng ean)\n",
    "        \"\"\"\n",
    "\n",
    "        data_for_threshold_tuning = self.val_loader.dataset.data\n",
    "        indexes, logit_predictions, y_true, _ = self.custom_predict(\n",
    "            data_for_threshold_tuning, hypertuning_threshold=True\n",
    "        )\n",
    "\n",
    "        thresholds_list = np.linspace(0.0, 1.0, 101)[::-1]\n",
    "        optimal_thresholds_dict = {}\n",
    "        optimal_scores = []\n",
    "        for ids_one_level in self.ids_each_level:\n",
    "            y_true_one_level = y_true[:, ids_one_level]\n",
    "            logit_preds_one_level = logit_predictions[:, ids_one_level]\n",
    "\n",
    "            for j in range(len(ids_one_level)):\n",
    "                scores = []\n",
    "                for thresh_tmp in thresholds_list:\n",
    "                    metric = self.get_metric(\n",
    "                        logit_preds_one_level[:, j],\n",
    "                        y_true_one_level[:, j],\n",
    "                        beta_f1,\n",
    "                        thresh_tmp,\n",
    "                    )\n",
    "                    scores.append(metric)\n",
    "\n",
    "                max_threshold = 0\n",
    "                max_score = 0\n",
    "                for i in range(1, len(scores) - 1):\n",
    "                    # take running mean to have smoother results\n",
    "                    score = np.mean(scores[i - 1 : i + 2])\n",
    "                    if score >= max_score:\n",
    "                        max_score = score\n",
    "                        max_threshold = thresholds_list[i]\n",
    "\n",
    "                optimal_scores.append(max_score)\n",
    "\n",
    "                optimal_thresholds_dict[\n",
    "                    list(self.tagname_to_tagid.keys())[ids_one_level[j]]\n",
    "                ] = max_threshold\n",
    "\n",
    "        self.optimal_thresholds = optimal_thresholds_dict\n",
    "\n",
    "        return np.mean(optimal_scores)\n",
    "\n",
    "    def get_metric(self, preds: List[float], groundtruth: List[int], beta_f1: float, threshold_tmp: float):\n",
    "        \"\"\"\n",
    "        INPUTS:\n",
    "            - preds: list of all probabilities for one specific subtag\n",
    "            - groundtruth: list of all predictions for one specific subtag\n",
    "            - beta_f1: beta value used for computing the beta f1 score\n",
    "            - threshold_tmp: threshold tested\n",
    "        \n",
    "        OUTPUTS:\n",
    "            - f1 / beta_f1 score for specific subtag for specific threshold\n",
    "        \"\"\"\n",
    "\n",
    "        column_pred = [\n",
    "            1 if one_logit > threshold_tmp else 0 for one_logit in preds\n",
    "        ]\n",
    "\n",
    "        if self.multiclass:\n",
    "            metric = metrics.fbeta_score(\n",
    "                groundtruth,\n",
    "                column_pred,\n",
    "                beta_f1,\n",
    "                average='binary', \n",
    "                pos_label=1\n",
    "            )\n",
    "        else:\n",
    "            metric = metrics.f1_score(\n",
    "                groundtruth,\n",
    "                column_pred,\n",
    "                average='binary', \n",
    "                pos_label=1\n",
    "            )\n",
    "        return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "149c2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main class used to train model\n",
    "    \n",
    "class CustomTrainer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        training_column,\n",
    "        MODEL_DIR: str,\n",
    "        MODEL_NAME: str,\n",
    "        TOKENIZER_NAME: str,\n",
    "        dropout_rate: float,\n",
    "        train_params,\n",
    "        val_params,\n",
    "        gpu_nb: int,\n",
    "        MAX_EPOCHS: int,\n",
    "        weight_decay=0.02,\n",
    "        warmup_steps=500,\n",
    "        output_length=384,\n",
    "        max_len=150,\n",
    "        multiclass_bool=True,\n",
    "        keep_neg_examples_bool=False,\n",
    "        learning_rate=3e-5,\n",
    "        weighted_loss: str = \"sqrt\",\n",
    "        training_device: str = \"cuda\",\n",
    "        beta_f1: float = 0.8,\n",
    "        dim_hidden_layer: int = 256\n",
    "    ) -> None:\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.training_column = training_column\n",
    "        self.MODEL_DIR = MODEL_DIR\n",
    "        self.MODEL_NAME = MODEL_NAME\n",
    "        self.TOKENIZER_NAME = TOKENIZER_NAME\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.train_params = train_params\n",
    "        self.val_params = val_params\n",
    "        self.gpu_nb = gpu_nb\n",
    "        self.MAX_EPOCHS = MAX_EPOCHS\n",
    "        self.weight_decay = weight_decay\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.output_length = output_length\n",
    "        self.max_len = max_len\n",
    "        self.multiclass_bool = multiclass_bool\n",
    "        self.keep_neg_examples_bool = keep_neg_examples_bool\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weighted_loss = weighted_loss\n",
    "        self.training_device = training_device\n",
    "        self.beta_f1 = beta_f1\n",
    "        self.dim_hidden_layer = dim_hidden_layer\n",
    "\n",
    "    def train_model(self):\n",
    "        PATH_NAME = self.MODEL_DIR\n",
    "        if not os.path.exists(PATH_NAME):\n",
    "            os.makedirs(PATH_NAME)\n",
    "\n",
    "        early_stopping_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=2, mode=\"min\"\n",
    "        )\n",
    "\n",
    "        checkpoint_callback_params = {\n",
    "            \"save_top_k\": 1,\n",
    "            \"verbose\": True,\n",
    "            \"monitor\": \"val_loss\",\n",
    "            \"mode\": \"min\",\n",
    "        }\n",
    "\n",
    "        FILENAME = \"model_\" + self.training_column\n",
    "        dirpath_pillars = str(PATH_NAME)\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=dirpath_pillars, filename=FILENAME, **checkpoint_callback_params\n",
    "        )\n",
    "\n",
    "        logger = TensorBoardLogger(\"lightning_logs\", name=FILENAME)\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            logger=logger,\n",
    "            callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "            progress_bar_refresh_rate=40,\n",
    "            profiler=\"simple\",\n",
    "            #log_gpu_memory=False,\n",
    "            weights_summary=None,\n",
    "            gpus=self.gpu_nb,\n",
    "            #precision=16,\n",
    "            accumulate_grad_batches=1,\n",
    "            max_epochs=self.MAX_EPOCHS,\n",
    "            gradient_clip_val=1,\n",
    "            gradient_clip_algorithm=\"norm\"\n",
    "        )\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.TOKENIZER_NAME)\n",
    "        \n",
    "        model = Transformer(\n",
    "            model_name_or_path=self.MODEL_NAME,\n",
    "            train_dataset=self.train_dataset,\n",
    "            val_dataset=self.val_dataset,\n",
    "            train_params=self.train_params,\n",
    "            val_params=self.val_params,\n",
    "            tokenizer=tokenizer,\n",
    "            column_name=self.training_column,\n",
    "            gpus=self.gpu_nb,\n",
    "            plugin=\"deepspeed_stage_3_offload\",\n",
    "            accumulate_grad_batches=1,\n",
    "            max_epochs=self.MAX_EPOCHS,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            weight_decay=self.weight_decay,\n",
    "            warmup_steps=self.warmup_steps,\n",
    "            output_length=self.output_length,\n",
    "            learning_rate=self.learning_rate,\n",
    "            multiclass=self.multiclass_bool,\n",
    "            weighted_loss=self.weighted_loss,\n",
    "            training_device=self.training_device,\n",
    "            keep_neg_examples=self.keep_neg_examples_bool,\n",
    "            dim_hidden_layer=self.dim_hidden_layer\n",
    "        )\n",
    "\n",
    "        trainer.fit(model)\n",
    "        model.train_f1_score = model.hypertune_threshold(self.beta_f1)\n",
    "\n",
    "        #delete data from models\n",
    "        del model.training_loader\n",
    "        del model.val_loader\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1823358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns selection to train\n",
    "# here we train only primary tags: sectors and pillars/sub-pillars (1D & 2D)\n",
    "\n",
    "columns = [\n",
    "    'excerpt', \n",
    "    'entry_id', \n",
    "    'subpillars', \n",
    "    'sectors' \n",
    "    #'secondary_tags' we don't train secondary tags here \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "caf697fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = {  \n",
    "    \"train_batch_size\": 32,\n",
    "    \"val_batch_size\": 32,\n",
    "    \"max_len\": 512,\n",
    "    \"warmup_steps\": 10,\n",
    "    \"output_length\": 256,\n",
    "    \"nb_repetitions\": 1, # maximum number of model trainings to do while training models\n",
    "    \"model_name\":  \"microsoft/xtremedistil-l6-h256-uncased\",\n",
    "    \"tokenizer_name\": \"microsoft/xtremedistil-l6-h256-uncased\",\n",
    "    \"beta_f1\": 0.7,   \n",
    "    \"n_gpu\": 0, \n",
    "    # set a directory name for trianed-models checkpoints\n",
    "    \"model_dir\": \"./models\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "120cc49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_df, val_df, training_columns, hyperparaments):\n",
    "        \n",
    "    models = {}\n",
    "    \n",
    "    train_params = {\n",
    "        \"batch_size\": hyperparaments[\"train_batch_size\"],\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 0\n",
    "    }\n",
    "    \n",
    "    val_params = {\n",
    "        \"batch_size\": hyperparaments[\"val_batch_size\"],\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 0\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_nb = 1\n",
    "        training_device = \"cuda\"\n",
    "    else:\n",
    "        gpu_nb = 0\n",
    "        training_device = \"cpu\"\n",
    "        \n",
    "    print(f\"Training device: {training_device}\")\n",
    "        \n",
    "    for column in training_columns[2:]:\n",
    "        \n",
    "        #only for the severity tag, we have single label classification\n",
    "        multiclass_bool = column != \"severity\"\n",
    "        \n",
    "        best_score = 0\n",
    "        iter_nb = 0\n",
    "\n",
    "        while best_score < 0.3 and iter_nb < hyperparaments[\"nb_repetitions\"]:\n",
    "            \n",
    "            # for the purpose of our sample notebook we select a minimal number of epochs\n",
    "            max_epochs = 1\n",
    "\n",
    "            #depending on the training data size, we have different optimal training parameters\n",
    "            if len(train_df) > 120_000:\n",
    "                dropout_column = 0.2\n",
    "                weight_decay_col = 1e-3\n",
    "                dim_hidden_layer = 256\n",
    "                # max_epochs = 5\n",
    "                learning_rate = 8e-5\n",
    "\n",
    "            elif len(train_df) > 50_000:\n",
    "                dropout_column = 0.3\n",
    "                weight_decay_col = 3e-3\n",
    "                dim_hidden_layer = 256\n",
    "                # max_epochs = 8\n",
    "                learning_rate = 5e-5\n",
    "            else:\n",
    "                dropout_column = 0.3\n",
    "                weight_decay_col = 0.01\n",
    "                dim_hidden_layer = 256\n",
    "                # max_epochs = 12\n",
    "                learning_rate = 3e-5\n",
    "\n",
    "            #when the results are bad, lower learning rate and augment the weight decay\n",
    "            if iter_nb != 0:\n",
    "                learning_rate = learning_rate * 0.7\n",
    "                weight_decay_col = weight_decay_col * 2\n",
    "                \n",
    "            \n",
    "            model_trainer = CustomTrainer(\n",
    "                    train_dataset=train_df,\n",
    "                    val_dataset=val_df,\n",
    "                    MODEL_DIR=hyperparaments[\"model_dir\"],\n",
    "                    MODEL_NAME=hyperparaments[\"model_name\"],\n",
    "                    TOKENIZER_NAME=hyperparaments[\"tokenizer_name\"],\n",
    "                    training_column=column,\n",
    "                    gpu_nb=gpu_nb,\n",
    "                    train_params=train_params,\n",
    "                    val_params=val_params,\n",
    "                    MAX_EPOCHS=max_epochs,\n",
    "                    dropout_rate=dropout_column,\n",
    "                    weight_decay=weight_decay_col,\n",
    "                    learning_rate=learning_rate,\n",
    "                    max_len=hyperparaments[\"max_len\"],\n",
    "                    warmup_steps=hyperparaments[\"warmup_steps\"],\n",
    "                    output_length=hyperparaments[\"output_length\"],\n",
    "                    multiclass_bool=multiclass_bool,\n",
    "                    training_device=training_device,\n",
    "                    beta_f1=hyperparaments[\"beta_f1\"],\n",
    "                    dim_hidden_layer=dim_hidden_layer\n",
    "                )\n",
    "        \n",
    "            model = model_trainer.train_model()\n",
    "            model_score = model.train_f1_score\n",
    "            \n",
    "            if model_score > best_score:\n",
    "                best_score = model_score\n",
    "                models.update({column: model})\n",
    "\n",
    "            iter_nb += 1\n",
    "\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74121925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cpu\n",
      "Epoch 0:   0%|          | 0/299 [00:00<?, ?it/s]              "
     ]
    }
   ],
   "source": [
    "# run training\n",
    "\n",
    "models = run_training(train_df, val_df, training_columns=columns, hyperparaments=hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03961069",
   "metadata": {},
   "source": [
    "- you can also use resulting models in a second time loading from models checkpoints. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151e95e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm_subpillars = Transformer.load_from_checkpoint(\"./models/model_subpillars.ckpt\")\n",
    "# cm_sectors = Transformer.load_from_checkpoint(\"./models/model_sectors.ckpt\")\n",
    "# models = {\"subpillars\": cm_subpillars, \"sectors\": cm_sectors}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb63ec2",
   "metadata": {},
   "source": [
    "**hypertune threshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2318451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in list(models.keys()):\n",
    "    models[model_name].hyptertune_threshold(beta_f1=hyp[\"beta_f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3aa228",
   "metadata": {},
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54beedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions = {}\n",
    "\n",
    "for tag_name, trained_model in models.items():\n",
    "\n",
    "    predictions_one_model = trained_model.custom_predict(\n",
    "        test_df[\"excerpt\"], testing=True\n",
    "    )\n",
    "    raw_predictions[tag_name] = predictions_one_model\n",
    "\n",
    "outputs = {\n",
    "    \"preds\": raw_predictions,\n",
    "    \"thresholds\": trained_model.optimal_thresholds\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd48bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pillars_1d_tags = ['Covid-19',\n",
    " 'Casualties',\n",
    " 'Context',\n",
    " 'Displacement',\n",
    " 'Humanitarian Access',\n",
    " 'Shock/Event',\n",
    " 'Information And Communication']\n",
    "\n",
    "pillars_2d_tags = ['At Risk',\n",
    " 'Priority Interventions',\n",
    " 'Capacities & Response',\n",
    " 'Humanitarian Conditions',\n",
    " 'Impact',\n",
    " 'Priority Needs']\n",
    " \n",
    "def get_predictions_all(ratio_proba_threshold, \n",
    "    output_columns,\n",
    "    pillars_2d,\n",
    "    pillars_1d,\n",
    "    nb_entries: int, \n",
    "    ratio_nb: int):\n",
    "    \n",
    "    predictions = {column:[] for column in output_columns }\n",
    "    for entry_nb in range (nb_entries):\n",
    "        returns_sectors = ratio_proba_threshold['sectors'][entry_nb]       \n",
    "        returns_subpillars = ratio_proba_threshold['subpillars'][entry_nb] \n",
    "        \n",
    "        subpillars_2d_tags = {\n",
    "           key: value for key, value in returns_subpillars.items() if\\\n",
    "                key.split('->')[0] in pillars_2d\n",
    "        }\n",
    "        subpillars_1d_tags = {\n",
    "           key: value for key, value in returns_subpillars.items() if\\\n",
    "                key.split('->')[0] in pillars_1d\n",
    "        }\n",
    "\n",
    "        ratios_sectors_subpillars_2d =\\\n",
    "            list(returns_sectors.values()) + list(subpillars_2d_tags.values())\n",
    "\n",
    "        if np.any([item >= ratio_nb for item in ratios_sectors_subpillars_2d]):\n",
    "            preds_2d = get_preds_entry (subpillars_2d_tags, True, ratio_nb)\n",
    "            preds_sectors = get_preds_entry (returns_sectors, True, ratio_nb)  \n",
    "\n",
    "        else:\n",
    "            preds_2d = []\n",
    "            preds_sectors = []\n",
    "            \n",
    "        predictions['sectors'].append(preds_sectors)\n",
    "        predictions['subpillars_2d'].append(preds_2d)\n",
    "        \n",
    "        preds_1d = get_preds_entry (subpillars_1d_tags, False, ratio_nb)\n",
    "        predictions['subpillars_1d'].append(preds_1d)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def get_preds_entry (preds_column, return_at_least_one=True, ratio_nb=1, return_only_one=False):\n",
    "    preds_entry = [\n",
    "        sub_tag for sub_tag in list(preds_column.keys()) if preds_column[sub_tag]>ratio_nb\n",
    "    ]\n",
    "    if return_only_one:\n",
    "        preds_entry = [\n",
    "            sub_tag for sub_tag in list(preds_column.keys())\\\n",
    "                if preds_column[sub_tag]==max(list(preds_column.values()))\n",
    "        ]\n",
    "    if return_at_least_one:\n",
    "        if len(preds_entry)==0:\n",
    "            preds_entry = [\n",
    "                sub_tag for sub_tag in list(preds_column.keys())\\\n",
    "                    if preds_column[sub_tag]==max(list(preds_column.values()))\n",
    "            ]\n",
    "    return preds_entry\n",
    "\n",
    "final_preds = get_predictions_all(\n",
    "    outputs['preds'], #raw predictions returned\n",
    "    [ 'sectors', 'subpillars_2d', 'subpillars_1d'],\n",
    "    pillars_2d=pillars_2d_tags,\n",
    "    pillars_1d=pillars_1d_tags,\n",
    "    nb_entries=len(test_df), #total number of predictions to be postprocessed\n",
    "    ratio_nb=1)\n",
    "\n",
    "\n",
    "predictions_df = test_df[[\n",
    "    'excerpt', 'entry_id', 'sectors', 'subpillars_1d', 'subpillars_2d'\n",
    "]]\n",
    "predictions_df.rename(\n",
    "    columns= {\n",
    "        'sectors': 'sectors_gt', \n",
    "        'subpillars_1d': 'subpillars_1d_gt', \n",
    "        'subpillars_2d': 'subpillars_2d_gt'\n",
    "    }\n",
    "    )\n",
    "predictions_df['sectors_preds'] = final_preds['sectors']\n",
    "predictions_df['subpillars_2d_preds'] = final_preds['subpillars_2d']\n",
    "predictions_df['subpillars_1d_preds'] = final_preds['subpillars_1d']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ee122e",
   "metadata": {},
   "source": [
    "- have insight on results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac6299e",
   "metadata": {},
   "source": [
    "- get general results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f37f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['sectors', 'subpillars_2d', 'subpillars_1d']\n",
    "raw_results = {}\n",
    "\n",
    "raw_results['sectors'] = compare_preds(predictions_df['sectors_gt'].values, predictions_df['sectors_preds'].values)\n",
    "raw_results['subpillars_2d'] = compare_preds(predictions_df['subpillars_2d_gt'].values, predictions_df['subpillars_2d_preds'].values)\n",
    "raw_results['subpillars_1d'] = compare_preds(predictions_df['subpillars_1d_gt'].values, predictions_df['subpillars_1d_preds'].values)\n",
    "    \n",
    "df_results_raw = pd.DataFrame.from_dict(raw_results, orient='columns')\n",
    "df_results_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb8011e",
   "metadata": {},
   "source": [
    "- get detailed results (for each tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4297178",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sectors_detailed = assess_performance (\n",
    "    predictions_df['sectors_preds'].tolist(), \n",
    "    predictions_df['sectors_gt'].tolist(), \n",
    "    sorted(list(set(flatten(predictions_df['sectors_gt'])))),\n",
    "    'sectors'\n",
    "    )\n",
    "results_sectors_detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a08543",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_subpillars_2d_detailed = assess_performance (\n",
    "    predictions_df['subpillars_2d_preds'].tolist(), \n",
    "    predictions_df['subpillars_2d_gt'].tolist(), \n",
    "    sorted(list(set(flatten(predictions_df['subpillars_2d_gt'])))),\n",
    "    'subpillars_2d'\n",
    "    )\n",
    "results_subpillars_2d_detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c15ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_subpillars_1d_detailed = assess_performance (\n",
    "    predictions_df['subpillars_1d_preds'].tolist(), \n",
    "    predictions_df['subpillars_1d_gt'].tolist(), \n",
    "    sorted(list(set(flatten(predictions_df['subpillars_1d_gt'])))),\n",
    "    'subpillars_1d'\n",
    "    )\n",
    "results_subpillars_1d_detailed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340406d",
   "metadata": {},
   "source": [
    "Once you have the predictions it is easy to get the metrics you want against the targets in the test dataset.\n",
    "\n",
    "### Results\n",
    "For completeness we report here our results, obtained with a larger number of epochs and all data.\n",
    "\n",
    "#### General results\n",
    "<div> <center><img src=\"./img/image.png\" alt=\"Drawing\" style=\"width: 600px;\"/></center></div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf30201",
   "metadata": {},
   "source": [
    "#### Specific Results\n",
    "**sectors**\n",
    "<div> <center><img src=\"./img/sectors_with_scores.png\" alt=\"Drawing\" style=\"width: 900px;\"/></center></div> \n",
    "\n",
    "- `sectors` tag is performing well overall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d793194f",
   "metadata": {},
   "source": [
    "**subpillars_2d**\n",
    "<div> <center><img src=\"./img/subpillars2d_with_scores.png\" alt=\"Drawing\" style=\"width: 900px;\"/></center></div> \n",
    "\n",
    "**subpillars_1d**\n",
    "<div> <center><img src=\"./img/subpillars1d_with_scores.png\" alt=\"Drawing\" style=\"width: 900px;\"/></center></div> \n",
    "\n",
    "- For the `subpillars` tags, the performance differs a lot depending on the tag. Generally, underrepresented tags are the ones performing the worst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d612aa",
   "metadata": {},
   "source": [
    "### Current challenges in our classification models:\n",
    "- **Data**: more data samples for underrepresented tags to have better performances\n",
    "- **Models**: Adapt models to some specific tasks: \n",
    "    - Number related tags\n",
    "    - organization related tags (International vs National vs Local Reponse)\n",
    "- **Deployment**: reduce deployment costs (log models on cpu instances or lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e667e8a",
   "metadata": {},
   "source": [
    "### Next steps in NLP:\n",
    "- `Humanitarian Backbone`: Create a humanitarian backbone using humanitarian data scraped from the web\n",
    "- `Automatic text extraction`: Having a raw document, being able to extract automatically relevant entries\n",
    "- `Natural Language Generation`: Write humanitarian Analysis articles using NLP using the tagged data\n",
    "- `Question Answering using tagged data`: Example: return number of people at risk for a specific project and a specific country"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a5ddf8e25d962f331e8059973cfd97c5aef9d0ccfdd243943e9f1f512e91043"
  },
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
