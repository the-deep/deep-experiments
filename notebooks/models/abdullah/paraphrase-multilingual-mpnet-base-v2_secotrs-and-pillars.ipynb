{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "kernel `conda_pytorch_latest_p36`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install icecream\n",
    "# !pip install tqdm\n",
    "# !pip install torchmetrics\n",
    "# !pip install pytorch_lightning\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:49:30.843642Z",
     "start_time": "2021-06-01T14:49:30.663973Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from icecream import ic\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.functional import accuracy, f1, auroc\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.core.decorators import auto_move_data\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from matplotlib import rc\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from transformers.optimization import (\n",
    "    Adafactor,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:49:31.657777Z",
     "start_time": "2021-06-01T14:49:31.631040Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:49:32.921745Z",
     "start_time": "2021-06-01T14:49:32.910873Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:49:35.745930Z",
     "start_time": "2021-06-01T14:49:35.741002Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED=2021\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:49:41.389959Z",
     "start_time": "2021-06-01T14:49:41.387543Z"
    }
   },
   "outputs": [],
   "source": [
    "ic.configureOutput(outputFunction=sys.stdout.write, includeContext=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:57:02.359181Z",
     "start_time": "2021-06-01T14:57:02.353630Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:57:28.545897Z",
     "start_time": "2021-06-01T14:57:02.782629Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-05\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "DATA_ROOT_DIR = \"/home/ec2-user/SageMaker/deep-experiments/data/frameworks_data/data_v0.4.3\"\n",
    "TRAIN_PATH = os.path.join(DATA_ROOT_DIR, \"data_v0.4.3_train.csv\")\n",
    "VAL_PATH = os.path.join(DATA_ROOT_DIR, \"data_v0.4.3_val.csv\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:57:29.882333Z",
     "start_time": "2021-06-01T14:57:28.547379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Agriculture': 0,\n",
       " 'Capacities & Response': 1,\n",
       " 'Cross': 2,\n",
       " 'Education': 3,\n",
       " 'Food Security': 4,\n",
       " 'Health': 5,\n",
       " 'Humanitarian Conditions': 6,\n",
       " 'Impact': 7,\n",
       " 'Livelihoods': 8,\n",
       " 'Logistics': 9,\n",
       " 'Nutrition': 10,\n",
       " 'People At Risk': 11,\n",
       " 'Priority Interventions': 12,\n",
       " 'Priority Needs': 13,\n",
       " 'Protection': 14,\n",
       " 'Shelter': 15,\n",
       " 'WASH': 16}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = pd.read_csv(TRAIN_PATH)\n",
    "val_dataset = pd.read_csv(VAL_PATH)\n",
    "##\n",
    "train_dataset[\"sectors\"] = train_dataset[\"sectors\"].apply(literal_eval)\n",
    "train_dataset[\"pillars\"] = train_dataset[\"pillars\"].apply(literal_eval)\n",
    "val_dataset[\"sectors\"] = val_dataset[\"sectors\"].apply(literal_eval)\n",
    "val_dataset[\"pillars\"] = val_dataset[\"pillars\"].apply(literal_eval)\n",
    "##\n",
    "train_dataset[\"tags_2d_mat\"] = train_dataset[\"sectors\"] + train_dataset[\"pillars\"]\n",
    "val_dataset[\"tags_2d_mat\"] = val_dataset[\"sectors\"] + val_dataset[\"pillars\"]\n",
    "##\n",
    "tag_set = set()\n",
    "for tags_i in train_dataset[\"tags_2d_mat\"]:\n",
    "    tag_set.update(tags_i)\n",
    "tagname_to_tagid = {tag:i for i, tag in enumerate(list(sorted(tag_set)))}\n",
    "tagname_to_tagid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:57:29.890405Z",
     "start_time": "2021-06-01T14:57:29.883645Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tagname_to_tagid, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.excerpt_text = dataframe[\"excerpt\"].tolist(\n",
    "        ) if dataframe is not None else None\n",
    "        self.targets = self.data[\"tags_2d_mat\"].tolist(\n",
    "        ) if dataframe is not None else None\n",
    "        self.tagname_to_tagid = tagname_to_tagid\n",
    "        self.tagid_to_tagname = list(tagname_to_tagid.keys())\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def encode_example(self,\n",
    "                       excerpt_text: str,\n",
    "                       index=None,\n",
    "                       as_batch: bool = False):\n",
    "        \n",
    "        inputs = self.tokenizer(excerpt_text,\n",
    "                                            None,\n",
    "                                            truncation=True,\n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length=self.max_len,\n",
    "                                            padding=\"max_length\",\n",
    "                                            return_token_type_ids=True)\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        targets = None\n",
    "        if self.targets:\n",
    "            target_indices = [\n",
    "                self.tagname_to_tagid[target]\n",
    "                for target in self.targets[index]\n",
    "                if target in self.tagname_to_tagid\n",
    "            ]\n",
    "            targets = np.zeros(len(self.tagname_to_tagid), dtype=np.int)\n",
    "            targets[target_indices] = 1\n",
    "\n",
    "        encoded = {\n",
    "            'ids':\n",
    "            torch.tensor(ids, dtype=torch.long),\n",
    "            'mask':\n",
    "            torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids':\n",
    "            torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets':\n",
    "            torch.tensor(targets, dtype=torch.float32)\n",
    "            if targets is not None else None\n",
    "        }\n",
    "        if as_batch:\n",
    "            return {\n",
    "                \"ids\": encoded[\"ids\"].unsqueeze(0),\n",
    "                \"mask\": encoded[\"mask\"].unsqueeze(0),\n",
    "                \"token_type_ids\": encoded[\"ids\"].unsqueeze(0)\n",
    "            }\n",
    "        return encoded\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.excerpt_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        excerpt_text = str(self.excerpt_text[index])\n",
    "        return self.encode_example(excerpt_text, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:57:29.900276Z",
     "start_time": "2021-06-01T14:57:29.891880Z"
    }
   },
   "outputs": [],
   "source": [
    "training_set = CustomDataset(train_dataset, tagname_to_tagid, tokenizer,\n",
    "                              MAX_LEN)\n",
    "val_set = CustomDataset(val_dataset, tagname_to_tagid, tokenizer,\n",
    "                         MAX_LEN)\n",
    "\n",
    "val_set_frac = CustomDataset(val_dataset.sample(frac=.01),\n",
    "                              tagname_to_tagid, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:57:29.904210Z",
     "start_time": "2021-06-01T14:57:29.901478Z"
    }
   },
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'batch_size': TRAIN_BATCH_SIZE,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 4\n",
    "}\n",
    "\n",
    "val_params = {\n",
    "    'batch_size': VALID_BATCH_SIZE,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 4\n",
    "}\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "val_loader = DataLoader(val_set, **val_params)\n",
    "val_loader_frac = DataLoader(val_set_frac, **val_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:57:29.909386Z",
     "start_time": "2021-06-01T14:57:29.905678Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name_or_path: str, num_labels:int):\n",
    "        super().__init__()\n",
    "        self.l1 = AutoModel.from_pretrained(model_name_or_path)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, num_labels)\n",
    "    def forward(self, inputs):\n",
    "        output = self.l1(inputs[\"ids\"],\n",
    "                            attention_mask=inputs[\"mask\"],)\n",
    "        output = output.last_hidden_state\n",
    "        output = self.l2(output)\n",
    "        output = self.l3(output)\n",
    "        return output[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T15:04:49.596074Z",
     "start_time": "2021-06-01T15:04:49.543948Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 model_name_or_path: str,\n",
    "                 num_labels: int,\n",
    "                 empty_dataset: CustomDataset,\n",
    "                 pred_threshold: float = .5,\n",
    "                 learning_rate: float = 2e-5,\n",
    "                 adam_epsilon: float = 1e-8,\n",
    "                 warmup_steps: int = 0,\n",
    "                 weight_decay: float = 0.0,\n",
    "                 train_batch_size: int = 32,\n",
    "                 eval_batch_size: int = 32,\n",
    "                 eval_splits: Optional[list] = None,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = Model(model_name_or_path, num_labels)\n",
    "        self.empty_dataset = empty_dataset\n",
    "        self.pred_threshold = pred_threshold\n",
    "\n",
    "        self.f1_score_train = torchmetrics.F1(\n",
    "            num_classes=2,\n",
    "            threshold=0.5,\n",
    "            average='macro',\n",
    "            mdmc_average=\"samplewise\",\n",
    "            ignore_index=None,\n",
    "            top_k=None,\n",
    "            multiclass=True,\n",
    "            compute_on_step=True,\n",
    "            dist_sync_on_step=False,\n",
    "            process_group=None,\n",
    "            dist_sync_fn=None,\n",
    "        )\n",
    "\n",
    "        self.f1_score_val = torchmetrics.F1(\n",
    "            num_classes=2,\n",
    "            threshold=0.5,\n",
    "            average='macro',\n",
    "            mdmc_average=\"samplewise\",\n",
    "            ignore_index=None,\n",
    "            top_k=None,\n",
    "            multiclass=True,\n",
    "            compute_on_step=True,\n",
    "            dist_sync_on_step=False,\n",
    "            process_group=None,\n",
    "            dist_sync_fn=None,\n",
    "        )\n",
    "    @auto_move_data\n",
    "    def forward(self, inputs):\n",
    "        output = self.model(inputs)\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(batch)\n",
    "        loss = F.binary_cross_entropy_with_logits(outputs, batch[\"targets\"])\n",
    "\n",
    "        self.f1_score_train(torch.sigmoid(outputs),\n",
    "                            batch[\"targets\"].to(dtype=torch.long))\n",
    "        self.log(\"train_f1\", self.f1_score_train, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        outputs = self(batch)\n",
    "        val_loss = F.binary_cross_entropy_with_logits(outputs,\n",
    "                                                      batch[\"targets\"])\n",
    "\n",
    "        self.f1_score_val(torch.sigmoid(outputs),\n",
    "                          batch[\"targets\"].to(dtype=torch.long))\n",
    "        self.log(\"val_f1\",\n",
    "                 self.f1_score_val,\n",
    "                 on_step=True,\n",
    "                 on_epoch=True,\n",
    "                 prog_bar=True,\n",
    "                 logger=False)\n",
    "        \n",
    "        self.log(\"val_loss\",\n",
    "                 val_loss,\n",
    "                 on_step=True,\n",
    "                 on_epoch=True,\n",
    "                 prog_bar=True,\n",
    "                 logger=False)\n",
    "        return {'val_loss': val_loss}\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        logits = self(batch)\n",
    "        preds = (torch.sigmoid(logits) > .5)\n",
    "        return {\"preds\": preds, \"targets_i\": batch[\"targets\"]}\n",
    "\n",
    "    def on_test_epoch_end(self, outputs):\n",
    "        preds = torch.cat([output[\"preds\"] for output in outputs]).cpu()\n",
    "        targets = torch.cat([output[\"targets_i\"] for output in outputs]).cpu()\n",
    "        recalls = []\n",
    "        precisions = []\n",
    "        f1_scores = []\n",
    "        for i in range(targets.shape[1]):\n",
    "            class_roc_auc = auroc(preds[:, i], targets[:, i])\n",
    "            self.log(\n",
    "                f\"{self.empty_dataset.sectorid_to_sectorname[i]}_roc_auc/Train\",\n",
    "                class_roc_auc)\n",
    "            class_f1 = metrics.f1_score(targets[:, i], preds[:, i])\n",
    "            self.log(\n",
    "                f\"{self.empty_dataset.sectorid_to_sectorname[i]}_f1/Train\",\n",
    "                class_f1)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        output = self(batch)\n",
    "        return {\"logits\": output}\n",
    "\n",
    "    def on_predict_epoch_end(self, outputs):\n",
    "        logits = torch.cat([output[\"logits\"] for output in outputs[0]])\n",
    "        preds = torch.sigmoid(logits) >= self.pred_threshold\n",
    "        pred_classes = []\n",
    "        for pred in preds:\n",
    "            pred_classes_i = [\n",
    "                self.empty_dataset.sectorid_to_sectorname[i]\n",
    "                for i, p in enumerate(pred) if p\n",
    "            ]\n",
    "            pred_classes.append(pred_classes_i)\n",
    "        self.log({\"pred_classes\": pred_classes})\n",
    "\n",
    "    def custom_predict(self, inputs):\n",
    "        self.eval()\n",
    "        self.freeze()\n",
    "        as_batch = False\n",
    "        if isinstance(inputs, str):\n",
    "            as_batch = True\n",
    "        inputs = self.empty_dataset.encode_example(inputs, as_batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self(inputs)\n",
    "        preds = (torch.sigmoid(logits) >= self.pred_threshold)\n",
    "        pred_classes = []\n",
    "        for pred in preds:\n",
    "            pred_classes_i = [\n",
    "                self.empty_dataset.sectorid_to_sectorname[i]\n",
    "                for i, p in enumerate(pred) if p\n",
    "            ]\n",
    "            pred_classes.append(pred_classes_i)\n",
    "        return pred_classes\n",
    "\n",
    "    def total_steps(self) -> int:\n",
    "        \"\"\"The number of total training steps that will be run. Used for lr scheduler purposes.\"\"\"\n",
    "        self.dataset_size = len(self.train_dataloader().dataset)\n",
    "        num_devices = max(1, self.hparams.gpus)  # TODO: consider num_tpu_cores\n",
    "        effective_batch_size = self.hparams.train_batch_size * self.hparams.accumulate_grad_batches * num_devices\n",
    "        return (self.dataset_size /\n",
    "                effective_batch_size) * self.hparams.max_epochs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\":\n",
    "                self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\":\n",
    "                0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.hparams.learning_rate,\n",
    "                          eps=self.hparams.adam_epsilon)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.hparams.warmup_steps,\n",
    "            num_training_steps=self.total_steps())\n",
    "        scheduler = {\n",
    "            'scheduler': scheduler,\n",
    "            'interval': 'step',\n",
    "            'frequency': 1\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return training_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_loader\n",
    "    \n",
    "    def custom_eval(self, eval_dataloader):\n",
    "        if self.device.type == \"cpu\":\n",
    "            self.to(\"cuda\")\n",
    "        self.eval()\n",
    "        self.freeze()\n",
    "        preds_val_all = []\n",
    "        y_true = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(eval_dataloader, total=len(eval_dataloader.dataset)//eval_dataloader.batch_size):\n",
    "                logits = self({\"ids\": batch[\"ids\"].to(\"cuda\"), \"mask\": batch[\"mask\"].to(\"cuda\"), \"token_type_ids\": batch[\"token_type_ids\"].to(\"cuda\")})\n",
    "                preds_batch = np.zeros(logits.shape, dtype=np.int)\n",
    "                preds_batch[(torch.sigmoid(logits) > self.pred_threshold).cpu().nonzero(as_tuple=True)] = 1\n",
    "                preds_val_all.append(preds_batch)\n",
    "                y_true.append(batch[\"targets\"].numpy().astype(np.int))\n",
    "\n",
    "        preds_val_all = np.concatenate(preds_val_all)\n",
    "        y_true = np.concatenate(y_true)\n",
    "\n",
    "        f1_scores = []\n",
    "        recalls = []\n",
    "        precisions = []\n",
    "        accuracies = []\n",
    "        supports = []\n",
    "        tagname_to_tagid = self.empty_dataset.tagname_to_tagid\n",
    "        for tag_name, tag_id in tagname_to_tagid.items():\n",
    "            cls_rprt = classification_report(y_true[:, tag_id], preds_val_all[:, tag_id], output_dict=True)\n",
    "            precisions.append(cls_rprt[\"macro avg\"][\"precision\"])\n",
    "            recalls.append(cls_rprt[\"macro avg\"][\"recall\"])\n",
    "            f1_scores.append(cls_rprt[\"macro avg\"][\"f1-score\"])\n",
    "            supports.append(cls_rprt[\"1\"][\"support\"])\n",
    "            accuracies.append(cls_rprt[\"accuracy\"])\n",
    "\n",
    "        metrics_df = pd.DataFrame({\n",
    "            \"Sector\": list(tagname_to_tagid.keys()),\n",
    "            \"Precision\": precisions,\n",
    "            \"Recall\": recalls,\n",
    "            \"F1 Score\": f1_scores,\n",
    "            \"Accuracy\": accuracies,\n",
    "            \"Support\": supports,\n",
    "        })\n",
    "        return metrics_df\n",
    "        #multilabel_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_name = \"-\".join(MODEL_NAME.split(\"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T15:04:59.876834Z",
     "start_time": "2021-06-01T15:04:59.868499Z"
    }
   },
   "outputs": [],
   "source": [
    "dirpath = f\"./checkpoints-sectors-and-pillars{log_dir_name}\"\n",
    "if not os.path.exists(dirpath):\n",
    "    os.makedirs(dirpath)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "  dirpath=dirpath,\n",
    "  save_top_k=1,\n",
    "  verbose=True,\n",
    "  monitor=\"val_loss\",\n",
    "  mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T15:05:23.222350Z",
     "start_time": "2021-06-01T15:05:23.216450Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"lightning_logs\", name=f\"sector-and-pillar-classifier-{log_dir_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T15:05:36.062619Z",
     "start_time": "2021-06-01T15:05:36.056457Z"
    }
   },
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T15:02:36.756808Z",
     "start_time": "2021-06-01T15:02:35.143455Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "    progress_bar_refresh_rate=30,\n",
    "    profiler=\"simple\",\n",
    "    log_gpu_memory=True,\n",
    "    weights_summary=None,\n",
    "    gpus=1,\n",
    "    accumulate_grad_batches=1,\n",
    "    max_epochs=EPOCHS,\n",
    "    gradient_clip_val=1,\n",
    "    gradient_clip_algorithm='norm',\n",
    "    #overfit_batches=1,\n",
    "    #limit_predict_batches=2,\n",
    "    #limit_test_batches=2,\n",
    "    #fast_dev_run=True,\n",
    "    #limit_train_batches=1,\n",
    "    #limit_val_batches=1,\n",
    "    #limit_test_batches: Union[int, float] = 1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T15:02:51.684436Z",
     "start_time": "2021-06-01T15:02:41.889881Z"
    }
   },
   "outputs": [],
   "source": [
    "empty_dataset = CustomDataset(None, tagname_to_tagid, tokenizer,\n",
    "                         MAX_LEN)\n",
    "model = Transformer(MODEL_NAME,\n",
    "                           len(tagname_to_tagid),\n",
    "                           empty_dataset,\n",
    "                           gpus=1,\n",
    "                           precision=16,\n",
    "                           plugin='deepspeed_stage_3_offload',\n",
    "                           accumulate_grad_batches=1,\n",
    "                           max_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (model): Model(\n",
       "    (l1): XLMRobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): RobertaPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (l2): Dropout(p=0.3, inplace=False)\n",
       "    (l3): Linear(in_features=768, out_features=17, bias=True)\n",
       "  )\n",
       "  (f1_score_train): F1()\n",
       "  (f1_score_val): F1()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-24T13:40:56.746204Z",
     "start_time": "2021-05-24T13:39:46.376623Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92343bcd6944a148944b354b05b2676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2021\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56242d117cc24d10870fd4366a96fdf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 2832: val_loss reached 0.14427 (best 0.14427), saving model to \"/home/ec2-user/SageMaker/deep-experiments/notebooks/models/abdullah/modeling_data_v0.4.3/checkpoints-sectors-and-pillarssentence-transformers-paraphrase-multilingual-mpnet-base-v2/epoch=0-step=2832.ckpt\" as top 1\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 5665: val_loss reached 0.13730 (best 0.13730), saving model to \"/home/ec2-user/SageMaker/deep-experiments/notebooks/models/abdullah/modeling_data_v0.4.3/checkpoints-sectors-and-pillarssentence-transformers-paraphrase-multilingual-mpnet-base-v2/epoch=1-step=5665.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 8498: val_loss was not in top 1\n",
      "FIT Profiler Report\n",
      "\n",
      "Action                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total                              \t|  -              \t|_              \t|  1.8249e+04     \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------\n",
      "run_training_epoch                 \t|  6070.7         \t|3              \t|  1.8212e+04     \t|  99.798         \t|\n",
      "run_training_batch                 \t|  2.0531         \t|8499           \t|  1.7449e+04     \t|  95.617         \t|\n",
      "optimizer_step_and_closure_0       \t|  2.034          \t|8499           \t|  1.7287e+04     \t|  94.728         \t|\n",
      "training_step_and_backward         \t|  1.9078         \t|8499           \t|  1.6215e+04     \t|  88.853         \t|\n",
      "backward                           \t|  1.166          \t|8499           \t|  9909.6         \t|  54.302         \t|\n",
      "model_forward                      \t|  0.73443        \t|8499           \t|  6241.9         \t|  34.204         \t|\n",
      "training_step                      \t|  0.734          \t|8499           \t|  6238.2         \t|  34.184         \t|\n",
      "evaluation_step_and_end            \t|  1.4328         \t|476            \t|  682.03         \t|  3.7373         \t|\n",
      "validation_step                    \t|  1.4325         \t|476            \t|  681.88         \t|  3.7366         \t|\n",
      "on_validation_end                  \t|  6.9265         \t|4              \t|  27.706         \t|  0.15182        \t|\n",
      "get_train_batch                    \t|  0.0030804      \t|8499           \t|  26.18          \t|  0.14346        \t|\n",
      "on_train_batch_end                 \t|  0.00024678     \t|8499           \t|  2.0974         \t|  0.011493       \t|\n",
      "cache_result                       \t|  3.0288e-05     \t|35469          \t|  1.0743         \t|  0.0058868      \t|\n",
      "on_after_backward                  \t|  4.856e-05      \t|8499           \t|  0.41271        \t|  0.0022616      \t|\n",
      "on_batch_start                     \t|  2.7237e-05     \t|8499           \t|  0.23149        \t|  0.0012685      \t|\n",
      "on_before_zero_grad                \t|  2.6481e-05     \t|8499           \t|  0.22506        \t|  0.0012333      \t|\n",
      "on_batch_end                       \t|  2.5565e-05     \t|8499           \t|  0.21728        \t|  0.0011906      \t|\n",
      "training_step_end                  \t|  1.9587e-05     \t|8499           \t|  0.16647        \t|  0.00091219     \t|\n",
      "on_train_batch_start               \t|  1.7649e-05     \t|8499           \t|  0.15           \t|  0.00082194     \t|\n",
      "on_validation_start                \t|  0.024963       \t|4              \t|  0.099852       \t|  0.00054717     \t|\n",
      "on_train_start                     \t|  0.091095       \t|1              \t|  0.091095       \t|  0.00049918     \t|\n",
      "on_validation_batch_end            \t|  0.00018867     \t|476            \t|  0.089809       \t|  0.00049213     \t|\n",
      "on_validation_batch_start          \t|  2.8469e-05     \t|476            \t|  0.013551       \t|  7.4257e-05     \t|\n",
      "validation_step_end                \t|  2.007e-05      \t|476            \t|  0.0095534      \t|  5.235e-05      \t|\n",
      "on_train_epoch_start               \t|  0.002723       \t|3              \t|  0.0081689      \t|  4.4764e-05     \t|\n",
      "on_train_end                       \t|  0.00086843     \t|1              \t|  0.00086843     \t|  4.7588e-06     \t|\n",
      "on_train_epoch_end                 \t|  0.00024532     \t|3              \t|  0.00073597     \t|  4.0329e-06     \t|\n",
      "on_epoch_start                     \t|  2.6281e-05     \t|7              \t|  0.00018397     \t|  1.0081e-06     \t|\n",
      "on_epoch_end                       \t|  2.0088e-05     \t|7              \t|  0.00014062     \t|  7.7054e-07     \t|\n",
      "on_validation_epoch_end            \t|  3.4575e-05     \t|4              \t|  0.0001383      \t|  7.5785e-07     \t|\n",
      "on_validation_epoch_start          \t|  2.1868e-05     \t|4              \t|  8.7471e-05     \t|  4.7932e-07     \t|\n",
      "on_fit_start                       \t|  3.6345e-05     \t|1              \t|  3.6345e-05     \t|  1.9916e-07     \t|\n",
      "on_val_dataloader                  \t|  2.8002e-05     \t|1              \t|  2.8002e-05     \t|  1.5344e-07     \t|\n",
      "on_before_accelerator_backend_setup\t|  2.3166e-05     \t|1              \t|  2.3166e-05     \t|  1.2694e-07     \t|\n",
      "on_train_dataloader                \t|  1.9801e-05     \t|1              \t|  1.9801e-05     \t|  1.085e-07      \t|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f50eab7f6c74bb5b8deb703d3f9542f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sector</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agriculture</td>\n",
       "      <td>0.868800</td>\n",
       "      <td>0.766106</td>\n",
       "      <td>0.808702</td>\n",
       "      <td>0.985109</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Capacities &amp; Response</td>\n",
       "      <td>0.788465</td>\n",
       "      <td>0.789001</td>\n",
       "      <td>0.788733</td>\n",
       "      <td>0.926933</td>\n",
       "      <td>962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cross</td>\n",
       "      <td>0.788319</td>\n",
       "      <td>0.752528</td>\n",
       "      <td>0.767614</td>\n",
       "      <td>0.846719</td>\n",
       "      <td>2286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Education</td>\n",
       "      <td>0.912695</td>\n",
       "      <td>0.920760</td>\n",
       "      <td>0.916679</td>\n",
       "      <td>0.974387</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food Security</td>\n",
       "      <td>0.905832</td>\n",
       "      <td>0.914187</td>\n",
       "      <td>0.909950</td>\n",
       "      <td>0.962474</td>\n",
       "      <td>1176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Health</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.905291</td>\n",
       "      <td>0.898062</td>\n",
       "      <td>0.916311</td>\n",
       "      <td>2822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Humanitarian Conditions</td>\n",
       "      <td>0.787925</td>\n",
       "      <td>0.780949</td>\n",
       "      <td>0.784245</td>\n",
       "      <td>0.819815</td>\n",
       "      <td>3055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Impact</td>\n",
       "      <td>0.775981</td>\n",
       "      <td>0.754778</td>\n",
       "      <td>0.764390</td>\n",
       "      <td>0.846818</td>\n",
       "      <td>2168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Livelihoods</td>\n",
       "      <td>0.834126</td>\n",
       "      <td>0.834785</td>\n",
       "      <td>0.834455</td>\n",
       "      <td>0.925643</td>\n",
       "      <td>1297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistics</td>\n",
       "      <td>0.847351</td>\n",
       "      <td>0.683824</td>\n",
       "      <td>0.739092</td>\n",
       "      <td>0.981634</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nutrition</td>\n",
       "      <td>0.921423</td>\n",
       "      <td>0.901645</td>\n",
       "      <td>0.911272</td>\n",
       "      <td>0.986499</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>People At Risk</td>\n",
       "      <td>0.782338</td>\n",
       "      <td>0.645718</td>\n",
       "      <td>0.687592</td>\n",
       "      <td>0.934280</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Priority Interventions</td>\n",
       "      <td>0.857006</td>\n",
       "      <td>0.596364</td>\n",
       "      <td>0.649474</td>\n",
       "      <td>0.985605</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Priority Needs</td>\n",
       "      <td>0.904733</td>\n",
       "      <td>0.557999</td>\n",
       "      <td>0.597432</td>\n",
       "      <td>0.981436</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Protection</td>\n",
       "      <td>0.874925</td>\n",
       "      <td>0.868892</td>\n",
       "      <td>0.871855</td>\n",
       "      <td>0.913829</td>\n",
       "      <td>2178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Shelter</td>\n",
       "      <td>0.878022</td>\n",
       "      <td>0.879830</td>\n",
       "      <td>0.878923</td>\n",
       "      <td>0.965849</td>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>WASH</td>\n",
       "      <td>0.909586</td>\n",
       "      <td>0.925287</td>\n",
       "      <td>0.917250</td>\n",
       "      <td>0.972799</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Sector  Precision    Recall  F1 Score  Accuracy  Support\n",
       "0               Agriculture   0.868800  0.766106  0.808702  0.985109      233\n",
       "1     Capacities & Response   0.788465  0.789001  0.788733  0.926933      962\n",
       "2                     Cross   0.788319  0.752528  0.767614  0.846719     2286\n",
       "3                 Education   0.912695  0.920760  0.916679  0.974387      836\n",
       "4             Food Security   0.905832  0.914187  0.909950  0.962474     1176\n",
       "5                    Health   0.891667  0.905291  0.898062  0.916311     2822\n",
       "6   Humanitarian Conditions   0.787925  0.780949  0.784245  0.819815     3055\n",
       "7                    Impact   0.775981  0.754778  0.764390  0.846818     2168\n",
       "8               Livelihoods   0.834126  0.834785  0.834455  0.925643     1297\n",
       "9                 Logistics   0.847351  0.683824  0.739092  0.981634      237\n",
       "10                Nutrition   0.921423  0.901645  0.911272  0.986499      409\n",
       "11           People At Risk   0.782338  0.645718  0.687592  0.934280      750\n",
       "12   Priority Interventions   0.857006  0.596364  0.649474  0.985605      165\n",
       "13           Priority Needs   0.904733  0.557999  0.597432  0.981436      206\n",
       "14               Protection   0.874925  0.868892  0.871855  0.913829     2178\n",
       "15                  Shelter   0.878022  0.879830  0.878923  0.965849      767\n",
       "16                     WASH   0.909586  0.925287  0.917250  0.972799      891"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics_val = model.custom_eval(val_loader)\n",
    "df_metrics_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654b4a31f89b459884b3d6b261ae7056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2832 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sector</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agriculture</td>\n",
       "      <td>0.933307</td>\n",
       "      <td>0.862145</td>\n",
       "      <td>0.894381</td>\n",
       "      <td>0.991219</td>\n",
       "      <td>2101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Capacities &amp; Response</td>\n",
       "      <td>0.904108</td>\n",
       "      <td>0.860594</td>\n",
       "      <td>0.880760</td>\n",
       "      <td>0.960542</td>\n",
       "      <td>8772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cross</td>\n",
       "      <td>0.857489</td>\n",
       "      <td>0.811753</td>\n",
       "      <td>0.831191</td>\n",
       "      <td>0.888851</td>\n",
       "      <td>20577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Education</td>\n",
       "      <td>0.952700</td>\n",
       "      <td>0.957894</td>\n",
       "      <td>0.955279</td>\n",
       "      <td>0.986310</td>\n",
       "      <td>7522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food Security</td>\n",
       "      <td>0.928665</td>\n",
       "      <td>0.941181</td>\n",
       "      <td>0.934798</td>\n",
       "      <td>0.972709</td>\n",
       "      <td>10585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Health</td>\n",
       "      <td>0.933198</td>\n",
       "      <td>0.937944</td>\n",
       "      <td>0.935528</td>\n",
       "      <td>0.947702</td>\n",
       "      <td>25400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Humanitarian Conditions</td>\n",
       "      <td>0.869639</td>\n",
       "      <td>0.855298</td>\n",
       "      <td>0.861964</td>\n",
       "      <td>0.887968</td>\n",
       "      <td>26491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Impact</td>\n",
       "      <td>0.870781</td>\n",
       "      <td>0.826737</td>\n",
       "      <td>0.845933</td>\n",
       "      <td>0.902871</td>\n",
       "      <td>19253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Livelihoods</td>\n",
       "      <td>0.896912</td>\n",
       "      <td>0.864603</td>\n",
       "      <td>0.879773</td>\n",
       "      <td>0.948231</td>\n",
       "      <td>11672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistics</td>\n",
       "      <td>0.915892</td>\n",
       "      <td>0.754568</td>\n",
       "      <td>0.815047</td>\n",
       "      <td>0.988605</td>\n",
       "      <td>1765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nutrition</td>\n",
       "      <td>0.930752</td>\n",
       "      <td>0.929519</td>\n",
       "      <td>0.930135</td>\n",
       "      <td>0.989134</td>\n",
       "      <td>3679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>People At Risk</td>\n",
       "      <td>0.862320</td>\n",
       "      <td>0.703062</td>\n",
       "      <td>0.756170</td>\n",
       "      <td>0.947525</td>\n",
       "      <td>6715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Priority Interventions</td>\n",
       "      <td>0.935728</td>\n",
       "      <td>0.732795</td>\n",
       "      <td>0.802479</td>\n",
       "      <td>0.990028</td>\n",
       "      <td>1515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Priority Needs</td>\n",
       "      <td>0.919058</td>\n",
       "      <td>0.566308</td>\n",
       "      <td>0.610635</td>\n",
       "      <td>0.982086</td>\n",
       "      <td>1826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Protection</td>\n",
       "      <td>0.918776</td>\n",
       "      <td>0.909068</td>\n",
       "      <td>0.913801</td>\n",
       "      <td>0.942241</td>\n",
       "      <td>19601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Shelter</td>\n",
       "      <td>0.930265</td>\n",
       "      <td>0.923277</td>\n",
       "      <td>0.926736</td>\n",
       "      <td>0.979548</td>\n",
       "      <td>6904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>WASH</td>\n",
       "      <td>0.945540</td>\n",
       "      <td>0.956347</td>\n",
       "      <td>0.950862</td>\n",
       "      <td>0.983961</td>\n",
       "      <td>8018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Sector  Precision    Recall  F1 Score  Accuracy  Support\n",
       "0               Agriculture   0.933307  0.862145  0.894381  0.991219     2101\n",
       "1     Capacities & Response   0.904108  0.860594  0.880760  0.960542     8772\n",
       "2                     Cross   0.857489  0.811753  0.831191  0.888851    20577\n",
       "3                 Education   0.952700  0.957894  0.955279  0.986310     7522\n",
       "4             Food Security   0.928665  0.941181  0.934798  0.972709    10585\n",
       "5                    Health   0.933198  0.937944  0.935528  0.947702    25400\n",
       "6   Humanitarian Conditions   0.869639  0.855298  0.861964  0.887968    26491\n",
       "7                    Impact   0.870781  0.826737  0.845933  0.902871    19253\n",
       "8               Livelihoods   0.896912  0.864603  0.879773  0.948231    11672\n",
       "9                 Logistics   0.915892  0.754568  0.815047  0.988605     1765\n",
       "10                Nutrition   0.930752  0.929519  0.930135  0.989134     3679\n",
       "11           People At Risk   0.862320  0.703062  0.756170  0.947525     6715\n",
       "12   Priority Interventions   0.935728  0.732795  0.802479  0.990028     1515\n",
       "13           Priority Needs   0.919058  0.566308  0.610635  0.982086     1826\n",
       "14               Protection   0.918776  0.909068  0.913801  0.942241    19601\n",
       "15                  Shelter   0.930265  0.923277  0.926736  0.979548     6904\n",
       "16                     WASH   0.945540  0.956347  0.950862  0.983961     8018"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics_train = model.custom_eval(training_loader)\n",
    "df_metrics_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
