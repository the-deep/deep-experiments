{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These requirements are necessary if you launch this notebook from SageMaker instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install mlflow\\n!pip install pytorch-lightning\\n!pip install transformers\\n!pip install tqdm\\n!pip install sagemaker\\n!pip install s3fs\\n!pip install smdebug'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!pip install mlflow\n",
    "!pip install pytorch-lightning\n",
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "!pip install sagemaker\n",
    "!pip install s3fs\n",
    "!pip install smdebug\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from typing import Any, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:49:30.843642Z",
     "start_time": "2021-06-01T14:49:30.663973Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.functional import accuracy, f1, auroc\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.core.decorators import auto_move_data\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local constants, regarding the data, MLFlow server, paths, etc..: use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deep.constants import *\n",
    "from deep.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T15:42:32.024647Z",
     "start_time": "2021-05-27T15:42:31.984694Z"
    }
   },
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:29:20.899415Z",
     "start_time": "2021-06-09T08:29:19.327852Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session(default_bucket=DEV_BUCKET.name)\n",
    "role = SAGEMAKER_ROLE\n",
    "role_arn = SAGEMAKER_ROLE_ARN\n",
    "tracking_uri = MLFLOW_SERVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/10/26 11:10:57 INFO mlflow.sagemaker: Using the python_function flavor for deployment!\n",
      "2021/10/26 11:10:58 INFO mlflow.sagemaker: No model data bucket specified, using the default bucket\n",
      "2021/10/26 11:10:59 INFO mlflow.sagemaker: Default bucket `mlflow-sagemaker-us-east-1-961104659532` already exists. Skipping creation.\n",
      "2021/10/26 11:15:14 INFO mlflow.sagemaker: tag response: {'ResponseMetadata': {'RequestId': 'BSBZ6SJBDXZCRGXX', 'HostId': 'cLvNgOUnCPJriUAD6zmuL8c7KeTl6Xqeewrodktd3yVz6XNBnH/IeTOrY+x0HU5muuSVmN5sLrg=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'cLvNgOUnCPJriUAD6zmuL8c7KeTl6Xqeewrodktd3yVz6XNBnH/IeTOrY+x0HU5muuSVmN5sLrg=', 'x-amz-request-id': 'BSBZ6SJBDXZCRGXX', 'date': 'Tue, 26 Oct 2021 09:15:14 GMT', 'content-length': '0', 'server': 'AmazonS3'}, 'RetryAttempts': 0}}\n",
      "2021/10/26 11:15:14 INFO mlflow.sagemaker: Creating new endpoint with name: test-pyfunc-all-models ...\n",
      "2021/10/26 11:15:14 INFO mlflow.sagemaker: Created model with arn: arn:aws:sagemaker:us-east-1:961104659532:model/test-pyfunc-all-models-model-rm35oazgt0op7hwcbga8og\n",
      "2021/10/26 11:15:15 INFO mlflow.sagemaker: Created endpoint configuration with arn: arn:aws:sagemaker:us-east-1:961104659532:endpoint-config/test-pyfunc-all-models-config-024rdlfzs9apmc5o5crumw\n",
      "2021/10/26 11:15:15 INFO mlflow.sagemaker: Created endpoint with arn: arn:aws:sagemaker:us-east-1:961104659532:endpoint/test-pyfunc-all-models\n"
     ]
    }
   ],
   "source": [
    "sagemaker.deploy(\n",
    "    'test-pyfunc-all-models',\n",
    "    's3://deep-mlflow-artifact/16/1540c9852a7c47eab93beb012b8d7749/artifacts/pyfunc_models_all',\n",
    "    execution_role_arn=SAGEMAKER_ROLE_ARN,\n",
    "    image_url=\"961104659532.dkr.ecr.us-east-1.amazonaws.com/mlflow-pyfunc:latest\",\n",
    "    region_name=\"us-east-1\",\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    synchronous=False,\n",
    "    archive=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_excel(os.path.join('..', '..', '..', '..', 'feedback_output.xlsx'))\n",
    "test_data = test_data[['Entry']].rename(columns={'Entry':'excerpt'})\n",
    "test_data = test_data[test_data.excerpt.apply(lambda x: 'NONE' != x.upper())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "client = boto3.session.Session().client(\"sagemaker-runtime\", region_name='us-east-1')\n",
    "\n",
    "data = test_data\n",
    "input_json = data.to_json(orient=\"split\")\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName='test-pyfunc-all-models',\n",
    "    Body=input_json,\n",
    "    ContentType=\"application/json; format=pandas-split\",\n",
    ")\n",
    "output = response[\"Body\"].read().decode(\"ascii\")\n",
    "end = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "output = literal_eval(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = output[0]\n",
    "thresholds = output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "multilabel_columns = [\n",
    "    'sectors', \n",
    "    'pillars_2d',\n",
    "    'pillars_1d',\n",
    "    'subpillars_2d', \n",
    "    'subpillars_1d', \n",
    "    'demographic_groups', \n",
    "    'affected_groups', \n",
    "    'specific_needs_groups'\n",
    "    ]\n",
    "\n",
    "no_subpillar_columns = [\n",
    "    'sectors',\n",
    "    'demographic_groups', \n",
    "    'affected_groups', \n",
    "    'specific_needs_groups',\n",
    "    'subpillars_2d', \n",
    "    'subpillars_1d',\n",
    "    ]\n",
    "\n",
    "all_columns = [\n",
    "    'sectors', \n",
    "    'subpillars_2d', \n",
    "    'subpillars_2d_postprocessed',\n",
    "    'subpillars_1d', \n",
    "    'subpillars_1d_postprocessed',\n",
    "    'demographic_groups', \n",
    "    'affected_groups', \n",
    "    'specific_needs_groups',\n",
    "    'severity'\n",
    "    ]\n",
    "\n",
    "def postprocess_subpillars (ratios_pillars, ratios_subpillars, return_at_least_one=True):\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    results_subpillars = []\n",
    "\n",
    "    ratios_subpillars_changed = {name: {} for name, _ in ratios_pillars.items()}\n",
    "    for column_name, ratio in ratios_subpillars.items():\n",
    "        split_column = column_name.split('->')\n",
    "        ratios_subpillars_changed[split_column[0]].update({\n",
    "            split_column[1]: ratio\n",
    "        })\n",
    "\n",
    "    positive_pillars = [\n",
    "        column_name for column_name, ratio in ratios_pillars.items() if ratio >= 1\n",
    "        ]\n",
    "    if len (positive_pillars) == 0 and return_at_least_one:\n",
    "        positive_pillars = [\n",
    "        column_name for column_name, ratio in ratios_pillars.items() \\\n",
    "            if ratio == max(list(ratios_pillars.values()))\n",
    "        ]\n",
    "\n",
    "    if len (positive_pillars) == 0:\n",
    "        return []\n",
    "    \n",
    "    for column_tmp in positive_pillars:\n",
    "        dict_results_column = ratios_subpillars_changed[column_tmp]\n",
    "        preds_column_tmp = [\n",
    "            f\"{column_tmp}->{subtag}\" for subtag, value in dict_results_column.items() if value >=1\n",
    "        ]\n",
    "        if len(preds_column_tmp)==0:\n",
    "            preds_column_tmp = [\n",
    "            subtag for subtag, value in dict_results_column.items() \\\n",
    "                if value == max(list(dict_results_column.values()))\n",
    "        ]\n",
    "        results_subpillars.append(preds_column_tmp)\n",
    "        \n",
    "    return flatten(results_subpillars)\n",
    "\n",
    "def get_predictions(test_probas, thresholds_dict):  \n",
    "    \"\"\"\n",
    "    test_probas structure example: {\n",
    "        'sectors':[\n",
    "            {'Nutrition': 0.032076582, 'Shelter': 0.06674846}, \n",
    "            {'Cross': 0.21885818,'Education': 0.07529669}\n",
    "        ],\n",
    "        'demographic_groups':[\n",
    "            {'Children/Youth Female (5 to 17 years old)': 0.47860646, 'Children/Youth Male (5 to 17 years old)': 0.42560646},\n",
    "            {'Children/Youth Male (5 to 17 years old)': 0.47860646, 'Infants/Toddlers (<5 years old)': 0.85}\n",
    "        ],\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "    }\n",
    "    \n",
    "    thresholds_dict structure example: {\n",
    "        'sectors':{\n",
    "            'Agriculture': 0.2,\n",
    "            'Cross': 0.02,\n",
    "            .\n",
    "            .\n",
    "        },\n",
    "        'subpillars_2d':{\n",
    "            'Humanitarian Conditions->Physical And Mental Well Being': 0.7,\n",
    "            .\n",
    "            .\n",
    "        },\n",
    "        .\n",
    "        .     \n",
    "    }\n",
    "    \n",
    "    First iteration:\n",
    "    - create dict which has the same structure as 'test_probas': \n",
    "    - contains ratio probability of output divided by the threshold\n",
    "    \n",
    "    Second iteration:\n",
    "    - keep ratios superior to 1 except:\n",
    "        - for subpillars_2d: when no ratio is superior to 1 but there is at least one prediction for sectors\n",
    "        - for severity (no threshold, just keep max if there is 'Humanitarian Conditions' in secondary tags outputs)\n",
    "    \"\"\"\n",
    "\n",
    "    #create dict of ratio between probability of output and threshold\n",
    "    ratio_proba_threshold = {}\n",
    "    for column in multilabel_columns:\n",
    "        preds_column = test_probas[column]\n",
    "        dict_keys = list(thresholds_dict[column].keys())\n",
    "        nb_entries = len([i for i in test_probas['sectors'] if i])\n",
    "\n",
    "        returned_values_column = []\n",
    "        for preds_sent in preds_column:\n",
    "            dict_entry = {key:preds_sent[key]/thresholds_dict[column][key] for key in dict_keys }\n",
    "            returned_values_column.append(dict_entry)\n",
    "        ratio_proba_threshold[column] = returned_values_column\n",
    "\n",
    "    predictions = {column:[] for column in all_columns}\n",
    "    for entry_nb in range (nb_entries):\n",
    "\n",
    "        # get the entries where the ratio is superior to 1 and put them in a dict {prediction:probability}\n",
    "        for column in no_subpillar_columns:\n",
    "            preds_column = ratio_proba_threshold[column][entry_nb]\n",
    "            preds_entry = [\n",
    "                sub_tag for sub_tag in list(preds_column.keys()) if ratio_proba_threshold[column][entry_nb][sub_tag]>1\n",
    "            ]\n",
    "\n",
    "            #postprocessing to keep only cross if more than one prediction\n",
    "            if column=='sectors' and len(preds_entry)>1:\n",
    "                preds_entry.append('Cross')\n",
    "\n",
    "            predictions[column].append(list(np.unique(preds_entry)))\n",
    "\n",
    "        preds_2d = postprocess_subpillars(\n",
    "            ratio_proba_threshold['pillars_2d'][entry_nb],\n",
    "            ratio_proba_threshold['subpillars_2d'][entry_nb],\n",
    "            True\n",
    "            )\n",
    "\n",
    "        preds_1d = postprocess_subpillars(\n",
    "            ratio_proba_threshold['pillars_1d'][entry_nb],\n",
    "            ratio_proba_threshold['subpillars_1d'][entry_nb],\n",
    "            False\n",
    "            )\n",
    "\n",
    "        predictions['subpillars_2d_postprocessed'].append(preds_2d)\n",
    "        predictions['subpillars_1d_postprocessed'].append(preds_1d)\n",
    "\n",
    "        #postprocess 'subpillars_2d'\n",
    "        if len(predictions['sectors'][entry_nb])>0 and len(predictions['subpillars_2d'][entry_nb])==0:\n",
    "            predictions['subpillars_2d'][entry_nb] = [\n",
    "                sub_tag for sub_tag in list(preds_column.keys()) if\\\n",
    "                        test_probas[column][entry_nb][sub_tag] == max(list(test_probas[column][entry_nb].values()))\n",
    "            ]\n",
    "\n",
    "        if len(predictions['sectors'][entry_nb])==0 and len(predictions['subpillars_2d'][entry_nb])>0:\n",
    "            predictions['subpillars_2d'][entry_nb] = []\n",
    "            \n",
    "        #severity  predictions and output\n",
    "        if 'Humanitarian Conditions' in str(predictions['subpillars_2d'][entry_nb]):\n",
    "            pred_severity = [\n",
    "                sub_tag for sub_tag in list(test_probas['severity'][entry_nb].keys()) if\\\n",
    "                test_probas['severity'][entry_nb][sub_tag] == max(list(test_probas['severity'][entry_nb].values()))\n",
    "            ]\n",
    "\n",
    "            predictions['severity'].append(pred_severity)\n",
    "        else:\n",
    "            predictions['severity'].append([])\n",
    "            \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = get_predictions(preds, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def get_flat_matrix (column_of_columns, tag_to_id, nb_subtags):\n",
    "    matrix = [[\n",
    "        1 if tag_to_id[i] in column else 0 for i in range (nb_subtags)\n",
    "    ] for column in column_of_columns]\n",
    "    return flatten(matrix)\n",
    "\n",
    "def assess_performance (preds, groundtruth, subtags):\n",
    "    nb_subtags = len(subtags)\n",
    "    tag_to_id = {i:subtags[i] for i in range (nb_subtags)}\n",
    "    groundtruth_col = get_flat_matrix( groundtruth, tag_to_id, nb_subtags)\n",
    "    preds_col = get_flat_matrix( preds, tag_to_id, nb_subtags)\n",
    "    \n",
    "    results = {\n",
    "        'precision': metrics.precision_score(groundtruth_col, preds_col, average='macro'),\n",
    "        'recall': metrics.recall_score(groundtruth_col, preds_col, average='macro'),\n",
    "        'f1': metrics.fbeta_score(groundtruth_col, preds_col, 0.8, average='macro'),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
