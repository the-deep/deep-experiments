{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/selim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/selim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/selim/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/selim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from ast import literal_eval\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem import PorterStemmer  \n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "stop_words = set(stopwords.words())\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "import networkx as nx\n",
    "import community.community_louvain as community\n",
    "from ast import literal_eval\n",
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/selim/anaconda3/envs/deepl/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "DATA_PATH = os.path.join(\n",
    "    '..', '..', '..', '..', \"data\", \"frameworks_data\", 'data_v0.7.1'\n",
    ")\n",
    "\n",
    "full_df = pd.read_csv(os.path.join(DATA_PATH, 'full_dataset_with_translations.csv'))\n",
    "full_df['sectors'] = full_df.sectors.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Protection', 387),\n",
       " ('Health', 377),\n",
       " ('Cross', 368),\n",
       " ('Food Security', 163),\n",
       " ('Livelihoods', 136)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libya_entries = full_df[full_df.project_id==1621]\n",
    "Counter(flatten(libya_entries['sectors'])).most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "between_sequence_links = [\n",
    "    'Furthermore, ', 'Aditionally, ', 'Moreover, ', 'Besides, ', 'Aside from that, ', 'Also, ', 'In addition to that, ',\n",
    "    'On the other hand, ', 'On the other side, '\n",
    "    ]\n",
    "final_sent_link = ['Finally, ', 'Ultimately, ']\n",
    "first_sent_link = ['Firstly, ', 'First of all, ', 'In the first place, ']\n",
    "\n",
    "def clean_tweets(sentence):\n",
    "\n",
    "    if type(sentence) is not str:\n",
    "        sentence = str(sentence)\n",
    "\n",
    "    new_words = []\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        \n",
    "        #lower and remove punctuation\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
    "\n",
    "        #keep clean words and remove hyperlinks\n",
    "        word_not_nothing = new_word != ''\n",
    "        word_not_stop_word = new_word.lower() not in stop_words\n",
    "        #word_not_digit = ~new_word.isdigit()\n",
    "\n",
    "        if word_not_nothing and word_not_stop_word:\n",
    "\n",
    "            #lemmatize\n",
    "            new_word =  wordnet_lemmatizer.lemmatize(new_word, pos=\"v\")  \n",
    "\n",
    "            #stem\n",
    "            new_word = porter_stemmer.stem(new_word)\n",
    "\n",
    "            new_words.append(new_word)\n",
    "            \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def get_similarity_matrix(original_tweets):\n",
    "    \"\"\"\n",
    "    function to get similarity matrix from entries\n",
    "    \"\"\"\n",
    "    cleaned_tweet = [clean_tweets(one_tweet) for one_tweet in original_tweets] \n",
    "\n",
    "    #define and use tf-idf transformation\n",
    "    tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0)\n",
    "    tf_idf = tf.fit_transform(cleaned_tweet)\n",
    "\n",
    "    # get cosine similarity matrix\n",
    "    cosine_similarity_matrix = linear_kernel(tf_idf, tf_idf)\n",
    "    return cosine_similarity_matrix\n",
    "\n",
    "def build_graph(cosine_similarity_matrix):\n",
    "    \"\"\"\n",
    "    function to build graoh from similarity matrix\n",
    "    \"\"\"\n",
    "    graph_one_lang = nx.Graph()\n",
    "    matrix_shape = cosine_similarity_matrix.shape\n",
    "    for i in range (matrix_shape[0]):\n",
    "        for j in range (matrix_shape[1]):\n",
    "            #do only once\n",
    "            if i < j:\n",
    "                sim = cosine_similarity_matrix[i, j]\n",
    "                graph_one_lang.add_edge(i, j, weight=sim)\n",
    "                graph_one_lang.add_edge(j, i, weight=sim)\n",
    "\n",
    "    return graph_one_lang\n",
    "\n",
    "def get_sentences_to_omit(original_tweets: List[str]):\n",
    "    cosine_similarity_matrix = get_similarity_matrix(original_tweets)\n",
    "    too_similar_ids = np.argwhere(cosine_similarity_matrix > 0.6)\n",
    "    sentences_to_omit = []\n",
    "    for pair_ids in too_similar_ids:\n",
    "        if pair_ids[0]<pair_ids[1]:\n",
    "            sentences_to_omit.append(pair_ids[1])\n",
    "\n",
    "    return sentences_to_omit\n",
    "\n",
    "def get_summary(df: pd.DataFrame, tag: str):\n",
    "\n",
    "    \"\"\"\n",
    "    main function used for getting summary\n",
    "    \"\"\"\n",
    "\n",
    "    df_one_sector = df[df.sectors.apply(lambda x: tag in x)][['entry_id', 'excerpt']]\n",
    "\n",
    "    final_summary = []\n",
    "\n",
    "    original_tweets = df_one_sector.excerpt.tolist() \n",
    "    tweet_ids = df_one_sector.entry_id.tolist() \n",
    "    n_entries = len(original_tweets)    \n",
    "\n",
    "    # omit too similar entries\n",
    "    \n",
    "    sentence_ids_to_omit = get_sentences_to_omit(original_tweets)\n",
    "\n",
    "    new_excerpts = [original_tweets[i] for i in range (n_entries) if i not in sentence_ids_to_omit]\n",
    "    new_ids = [tweet_ids[i] for i in range (n_entries) if i not in sentence_ids_to_omit] \n",
    "    # get cosine similarity matrix\n",
    "    cosine_similarity_matrix = get_similarity_matrix(new_excerpts)\n",
    "\n",
    "    # create graph from similarity matrix\n",
    "    graph_one_lang = build_graph(cosine_similarity_matrix)\n",
    "\n",
    "    # louvain community\n",
    "    partition = community.best_partition(graph_one_lang)\n",
    "\n",
    "    ids = []\n",
    "    tweets = []\n",
    "    partitions = []\n",
    "    for key, val in partition.items():\n",
    "        ids.append(new_ids[key])\n",
    "        tweets.append(new_excerpts[key])\n",
    "        partitions.append(val)\n",
    "\n",
    "    df_partition = pd.DataFrame(\n",
    "        list(zip(\n",
    "            ids, \n",
    "            tweets,\n",
    "            partitions\n",
    "            )),\n",
    "        \n",
    "        columns=['entry_id', 'excerpt', 'partition']\n",
    "    ).sort_values(by='partition', inplace=False)\n",
    "\n",
    "    #res: dict where key is the group of the sentence and value is a list of ids of that group\n",
    "    res = defaultdict(list)\n",
    "    for key, val in sorted(partition.items()):\n",
    "        res[val].append(key)\n",
    "\n",
    "    for key, val in res.items():\n",
    "        df_one_part = df_partition[df_partition.partition==key]\n",
    "        sentences = df_one_part.excerpt\n",
    "        similarity_one_item = get_similarity_matrix(sentences)\n",
    "\n",
    "        graph_one_lang = build_graph(similarity_one_item)\n",
    "\n",
    "        scores = nx.pagerank(graph_one_lang)\n",
    "\n",
    "        ranked_sentence = ' '.join(\n",
    "            list(\n",
    "                dict(sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)[:3]).values()\n",
    "                )\n",
    "        )\n",
    "\n",
    "        changed_text  = f'summarize: {ranked_sentence}'\n",
    "        input_ids = tokenizer(changed_text, return_tensors=\"pt\", truncation=False).input_ids\n",
    "        outputs = model.generate(input_ids, max_length=256)\n",
    "        summarized_entries = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        #clean summary to omit too similar senteneces\n",
    "        summarized_entries_as_sentences = nltk.tokenize.sent_tokenize(summarized_entries)\n",
    "        sentence_ids_to_omit = get_sentences_to_omit(summarized_entries_as_sentences)\n",
    "        text_one_cluster = [\n",
    "            summarized_entries_as_sentences[i] for i in range (len(summarized_entries_as_sentences))\\\n",
    "                if i not in sentence_ids_to_omit\n",
    "            ]\n",
    "        clean_summary_one_cluster = ' '.join(\n",
    "            [\n",
    "            text_one_cluster[i].title() if i!=0 else text_one_cluster[0] for i in range (len(text_one_cluster))\n",
    "            ]\n",
    "            \n",
    "        )\n",
    "\n",
    "        final_summary.append(clean_summary_one_cluster)\n",
    "\n",
    "    n_clusters = len(final_summary)\n",
    "    final_summary_str = ''\n",
    "\n",
    "    first_sent_link_tmp = first_sent_link.copy()\n",
    "    final_sent_link_tmp = final_sent_link.copy()\n",
    "    between_sequence_links_tmp = between_sequence_links.copy()\n",
    "\n",
    "    for i in range (n_clusters):\n",
    "        \n",
    "        if i == 0:\n",
    "            link_word = random.choice(first_sent_link_tmp)\n",
    "            first_sent_link_tmp.remove(link_word)\n",
    "        elif i == (n_clusters-1):\n",
    "            link_word = random.choice(final_sent_link_tmp)\n",
    "            final_sent_link_tmp.remove(link_word)\n",
    "        else:\n",
    "            link_word = random.choice(between_sequence_links_tmp)\n",
    "            between_sequence_links_tmp.remove(link_word)\n",
    "\n",
    "        final_summary_str = final_summary_str + link_word + final_summary[i] + '\\n'\n",
    "    \n",
    "    return final_summary_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Livelihoods\n",
      "First of all, two-thirds of migrants surveyed had to resort to coping strategy in 30 days. They Had To Resort To Stress, Crisis Or Emergency Livelihood Coping Strategy. Most Refugees And Migrants In Libya Are Unable To Find Work Necessary To Support Themselves And Meet Their Food Needs.\n",
      "Besides, a number of communities in the region have been affected by the loss of income. The Loss Of Income Continues To Impact Peopleâ€™S Ability To Cover Basic Needs And Pay Rent.\n",
      "Furthermore, key informants in all municipalities reported that residents were negatively affected. The Negative Impact On Residents Ranged From Difficulties Faced In Accessing Work And Livelihood Opportunities.\n",
      "Ultimately, the lack of liquidity to buy necessary inputs, including medicine, fodder and live animals were mentioned as the main problems affecting livestock production. The Lack Of Liquidity To Buy Necessary Inputs, Including Medicine, Fodder And Live Animals Was Experienced By Most Livestock Herders Prior To The Crisis Of 2011 Up Until Today.\n",
      "\n",
      "\n",
      "Food Security\n",
      "In the first place, more than 3,200 migrants estimated to be in detention centres. More Than 654,000 Migrants And Refugees Are Currently In Libya. Of Particular Concern Are The More Than 1,800 Migrants And Refugees Held In State-Run Detention Centers.\n",
      "Aditionally, a third of migrants are classified as food insecure (32%) according to food Consumption Scores and the Livelihood Strategies indicators. An Additional Third Are Considered Marginally Food Insecure (34%) And At Risk Of Food Insecurity.\n",
      "Besides, most IDP families are staying with host families and in rented accommodations. Food, Shelter, Health, Non-Food Items And Wash Are The Most Identified Needs.\n",
      "Also, 47% of migrant and refugee households are on food. The Decreased Cost Of The Food Has Improved Food Security Indicators. But 30 Per Cent Of Households Still Report Food Affordability Challenges.\n",
      "On the other hand, key informants in all municipalities reported that residents were negatively affected. The Negative Impact On Residents Ranged From Difficulties Faced In Accessing Work And Livelihood Opportunities. Disproportionately Affecting Those Dependent On Casual Labour Livelihoods And Daily Wages.\n",
      "Ultimately, food prices spiked in the majority of cities directly after the COVID-19 measures were implemented. In Some Cities, Authorities Reportedly Intervened To Correct These Price Hikes. Shortages Of Basic Food Items, Such As Eggs, Vegetables And Wheat Products Were Most Frequently Reported.\n",
      "\n",
      "\n",
      "Health\n",
      "First of all, as of 21 April 2020, there has been 11 conflict-related incidents recorded this year. The Al Khadra General Hospital Was Hit, Injuring At Least One Health Worker And Damaging The Fully Functioning 400-Bed Medical Facility. Heavy Clashes Also Saw Closures Of Four Hospitals In Sabratha And Surman That Were Providing An Average Of 18,000 Medical Consultations Per Week.\n",
      "Also, there is a total of 112 suspected cases, 120 cases have been tested and 125 people have been placed into quarantine. The Majority Of Confirmed Cases Are From The Mantikas Of Sebha (270), Tripoli (164) And Misrata (35)\n",
      "On the other hand, around 24 per cent of Libyans and 80 per cent of migrants and refugees reported challenges accessing health services. Many Public Health Care Facilities Are Closed And Those That Are Open Lack Medicines, Supplies And Equipment. Many Facilities Have Been Directly Attacked Or Damaged Due To Fighting.\n",
      "Aditionally, no trauma response is provided in al Jabal Al Akhdar, Aljufra, Almarj, Derna, Ejdabia, Ghat, Murzug, Nalut, Sirt, Tobruk, Ubari, Wadi Ashshati, Zwara. There Are Only 40 Phc Facilities Which Were Assisted With No Support In The Following Districts.\n",
      "Besides, between 1,500 and 2,000 cases of acute diarrhoea are reported each week. The Possibility Of A Cholera Outbreak Remains High54.\n",
      "Ultimately, shortages and 150% price increases for all staple food commodities. Health Centres Are Reportedly Struggling To Obtain These Items. Shortages And Price Spikes Are Expected To Continue And Worsen With The Ongoing Road Closures.\n",
      "\n",
      "\n",
      "Protection\n",
      "Firstly, more than 3,200 migrants estimated to be in detention centres. More Than 1,800 Migrants And Refugees Are Currently In The State-Run Detention Centers. Of Particular Concern Are The More Than 1,800 Migrants And Refugees Held In State-Run Detention Centers.\n",
      "Moreover, six civilian casualties were reported in the first quarter of 2020. the remaining seven civilian casualties were from ground fighting.\n",
      "Aditionally, more than 149,000 people have been forced to leave their home since the beginning of the conflict in April 2019. 749,000 People Are Estimated To Be In Areas Affected By Clashes. The Sustained Use Of Air Strikes And Artillery Shelling Continues To Negatively Impact The Safety And Lives Of The Civilian Population In Southern Tripoli Region.\n",
      "Finally, 7 out of 24 assessed municipalities migrants were reported to be unable to move freely within the municipality due to restrictions on freedom of movement as public health measures. 54% Of Migrants Were Reported To Be Unable To Leave Or Arrive At These Municipalities Due To Restrictions On Movement Imposed. Key Informants In All Municipalities (Except Tobruk) Reported That Residents Were Negatively Affected Due To Restrictions On Freedom Of Movement Imposed As Public Health Measures.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['Livelihoods', 'Food Security', 'Health', 'Protection']\n",
    "for tag in columns:\n",
    "    print(tag)\n",
    "    partitions = get_summary(libya_entries, tag)\n",
    "    print(partitions)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a5ddf8e25d962f331e8059973cfd97c5aef9d0ccfdd243943e9f1f512e91043"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('deepl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
