{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/selim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/selim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/selim/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/selim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from ast import literal_eval\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem import PorterStemmer  \n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "stop_words = set(stopwords.words())\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "import networkx as nx\n",
    "import community.community_louvain as community\n",
    "from ast import literal_eval\n",
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/selim/anaconda3/envs/deepl/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "DATA_PATH = os.path.join(\n",
    "    '..', '..', '..', '..', \"data\", \"frameworks_data\", 'data_v0.7.1'\n",
    ")\n",
    "\n",
    "full_df = pd.read_csv(os.path.join(DATA_PATH, 'full_dataset_with_translations.csv'))\n",
    "full_df['sectors'] = full_df.sectors.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Protection', 387),\n",
       " ('Health', 377),\n",
       " ('Cross', 368),\n",
       " ('Food Security', 163),\n",
       " ('Livelihoods', 136)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libya_entries = full_df[full_df.project_id==1621]\n",
    "Counter(flatten(libya_entries['sectors'])).most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(sentence):\n",
    "\n",
    "    if type(sentence) is not str:\n",
    "        sentence = str(sentence)\n",
    "\n",
    "    new_words = []\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        \n",
    "        #lower and remove punctuation\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
    "\n",
    "        #keep clean words and remove hyperlinks\n",
    "        word_not_nothing = new_word != ''\n",
    "        word_not_stop_word = new_word.lower() not in stop_words\n",
    "        #word_not_digit = ~new_word.isdigit()\n",
    "\n",
    "        if word_not_nothing and word_not_stop_word:\n",
    "\n",
    "            #lemmatize\n",
    "            new_word =  wordnet_lemmatizer.lemmatize(new_word, pos=\"v\")  \n",
    "\n",
    "            #stem\n",
    "            new_word = porter_stemmer.stem(new_word)\n",
    "\n",
    "            new_words.append(new_word)\n",
    "            \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def get_similarity_matrix(original_tweets):\n",
    "    \"\"\"\n",
    "    function to get similarity matrix from entries\n",
    "    \"\"\"\n",
    "    cleaned_tweet = [clean_tweets(one_tweet) for one_tweet in original_tweets] \n",
    "\n",
    "    #define and use tf-idf transformation\n",
    "    tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0)\n",
    "    tf_idf = tf.fit_transform(cleaned_tweet)\n",
    "\n",
    "    # get cosine similarity matrix\n",
    "    cosine_similarity_matrix = linear_kernel(tf_idf, tf_idf)\n",
    "    return cosine_similarity_matrix\n",
    "\n",
    "def build_graph(cosine_similarity_matrix):\n",
    "    \"\"\"\n",
    "    function to build graoh from similarity matrix\n",
    "    \"\"\"\n",
    "    graph_one_lang = nx.Graph()\n",
    "    matrix_shape = cosine_similarity_matrix.shape\n",
    "    for i in range (matrix_shape[0]):\n",
    "        for j in range (matrix_shape[1]):\n",
    "            #do only once\n",
    "            if i < j:\n",
    "                sim = cosine_similarity_matrix[i, j]\n",
    "                graph_one_lang.add_edge(i, j, weight=sim)\n",
    "                graph_one_lang.add_edge(j, i, weight=sim)\n",
    "\n",
    "    return graph_one_lang\n",
    "\n",
    "def get_sentences_to_omit(original_tweets: List[str]):\n",
    "    cosine_similarity_matrix = get_similarity_matrix(original_tweets)\n",
    "    too_similar_ids = np.argwhere(cosine_similarity_matrix > 0.6)\n",
    "    sentences_to_omit = []\n",
    "    for pair_ids in too_similar_ids:\n",
    "        if pair_ids[0]<pair_ids[1]:\n",
    "            sentences_to_omit.append(pair_ids[1])\n",
    "\n",
    "    return sentences_to_omit\n",
    "\n",
    "between_sequence_links = ['Furthermore,', 'Aditionally,', 'Moreover,', 'Besides,', 'On top of that,',]\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "def get_summary(df: pd.DataFrame, tag: str):\n",
    "\n",
    "    \"\"\"\n",
    "    main function used for getting summary\n",
    "    \"\"\"\n",
    "\n",
    "    df_one_sector = df[df.sectors.apply(lambda x: tag in x)][['entry_id', 'excerpt']]\n",
    "\n",
    "    final_summary = []\n",
    "\n",
    "    original_tweets = df_one_sector.excerpt.tolist() \n",
    "    tweet_ids = df_one_sector.entry_id.tolist() \n",
    "    n_entries = len(original_tweets)    \n",
    "\n",
    "    # omit too similar entries\n",
    "    \n",
    "    sentence_ids_to_omit = get_sentences_to_omit(original_tweets)\n",
    "\n",
    "    new_excerpts = [original_tweets[i] for i in range (n_entries) if i not in sentence_ids_to_omit]\n",
    "    new_ids = [tweet_ids[i] for i in range (n_entries) if i not in sentence_ids_to_omit] \n",
    "    # get cosine similarity matrix\n",
    "    cosine_similarity_matrix = get_similarity_matrix(new_excerpts)\n",
    "\n",
    "    # create graph from similarity matrix\n",
    "    graph_one_lang = build_graph(cosine_similarity_matrix)\n",
    "\n",
    "    # louvain community\n",
    "    partition = community.best_partition(graph_one_lang)\n",
    "\n",
    "    ids = []\n",
    "    tweets = []\n",
    "    partitions = []\n",
    "    for key, val in partition.items():\n",
    "        ids.append(new_ids[key])\n",
    "        tweets.append(new_excerpts[key])\n",
    "        partitions.append(val)\n",
    "\n",
    "    df_partition = pd.DataFrame(\n",
    "        list(zip(\n",
    "            ids, \n",
    "            tweets,\n",
    "            partitions\n",
    "            )),\n",
    "        \n",
    "        columns=['entry_id', 'excerpt', 'partition']\n",
    "    ).sort_values(by='partition', inplace=False)\n",
    "\n",
    "    #res: dict where key is the group of the sentence and value is a list of ids of that group\n",
    "    res = defaultdict(list)\n",
    "    for key, val in sorted(partition.items()):\n",
    "        res[val].append(key)\n",
    "\n",
    "    for key, val in res.items():\n",
    "        df_one_part = df_partition[df_partition.partition==key]\n",
    "        sentences = df_one_part.excerpt\n",
    "        similarity_one_item = get_similarity_matrix(sentences)\n",
    "\n",
    "        graph_one_lang = build_graph(similarity_one_item)\n",
    "\n",
    "        scores = nx.pagerank(graph_one_lang)\n",
    "\n",
    "        ranked_sentence = ' '.join(\n",
    "            list(\n",
    "                dict(sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)[:3]).values()\n",
    "                )\n",
    "        )\n",
    "\n",
    "        changed_text  = f'summarize: {ranked_sentence}'\n",
    "        input_ids = tokenizer(changed_text, return_tensors=\"pt\", truncation=False).input_ids\n",
    "        outputs = model.generate(input_ids, max_length=256)\n",
    "        summarized_entries = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        #clean summary to omit too similar senteneces\n",
    "        summarized_entries_as_sentences = nltk.tokenize.sent_tokenize(summarized_entries)\n",
    "        sentence_ids_to_omit = get_sentences_to_omit(summarized_entries_as_sentences)\n",
    "        clean_summary_one_cluster = ' '.join(\n",
    "            [summarized_entries_as_sentences[i] for i in range (len(summarized_entries_as_sentences)) if i not in sentence_ids_to_omit]\n",
    "        )\n",
    "\n",
    "        final_summary.append(clean_summary_one_cluster)\n",
    "\n",
    "    return '\\n'.join(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health\n",
      "as of 21 April 2020, there has been 11 conflict-related incidents recorded this year. the al Khadra general hospital was hit, injuring at least one health worker and damaging the fully functioning 400-bed medical facility. heavy clashes also saw closures of four hospitals in Sabratha and Surman that were providing an average of 18,000 medical consultations per week.\n",
      "there is a total of 112 suspected cases, 120 cases have been tested and 125 people have been placed into quarantine. the majority of confirmed cases are from the mantikas of sebha (270), Tripoli (164) and Misrata (35)\n",
      "around 24 per cent of Libyans and 80 per cent of migrants and refugees reported challenges accessing health services. many public health care facilities are closed and those that are open lack medicines, supplies and equipment. many facilities have been directly attacked or damaged due to fighting.\n",
      "no trauma response is provided in al Jabal Al Akhdar, Aljufra, Almarj, Derna, Ejdabia, Ghat, Murzug, Nalut, Sirt, Tobruk, Ubari, Wadi Ashshati, Zwara. there are only 40 PHC facilities which were assisted with no support in the following districts.\n",
      "between 1,500 and 2,000 cases of acute diarrhoea are reported each week. the possibility of a cholera outbreak remains high54.\n",
      "shortages and 150% price increases for all staple food commodities. health centres are reportedly struggling to obtain these items. shortages and price spikes are expected to continue and worsen with the ongoing road closures.\n",
      "\n",
      "\n",
      "Protection\n",
      "more than 3,200 migrants estimated to be in detention centres. more than 1,800 migrants and refugees are currently in the state-run detention centers. of particular concern are the more than 1,800 migrants and refugees held in state-run detention centers.\n",
      "six civilian casualties resulted from ground fighting between forces affiliated to the government of national Accord. six civilian casualties (five deaths and one injury) from targeted killings. six injuries from IEDs could not be attributed to a specific party to the conflict.\n",
      "more than 149,000 people have been forced to leave their home since the beginning of the conflict in April 2019. 749,000 people are estimated to be in areas affected by clashes. the sustained use of air strikes and artillery shelling continues to negatively impact the safety and lives of the civilian population in southern Tripoli region.\n",
      "7 out of 24 assessed municipalities migrants were reported to be unable to move freely within the municipality due to restrictions on freedom of movement as public health measures. 54% of migrants were reported to be unable to leave or arrive at these municipalities due to restrictions on movement imposed. key informants in all municipalities (except tobruk) reported that residents were negatively affected due to restrictions on freedom of movement imposed as public health measures.\n",
      "\n",
      "\n",
      "Livelihoods\n",
      "two-thirds of migrants surveyed had to resort to coping strategy in 30 days. they had to resort to stress, crisis or emergency livelihood coping strategy. most refugees and migrants in Libya are unable to find work necessary to support themselves and meet their food needs.\n",
      "a number of communities in the region have been affected by the loss of income. the loss of income continues to impact peopleâ€™s ability to cover basic needs and pay rent.\n",
      "key informants in all municipalities reported that residents were negatively affected. the negative impact on residents ranged from difficulties faced in accessing work and livelihood opportunities.\n",
      "80% of farmers are involved in crop production for their own consumption and income generation. many people were forced to cease agricultural production because they did not have the means to continue. the lack of liquidity to buy necessary inputs, including medicine, fodder and live animals, was mentioned as the main problems affecting livestock production.\n",
      "\n",
      "\n",
      "Food Security\n",
      "more than 3,200 migrants estimated to be in detention centres. more than 654,000 migrants and refugees are currently in Libya. of particular concern are the more than 1,800 migrants and refugees held in state-run detention centers.\n",
      "65% of migrants surveyed had to resort to coping strategies due to lack of food or means to buy food. one in five reported having to work in exchange for food (20%) and/or having to reduce expenditure on essential non-food items (19%).\n",
      "food, shelter, health, non-food items and WASH are the most identified needs by affected communities in eastern Libya.\n",
      "70% of migrants living in informal settings had inadequate food consumption levels 32% had inadequate food consumption level. an additional third are considered marginally food insecure (34%) and at risk of food insecurity.\n",
      "key informants in all municipalities (except tobruk) reported that residents were negatively affected due to restrictions on freedom of movement imposed as public health measures. the negative impact on residents ranged from difficulties faced in accessing work and livelihood opportunities, disproportionately affecting those dependent on casual labour livelihoods and daily wages.\n",
      "food prices spiked in the majority of cities directly after the COVID-19 measures were implemented. in some cities, authorities reportedly intervened to correct these price hikes. shortages of basic food items, such as eggs, vegetables and wheat products were most frequently reported.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['Health', 'Protection', 'Livelihoods', 'Food Security']\n",
    "for tag in columns:\n",
    "    print(tag)\n",
    "    partitions = get_summary(libya_entries, tag)\n",
    "    print(partitions)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a5ddf8e25d962f331e8059973cfd97c5aef9d0ccfdd243943e9f1f512e91043"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('deepl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
