{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/selim/anaconda3/envs/deepl/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "DATA_PATH = os.path.join(\n",
    "    '..', '..', '..', '..', \"data\", \"frameworks_data\", 'data_v0.7.1'\n",
    ")\n",
    "\n",
    "full_df = pd.read_csv(os.path.join(DATA_PATH, 'full_dataset_with_translations.csv'))\n",
    "full_df['sectors'] = full_df.sectors.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "libya_entries = full_df[full_df.project_id==1621]\n",
    "\n",
    "libya_food_entries = libya_entries[libya_entries.sectors.apply(lambda x: 'Food Security' in x)][['entry_id', 'excerpt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/selim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/selim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/selim/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem import PorterStemmer  \n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "stop_words = set(stopwords.words())\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "import networkx as nx\n",
    "import community.community_louvain as community\n",
    "from ast import literal_eval\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(sentence):\n",
    "\n",
    "    if type(sentence) is not str:\n",
    "        sentence = str(sentence)\n",
    "\n",
    "    new_words = []\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        \n",
    "        #lower and remove punctuation\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
    "\n",
    "        #keep clean words and remove hyperlinks\n",
    "        word_not_nothing = new_word != ''\n",
    "        word_not_stop_word = new_word.lower() not in stop_words\n",
    "        #word_not_digit = ~new_word.isdigit()\n",
    "\n",
    "        if word_not_nothing and word_not_stop_word:\n",
    "\n",
    "            #lemmatize\n",
    "            new_word =  wordnet_lemmatizer.lemmatize(new_word, pos=\"v\")  \n",
    "\n",
    "            #stem\n",
    "            new_word = porter_stemmer.stem(new_word)\n",
    "\n",
    "            new_words.append(new_word)\n",
    "            \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def get_similarity_matrix(original_tweets):\n",
    "    \"\"\"\n",
    "    function to get similarity matrix from entries\n",
    "    \"\"\"\n",
    "    cleaned_tweet = [clean_tweets(one_tweet) for one_tweet in original_tweets] \n",
    "\n",
    "    #define and use tf-idf transformation\n",
    "    tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0)\n",
    "    tf_idf = tf.fit_transform(cleaned_tweet)\n",
    "\n",
    "    # get cosine similarity matrix\n",
    "    cosine_similarity_matrix = linear_kernel(tf_idf, tf_idf)\n",
    "    return cosine_similarity_matrix\n",
    "\n",
    "def build_graph(cosine_similarity_matrix):\n",
    "    \"\"\"\n",
    "    function to build graoh from similarity matrix\n",
    "    \"\"\"\n",
    "    graph_one_lang = nx.Graph()\n",
    "    matrix_shape = cosine_similarity_matrix.shape\n",
    "    for i in range (matrix_shape[0]):\n",
    "        for j in range (matrix_shape[1]):\n",
    "            #do only once\n",
    "            if i < j:\n",
    "                sim = cosine_similarity_matrix[i, j]\n",
    "                graph_one_lang.add_edge(i, j, weight=sim)\n",
    "                graph_one_lang.add_edge(j, i, weight=sim)\n",
    "\n",
    "    return graph_one_lang\n",
    "\n",
    "between_sequence_links = ['Furthermore,', 'Aditionally,', 'Moreover,', 'Besides,', 'On top of that,',]\n",
    "\n",
    "def get_summary(df: pd.DataFrame):\n",
    "\n",
    "    final_summary = []\n",
    "\n",
    "    original_tweets = df.excerpt.tolist() \n",
    "    tweet_ids = df.entry_id.tolist() \n",
    "    n_entries = len(original_tweets)\n",
    "\n",
    "    # get cosine similarity matrix\n",
    "    cosine_similarity_matrix = get_similarity_matrix(original_tweets)\n",
    "\n",
    "    # omit too similar entries\n",
    "    too_similar_ids = np.argwhere(cosine_similarity_matrix > 0.6)\n",
    "    sentences_to_omit = []\n",
    "    for pair_ids in too_similar_ids:\n",
    "        if pair_ids[0]<pair_ids[1]:\n",
    "            sentences_to_omit.append(pair_ids[1])\n",
    "\n",
    "    print(len(sentences_to_omit))\n",
    "\n",
    "    new_excerpts = [original_tweets[i] for i in range (n_entries) if i not in sentences_to_omit]\n",
    "    new_ids = [tweet_ids[i] for i in range (n_entries) if i not in sentences_to_omit] \n",
    "    # get cosine similarity matrix\n",
    "    cosine_similarity_matrix = get_similarity_matrix(new_excerpts)\n",
    "\n",
    "    # create graph from similarity matrix\n",
    "    graph_one_lang = build_graph(cosine_similarity_matrix)\n",
    "\n",
    "    # louvain community\n",
    "    partition = community.best_partition(graph_one_lang)\n",
    "\n",
    "    ids = []\n",
    "    tweets = []\n",
    "    partitions = []\n",
    "    for key, val in partition.items():\n",
    "        ids.append(new_ids[key])\n",
    "        tweets.append(new_excerpts[key])\n",
    "        partitions.append(val)\n",
    "\n",
    "    df_partition = pd.DataFrame(\n",
    "        list(zip(\n",
    "            ids, \n",
    "            tweets,\n",
    "            partitions\n",
    "            )),\n",
    "        \n",
    "        columns=['entry_id', 'excerpt', 'partition']\n",
    "    ).sort_values(by='partition', inplace=False)\n",
    "\n",
    "    #res: dict where key is the group of the sentence and value is a list of ids of that group\n",
    "    res = defaultdict(list)\n",
    "    for key, val in sorted(partition.items()):\n",
    "        res[val].append(key)\n",
    "\n",
    "    for key, val in res.items():\n",
    "        df_one_part = df_partition[df_partition.partition==key]\n",
    "        sentences = df_one_part.excerpt\n",
    "        similarity_one_item = get_similarity_matrix(sentences)\n",
    "\n",
    "        graph_one_lang = build_graph(similarity_one_item)\n",
    "\n",
    "        scores = nx.pagerank(graph_one_lang)\n",
    "\n",
    "        ranked_sentence = ' '.join(\n",
    "            list(\n",
    "                dict(sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)[:4]).values()\n",
    "                )\n",
    "        )\n",
    "\n",
    "        changed_text  = f'summarize: {ranked_sentence}'\n",
    "        input_ids = tokenizer(changed_text, return_tensors=\"pt\", truncation=False).input_ids\n",
    "        outputs = model.generate(input_ids, max_length=256)\n",
    "        summarized_entries = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "\n",
    "        final_summary.append(summarized_entries)\n",
    "\n",
    "\n",
    "    return '\\n'.join(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "more than 3,200 migrants estimated to be in detention centres. of particular concern are those held in detention centres. of particular concern are those held in detention centres.\n",
      "65% of migrants surveyed had to resort to coping strategies due to lack of food or means to buy food. one in five reported having to work in exchange for food (20%) and/or having to reduce expenditure on essential non-food items (19%).\n",
      "food, shelter, health, non-food items and health assistance are the most identified needs by affected communities in eastern Libya. food, shelter, health, non-food items and WASH are the most identified needs by affected communities in eastern Libya.\n",
      "a third of migrants are considered marginally food insecure (34%) and at risk of food insecurity. the ‘marginally food secure’ households have managed to meet the minimum food consumption through adopting livelihood coping strategies.\n",
      "key informants in all municipalities (except tobruk) reported that residents were negatively affected. the negative impact on residents ranged from difficulties faced in accessing work and livelihood opportunities. disproportionately affecting those dependent on casual labour livelihoods and daily wages.\n",
      "food prices spiked in the majority of cities directly after the COVID-19 measures were implemented. a rapid assessment on market functionality from 30 March to 1 April reported food price spikes. a rapid assessment on market functionality from 30 March to 1 April reported food price spikes in the majority of cities directly after the measures were implemented.\n"
     ]
    }
   ],
   "source": [
    "partitions = get_summary(libya_food_entries)\n",
    "print(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a5ddf8e25d962f331e8059973cfd97c5aef9d0ccfdd243943e9f1f512e91043"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('deepl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
