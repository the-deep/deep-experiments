{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/selim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/selim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/selim/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/selim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from ast import literal_eval\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem import PorterStemmer  \n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "stop_words = set(stopwords.words())\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "import networkx as nx\n",
    "import community.community_louvain as community\n",
    "from ast import literal_eval\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/selim/anaconda3/envs/deepl/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "DATA_PATH = os.path.join(\n",
    "    '..', '..', '..', '..', \"data\", \"frameworks_data\", 'data_v0.7.1'\n",
    ")\n",
    "\n",
    "full_df = pd.read_csv(os.path.join(DATA_PATH, 'full_dataset_with_translations.csv'))\n",
    "full_df['sectors'] = full_df.sectors.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "libya_entries = full_df[full_df.project_id==1621]\n",
    "\n",
    "libya_food_entries = libya_entries[libya_entries.sectors.apply(lambda x: 'Food Security' in x)][['entry_id', 'excerpt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(sentence):\n",
    "\n",
    "    if type(sentence) is not str:\n",
    "        sentence = str(sentence)\n",
    "\n",
    "    new_words = []\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        \n",
    "        #lower and remove punctuation\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
    "\n",
    "        #keep clean words and remove hyperlinks\n",
    "        word_not_nothing = new_word != ''\n",
    "        word_not_stop_word = new_word.lower() not in stop_words\n",
    "        #word_not_digit = ~new_word.isdigit()\n",
    "\n",
    "        if word_not_nothing and word_not_stop_word:\n",
    "\n",
    "            #lemmatize\n",
    "            new_word =  wordnet_lemmatizer.lemmatize(new_word, pos=\"v\")  \n",
    "\n",
    "            #stem\n",
    "            new_word = porter_stemmer.stem(new_word)\n",
    "\n",
    "            new_words.append(new_word)\n",
    "            \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def get_similarity_matrix(original_tweets):\n",
    "    \"\"\"\n",
    "    function to get similarity matrix from entries\n",
    "    \"\"\"\n",
    "    cleaned_tweet = [clean_tweets(one_tweet) for one_tweet in original_tweets] \n",
    "\n",
    "    #define and use tf-idf transformation\n",
    "    tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0)\n",
    "    tf_idf = tf.fit_transform(cleaned_tweet)\n",
    "\n",
    "    # get cosine similarity matrix\n",
    "    cosine_similarity_matrix = linear_kernel(tf_idf, tf_idf)\n",
    "    return cosine_similarity_matrix\n",
    "\n",
    "def build_graph(cosine_similarity_matrix):\n",
    "    \"\"\"\n",
    "    function to build graoh from similarity matrix\n",
    "    \"\"\"\n",
    "    graph_one_lang = nx.Graph()\n",
    "    matrix_shape = cosine_similarity_matrix.shape\n",
    "    for i in range (matrix_shape[0]):\n",
    "        for j in range (matrix_shape[1]):\n",
    "            #do only once\n",
    "            if i < j:\n",
    "                sim = cosine_similarity_matrix[i, j]\n",
    "                graph_one_lang.add_edge(i, j, weight=sim)\n",
    "                graph_one_lang.add_edge(j, i, weight=sim)\n",
    "\n",
    "    return graph_one_lang\n",
    "\n",
    "def get_sentences_to_omit(original_tweets: List[str]):\n",
    "    cosine_similarity_matrix = get_similarity_matrix(original_tweets)\n",
    "    too_similar_ids = np.argwhere(cosine_similarity_matrix > 0.6)\n",
    "    sentences_to_omit = []\n",
    "    for pair_ids in too_similar_ids:\n",
    "        if pair_ids[0]<pair_ids[1]:\n",
    "            sentences_to_omit.append(pair_ids[1])\n",
    "\n",
    "    return sentences_to_omit\n",
    "\n",
    "between_sequence_links = ['Furthermore,', 'Aditionally,', 'Moreover,', 'Besides,', 'On top of that,',]\n",
    "\n",
    "def get_summary(df: pd.DataFrame):\n",
    "\n",
    "    \"\"\"\n",
    "    main function used for getting summary\n",
    "    \"\"\"\n",
    "\n",
    "    final_summary = []\n",
    "\n",
    "    original_tweets = df.excerpt.tolist() \n",
    "    tweet_ids = df.entry_id.tolist() \n",
    "    n_entries = len(original_tweets)    \n",
    "\n",
    "    # omit too similar entries\n",
    "    \n",
    "    sentence_ids_to_omit = get_sentences_to_omit(original_tweets)\n",
    "\n",
    "    new_excerpts = [original_tweets[i] for i in range (n_entries) if i not in sentence_ids_to_omit]\n",
    "    new_ids = [tweet_ids[i] for i in range (n_entries) if i not in sentence_ids_to_omit] \n",
    "    # get cosine similarity matrix\n",
    "    cosine_similarity_matrix = get_similarity_matrix(new_excerpts)\n",
    "\n",
    "    # create graph from similarity matrix\n",
    "    graph_one_lang = build_graph(cosine_similarity_matrix)\n",
    "\n",
    "    # louvain community\n",
    "    partition = community.best_partition(graph_one_lang)\n",
    "\n",
    "    ids = []\n",
    "    tweets = []\n",
    "    partitions = []\n",
    "    for key, val in partition.items():\n",
    "        ids.append(new_ids[key])\n",
    "        tweets.append(new_excerpts[key])\n",
    "        partitions.append(val)\n",
    "\n",
    "    df_partition = pd.DataFrame(\n",
    "        list(zip(\n",
    "            ids, \n",
    "            tweets,\n",
    "            partitions\n",
    "            )),\n",
    "        \n",
    "        columns=['entry_id', 'excerpt', 'partition']\n",
    "    ).sort_values(by='partition', inplace=False)\n",
    "\n",
    "    #res: dict where key is the group of the sentence and value is a list of ids of that group\n",
    "    res = defaultdict(list)\n",
    "    for key, val in sorted(partition.items()):\n",
    "        res[val].append(key)\n",
    "\n",
    "    for key, val in res.items():\n",
    "        df_one_part = df_partition[df_partition.partition==key]\n",
    "        sentences = df_one_part.excerpt\n",
    "        similarity_one_item = get_similarity_matrix(sentences)\n",
    "\n",
    "        graph_one_lang = build_graph(similarity_one_item)\n",
    "\n",
    "        scores = nx.pagerank(graph_one_lang)\n",
    "\n",
    "        ranked_sentence = ' '.join(\n",
    "            list(\n",
    "                dict(sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)[:4]).values()\n",
    "                )\n",
    "        )\n",
    "\n",
    "        changed_text  = f'summarize: {ranked_sentence}'\n",
    "        input_ids = tokenizer(changed_text, return_tensors=\"pt\", truncation=False).input_ids\n",
    "        outputs = model.generate(input_ids, max_length=256)\n",
    "        summarized_entries = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        #clean summary to omit too similar senteneces\n",
    "        summarized_entries_as_sentences = nltk.tokenize.sent_tokenize(summarized_entries)\n",
    "        sentence_ids_to_omit = get_sentences_to_omit(summarized_entries_as_sentences)\n",
    "        clean_summary_one_cluster = ' '.join(\n",
    "            [summarized_entries_as_sentences[i] for i in range (len(summarized_entries_as_sentences)) if i not in sentence_ids_to_omit]\n",
    "        )\n",
    "\n",
    "        final_summary.append(clean_summary_one_cluster)\n",
    "\n",
    "    return '\\n'.join(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more than 3,200 migrants estimated to be in detention centres. of particular concern are those held in detention centres.\n",
      "65% of migrants surveyed had to resort to coping strategies due to lack of food or means to buy food. one in five migrants reported having to work in exchange for food (20%) and/or having to reduce expenditure on essential non-food items (19%).\n",
      "food, shelter, health, non-food items and health assistance are the most identified needs by affected communities in eastern Libya.\n",
      "food production has also been impacted, particularly in the south. food production has also been impacted, particularly in the informal economy.\n",
      "70% of migrants living in informal settings had inadequate food consumption levels. nearly half of migrants had borderline or poor consumption levels. living in unstable and improper accommodation seems to negatively influence food consumption.\n",
      "food prices spiked in the majority of cities directly after the COVID-19 measures were implemented. a rapid assessment on market functionality from 30 March to 1 April reported food price spikes.\n"
     ]
    }
   ],
   "source": [
    "partitions = get_summary(libya_food_entries)\n",
    "print(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a5ddf8e25d962f331e8059973cfd97c5aef9d0ccfdd243943e9f1f512e91043"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('deepl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
