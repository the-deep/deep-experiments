{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/selim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/selim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/selim/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/selim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from ast import literal_eval\n",
    "#from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "#from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem import PorterStemmer  \n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "stop_words = set(stopwords.words())\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import networkx as nx\n",
    "import community.community_louvain as community\n",
    "from ast import literal_eval\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/selim/anaconda3/envs/deepl/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "DATA_PATH = os.path.join(\n",
    "    '..', '..', '..', '..', \"data\", \"frameworks_data\", 'data_v0.7.1'\n",
    ")\n",
    "\n",
    "full_df = pd.read_csv(os.path.join(DATA_PATH, 'full_dataset_with_translations.csv'))\n",
    "\n",
    "kept_cols = ['sectors', 'affected_groups_level_3', 'specific_needs_groups', 'age', 'gender', 'subpillars_1d', 'subpillars_2d', 'severity']\n",
    "for col in kept_cols:\n",
    "    full_df[col] = full_df[col].apply(custom_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = ['entry_id', 'excerpt', 'project_id'] + kept_cols \n",
    "full_df[full_df.project_id.isin([1621, 2225, 2311])][all_cols].to_csv('summarization_df.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['pillars_2d'] = full_df.subpillars_2d.apply(lambda x: [item.split('->')[0] for item in x])\n",
    "full_df['pillars_1d'] = full_df.subpillars_1d.apply(lambda x: [item.split('->')[0] for item in x])\n",
    "full_df['affected_groups'] = full_df.affected_groups_level_3.apply(\n",
    "    lambda x: [item for item in x if item not in ['None', 'Others of Concern']]\n",
    ")\n",
    "full_df['sectors'] = full_df.sectors.apply(\n",
    "    lambda x: [item for item in x if item not in ['Cross']]\n",
    ")\n",
    "full_df['n_sectors'] = full_df.sectors.apply(\n",
    "    lambda x: len(x)\n",
    ")\n",
    "full_df['severity_scores'] = full_df['severity'].apply(get_severity_score)\n",
    "\n",
    "#Counter(flatten(libya_df.severity)).most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"project_id_to_name = {\n",
    "    1621: 'DFS Libya',\n",
    "    2225: 'IMMAP/DFS RDC',\n",
    "    2311: 'IMMAP/DFS Colombia'\n",
    "}\n",
    "\n",
    "def preprocess_entry(entry):\n",
    "    def remove_punct(sentence):\n",
    "        import string\n",
    "        return sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    def omit_stop_words(sentence):\n",
    "        return ' '.join([word for word in sentence.split() if word.lower() not in stop_words])\n",
    "\n",
    "    return omit_stop_words(remove_punct(entry))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"between_sequence_links = [\n",
    "    'Furthermore, ', 'Aditionally, ', 'Moreover, ', 'Besides, ', 'Aside from that, ', 'Also, ', 'In addition to that, ',\n",
    "    'On the other hand, ', 'On the other side, '\n",
    "    ]\n",
    "final_sent_link = ['Finally, ', 'Ultimately, ']\n",
    "first_sent_link = ['Firstly, ', 'First of all, ', 'In the first place, ']\"\"\"\n",
    "\n",
    "def get_summary_one_row(row):\n",
    "\n",
    "    \"\"\"\n",
    "    function used for getting summary for one task\n",
    "    \"\"\"\n",
    "    final_summary = []\n",
    "\n",
    "    original_excerpt = row.excerpt\n",
    "    severity_scores = row.severity_scores\n",
    "    top_n_sentences = row.n_clusters\n",
    "\n",
    "    cosine_similarity_matrix = get_similarity_matrix(original_excerpt)\n",
    "\n",
    "    graph_one_lang = build_graph(cosine_similarity_matrix)\n",
    "\n",
    "    scores_pagerank = nx.pagerank(graph_one_lang)\n",
    "    scores = {key: value * severity_scores[key] for key, value in scores_pagerank.items()}\n",
    "\n",
    "    top_n_sentence_ids = np.argsort(np.array(list(scores.values())))[::-1][:top_n_sentences]\n",
    "    used_ids = []\n",
    "\n",
    "    for id_tmp in top_n_sentence_ids:\n",
    "        row_id = cosine_similarity_matrix[id_tmp, :]\n",
    "        top_id_row = np.argsort(row_id)[::-1]\n",
    "        top_id_row = [id for id in top_id_row if id not in used_ids and id not in top_n_sentence_ids][:1]\n",
    "\n",
    "        top_2_id_row = [id_tmp] + top_id_row\n",
    "\n",
    "        used_ids += top_2_id_row\n",
    "\n",
    "        ranked_sentence = ' '.join([original_excerpt[id_tmp] for id_tmp in (top_2_id_row)]) \n",
    "            \n",
    "        summarized_entries = t2t_generation(ranked_sentence)\n",
    "        capitalized_summaries = ' '.join([sent.capitalize() for sent in nltk.tokenize.sent_tokenize(summarized_entries)])\n",
    "        final_summary.append(capitalized_summaries)\n",
    "\n",
    "    final_summary_str = ' '.join(final_summary)\n",
    "    \n",
    "    return final_summary_str\n",
    "\n",
    "def get_summary_one_part(df, col_name: str):\n",
    "\n",
    "    preprocessed_df = process_df(df, col_name)\n",
    "\n",
    "    final_string_one_part = '\\n'.join(\n",
    "        [get_summary_one_row(row) for index, row in preprocessed_df.iterrows()]\n",
    "    )\n",
    "    print(final_string_one_part)\n",
    "    return final_string_one_part\n",
    "\n",
    "def process_df(df: pd.DataFrame, col_name: str):\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy['tmp_tag_str'] = df_copy[col_name].apply(str)\n",
    "\n",
    "    grouped_df = df_copy.groupby('tmp_tag_str', as_index=False)[['entry_id', 'excerpt', 'severity_scores']].agg(lambda x: list(x))\n",
    "\n",
    "    grouped_df['len'] = grouped_df['entry_id'].apply(lambda x: len(x))\n",
    "    grouped_df = grouped_df[grouped_df.len>=3]\n",
    "    grouped_df['n_clusters'] = grouped_df['len'].apply(get_number_of_clusters)\n",
    "\n",
    "    grouped_df.sort_values(by='len', ascending=False, inplace=False).drop(columns='tmp_tag_str', inplace=False)\n",
    "\n",
    "    return grouped_df\n",
    "\n",
    "def get_report(full_df, tag: str, project_id: int):\n",
    "    \"\"\"\n",
    "    main function to get full report\n",
    "    \"\"\"\n",
    "    df = full_df[full_df.project_id==project_id].copy()\n",
    "    df_one_sector = df[df.sectors.apply(lambda x: tag in x)]\n",
    "    print(df_one_sector.shape)\n",
    "\n",
    "    summaries = defaultdict()\n",
    "    \"\"\"\n",
    "    # secondary tags\n",
    "    sec_tags_df = df_one_sector[\n",
    "        (df_one_sector.affected_groups_level_3.apply(lambda x: len(x)>0)) | \n",
    "        (df_one_sector.specific_needs_groups.apply(lambda x: len(x)>0))\n",
    "        ]\n",
    "    sec_tags_df['secondary_tags'] = sec_tags_df.apply(\n",
    "        lambda x: x.specific_needs_groups + x.affected_groups_level_3, axis=1\n",
    "    )\n",
    "    summaries['affected_specific'] = get_summary_one_part(sec_tags_df, 'secondary_tags')\n",
    "\n",
    "    # key trends\n",
    "    specific_tag_df = df_one_sector[df_one_sector.n_sectors==1]\n",
    "    specific_tag_df['subpillars'] = specific_tag_df.apply(\n",
    "        lambda x: x.subpillars_1d + x.subpillars_2d, axis=1\n",
    "    )\n",
    "    summaries['key_trends'] = get_summary_one_part(specific_tag_df, 'subpillars')\"\"\"\n",
    "\n",
    "\n",
    "    #relation to other sectors\n",
    "    many_tags_df = df_one_sector[df_one_sector.n_sectors>1]\n",
    "    many_tags_df['sectors'] = many_tags_df.sectors.apply(\n",
    "        lambda x: [item for item in x if item!='Cross']\n",
    "    )\n",
    "    relation_to_other_sectors_summary = ''\n",
    "\n",
    "    many_sectors_df = many_tags_df[many_tags_df.sectors.apply(lambda x: len(x)>2)]\n",
    "    relation_to_other_sectors_summary += get_summary_one_part(many_sectors_df, 'sectors')\n",
    "\n",
    "    two_sectors_df = many_tags_df[many_tags_df.sectors.apply(lambda x: len(x)==2)]\n",
    "    relation_to_other_sectors_summary =\\\n",
    "        relation_to_other_sectors_summary + '\\n' + get_summary_one_part(two_sectors_df, 'sectors')\n",
    "\n",
    "    summaries['other_sectors'] = relation_to_other_sectors_summary\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(377, 27)\n"
     ]
    }
   ],
   "source": [
    "full_report = get_report(full_df, 'Health', 1621)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"columns = ['Health']\\n\\n#for one_project_id in list(project_id_to_name.keys()):\\n#    print(f'FOR THE PROJECT {project_id_to_name[one_project_id]}')\\none_project_id = 1621\\n\\nfor tag in columns:\\n    print(tag)\\n    partitions = get_summary(full_df, tag, one_project_id)\\n    print(partitions)\\n    print('')\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"columns = ['Health']\n",
    "\n",
    "#for one_project_id in list(project_id_to_name.keys()):\n",
    "#    print(f'FOR THE PROJECT {project_id_to_name[one_project_id]}')\n",
    "one_project_id = 1621\n",
    "\n",
    "for tag in columns:\n",
    "    print(tag)\n",
    "    partitions = get_summary(full_df, tag, one_project_id)\n",
    "    print(partitions)\n",
    "    print('')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a5ddf8e25d962f331e8059973cfd97c5aef9d0ccfdd243943e9f1f512e91043"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('deepl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
