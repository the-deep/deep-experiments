{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from deep_parser import TextFromFile, TextFromWeb\n",
    "from deep_parser.helpers.errors import DocumentProcessingError\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import base64, json\n",
    "import timeout_decorator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_name = \"all_hum_data_24-12-2022.csv.gz\"\n",
    "#leads_df = pd.read_csv(\n",
    "#    df_name,\n",
    "#    compression=\"gzip\",\n",
    "#    low_memory=False,\n",
    "#    lineterminator=\"\\n\",\n",
    "#    usecols=[\"lead_id\", \"project_id\", \"url\"],\n",
    "#).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_df.to_csv('projects_leads_urls.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'pulled_leads'\n",
    "output_path = Path(output_path)\n",
    "output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "non_url_input_path = Path('pdfs-total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_websites():\n",
    "    print(\"###################### Start puling websites data.\")\n",
    "    \n",
    "    for (project_id, lead_id), group in tqdm(\n",
    "        list(leads_df.groupby([\"project_id\", \"lead_id\"]))\n",
    "    ):\n",
    "        project_id, lead_id = str(int(project_id)), str(int(lead_id))\n",
    "        urls = group[\"url\"].unique()\n",
    "\n",
    "        assert len(urls) == 1\n",
    "\n",
    "        url = urls[0]\n",
    "\n",
    "        if not isinstance(url, str):  # possibly np.nan\n",
    "            continue\n",
    "\n",
    "        if url.endswith(\".pdf\"):\n",
    "            print(f\"{url} is a PDF!\")\n",
    "            try:\n",
    "                parser = TextFromFile(url=url, from_web=True)\n",
    "                text, _ = parser.extract_text(output_format=\"list\")\n",
    "            except (RuntimeError, DocumentProcessingError, Exception) as e:\n",
    "                print(f\"Error {e} on PDF url {url}\")\n",
    "                continue\n",
    "        else:\n",
    "            try:\n",
    "                parser = TextFromWeb(url=url)\n",
    "                text = parser.extract_text(output_format=\"list\")\n",
    "                parser.close()\n",
    "            except (RuntimeError, DocumentProcessingError, Exception) as e:\n",
    "                print(f\"Error {e} on standard url {url}\")\n",
    "                continue\n",
    "\n",
    "        nested_dir_path = output_path / project_id\n",
    "        nested_dir_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        file_path = nested_dir_path / f\"{lead_id}.json\"\n",
    "        if file_path.exists():\n",
    "            continue\n",
    "\n",
    "        text = None\n",
    "\n",
    "        with open(file_path, \"w+\") as f:\n",
    "            json.dump(text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeout_decorator.timeout(5 * 60, use_signals=False)\n",
    "def extract(in_path, out_path):\n",
    "    if out_path.exists():\n",
    "        return False\n",
    "\n",
    "    with open(in_path, \"rb\") as f:\n",
    "        binary = base64.b64encode(f.read())\n",
    "\n",
    "    try:\n",
    "        document = TextFromFile(stream=binary, ext=\"pdf\")\n",
    "        text, _ = document.extract_text(output_format=\"list\")\n",
    "    except (RuntimeError, DocumentProcessingError):\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        # open(out_path, \"w\").write(text)\n",
    "        with open(out_path, \"w+\") as f:\n",
    "            json.dump(text, f)\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def work(in_path, out_path):\n",
    "    try:\n",
    "        return extract(in_path, out_path)\n",
    "    except timeout_decorator.TimeoutError:\n",
    "        print(f\"Timeout for File {in_path}!\")\n",
    "\n",
    "\n",
    "def pull_pdfs():\n",
    "    print(\"###################### Start pulling pdf data.\")\n",
    "\n",
    "    name_to_id = {\n",
    "        row[\"url\"].rstrip(\"/\").split(\"/\")[-1]: (int(row[\"lead_id\"]), int(row[\"project_id\"]))\n",
    "        for _, row in leads_df.iterrows()\n",
    "        if row[\"url\"] is not np.nan\n",
    "    }\n",
    "\n",
    "    paths = [Path(p) for p in glob(str(non_url_input_path / \"*.pdf\"))]\n",
    "\n",
    "    for in_path in tqdm(paths):\n",
    "        name = Path(in_path).name\n",
    "        lead_id, project_id = name_to_id[name]\n",
    "\n",
    "        out_path = output_path / str(project_id) / f\"{lead_id}.json\"\n",
    "        out_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        work(in_path, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repull data\n",
    "# pull urls\n",
    "# pull pdf\n",
    "# solve duplicates issue\n",
    "# run code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web",
   "language": "python",
   "name": "web"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a5ddf8e25d962f331e8059973cfd97c5aef9d0ccfdd243943e9f1f512e91043"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
