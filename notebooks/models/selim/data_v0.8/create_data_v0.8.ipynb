{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from typing import List, Union, Dict\n",
    "from collections import defaultdict\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display\n",
    "import re\n",
    "import operator\n",
    "from copy import copy\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t: List[List]) -> List:\n",
    "    \"\"\"flatten list of lists\"\"\"\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "def custom_eval(x):\n",
    "    if str(x)=='nan':\n",
    "        return {}\n",
    "    if str(x)=='[None]':\n",
    "        return {}\n",
    "    if type(x)==list:\n",
    "        return x\n",
    "    if type(x) is dict:\n",
    "        return x    \n",
    "    else:\n",
    "        return literal_eval(x)\n",
    "\n",
    "def item2list(item):\n",
    "    if type(item) is list:\n",
    "        return list(set(item))\n",
    "    else:\n",
    "        return [item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = 'all_hum_data_21-12-2022.csv.gz'\n",
    "data = pd.read_csv(df_name, compression = 'gzip', low_memory=False,\n",
    "                 lineterminator='\\n')\n",
    "\n",
    "geo_location_df = pd.read_csv('tables/geo_locations.csv', usecols=['id', 'title'])\n",
    "geo_locations_dict = dict(zip(geo_location_df['id'].tolist(), geo_location_df['title'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['entry_id', 'created_at', 'excerpt', 'analysis_framework_id', 'lead_id',\n",
       "       'project_id', 'title', 'outputs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337231/337231 [00:36<00:00, 9206.17it/s] \n"
     ]
    }
   ],
   "source": [
    "data['outputs'] = data['outputs'].progress_apply(custom_eval)\n",
    "data['all'] = data['outputs'].apply(\n",
    "    lambda x: [item2list(item) for item in list(x.values())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### final mapping sheet\n",
    "ordered_columns = ['Original first level',\n",
    "       'Original second level', 'Original third level', 'NLP Type', 'NLP first level',\n",
    "       'NLP second level', 'NLP third level', 'NLP fourth level']\n",
    "\n",
    "hum_mapping_sheet = mapping_sheet_primary_tags = pd.read_csv(\n",
    "    \"tables/mapping_sheet_second_version_tmp.csv\"\n",
    ").drop(columns=[\"Reversible\", \"Remarks\"])[ordered_columns].reset_index(drop=True)\n",
    "original_sheet = hum_mapping_sheet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3128, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_levels_cols = [\n",
    "    \"Original first level\",\n",
    "    \"Original second level\",\n",
    "    \"Original third level\",\n",
    "]\n",
    "\n",
    "mapped_levels_cols = [\n",
    "    \"NLP first level\",\n",
    "    \"NLP second level\",\n",
    "    \"NLP third level\",\n",
    "]\n",
    "\n",
    "for one_level in original_levels_cols:\n",
    "    hum_mapping_sheet[one_level] = hum_mapping_sheet[one_level].apply(\n",
    "        lambda x: x.lower().replace(\"\\t\", \"\").replace(\"•\", \"\").strip()\n",
    "        if type(x) is str\n",
    "        else x\n",
    "    )\n",
    "\n",
    "for one_level in mapped_levels_cols:\n",
    "    hum_mapping_sheet[one_level] = hum_mapping_sheet[one_level].apply(\n",
    "        lambda x: x.capitalize().strip() if type(x) is str else x\n",
    "    )\n",
    "\n",
    "hum_mapping_sheet.drop_duplicates(inplace=True)\n",
    "\n",
    "hum_mapping_sheet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_TYPES = [\n",
    "    \"2D\",\n",
    "    \"nan\",\n",
    "    \"1D\",\n",
    "    \"Sector\",\n",
    "    \"DEMOGRAPHIC GROUPS\",\n",
    "    \"SPECIFIC NEEDS GROUPS\",\n",
    "    \"AFFECTED GROUPS\",\n",
    "    \"SEVERITY\",\n",
    "    \"RELIABILITY\",\n",
    "]\n",
    "\n",
    "#TODO: review this and add demographic groups new level\n",
    "\n",
    "def get_final_nlp_name(row: pd.Series):\n",
    "\n",
    "    final_outputs_one_row = []\n",
    "\n",
    "    if str(row[\"NLP Type\"]) != \"nan\" and str(row[\"NLP first level\"]) != \"nan\":\n",
    "\n",
    "        final_str = row[\"NLP Type\"].lower().replace(\"•\", \"\").replace(\"\\t\", \"\").strip().replace(\" \", \"_\")\n",
    "\n",
    "        ### affected groups\n",
    "        if final_str == \"affected_groups\":\n",
    "            if row[\"NLP first level\"] == \"Affected\" and str(row['NLP second level'])!='nan':\n",
    "\n",
    "                final_outputs_one_row.append(\n",
    "                    f\"first_level_tags->Affected->{row['NLP second level'].capitalize()}\"\n",
    "                )\n",
    "                if str(row['NLP third level'])!='nan':\n",
    "                    final_outputs_one_row.append(\n",
    "                        f\"secondary_tags->{row['NLP second level'].capitalize()}->{row['NLP third level'].capitalize()}\"\n",
    "                    )\n",
    "                if str(row['NLP fourth level'])!='nan':\n",
    "                    final_outputs_one_row.append(\n",
    "                        f\"secondary_tags->{row['NLP second level'].capitalize()}->{row['NLP fourth level'].capitalize()}\"\n",
    "                    )\n",
    "                    \n",
    "        ### severity and reliability and specific needs groups\n",
    "        elif final_str in [\"reliability\", \"severity\", \"specific_needs_groups\"]:\n",
    "            final_outputs_one_row.append(\n",
    "                f\"secondary_tags->{final_str}->{row['NLP first level'].capitalize()}\"\n",
    "            )\n",
    "\n",
    "        ### demographic groups\n",
    "        elif final_str == \"demographic_groups\":\n",
    "            if str(row[\"NLP first level\"]) != \"nan\":\n",
    "                final_outputs_one_row.append(\n",
    "                    f\"secondary_tags->Gender->{row['NLP first level'].capitalize()}\"\n",
    "                )\n",
    "            if str(row[\"NLP second level\"]) != \"nan\":\n",
    "                final_outputs_one_row.append(\n",
    "                    f\"secondary_tags->All->{row['NLP second level'].capitalize()}\"\n",
    "                )\n",
    "            if str(row[\"NLP third level\"]) != \"nan\":\n",
    "                final_outputs_one_row.append(\n",
    "                    f\"secondary_tags->All->{row['NLP third level'].capitalize()}\"\n",
    "                )\n",
    "            if str(row[\"NLP fourth level\"]) != \"nan\":\n",
    "                final_outputs_one_row.append(\n",
    "                    f\"secondary_tags->All->{row['NLP fourth level'].capitalize()}\"\n",
    "                )\n",
    "\n",
    "        elif final_str == \"sector\":\n",
    "            final_outputs_one_row.append(\n",
    "                f\"first_level_tags->sectors->{row['NLP first level'].capitalize()}\"\n",
    "            )\n",
    "            if str(row[\"NLP second level\"]) != \"nan\":\n",
    "                final_outputs_one_row.append(\n",
    "                    f\"subsectors->{row['NLP first level'].capitalize()}->{row['NLP second level'].capitalize()}\"\n",
    "                )\n",
    "\n",
    "        ### 1d and 2d subpillars\n",
    "        elif final_str in [\"1d\", \"2d\"]:\n",
    "            # sometimes only pillar\n",
    "            if str(row[\"NLP second level\"]) == \"nan\":\n",
    "                final_outputs_one_row.append(\n",
    "                    f\"first_level_tags->pillars_{final_str[0]}d->{row['NLP first level'].capitalize()}\"\n",
    "                )\n",
    "            else:\n",
    "                final_outputs_one_row.append(\n",
    "                    f\"subpillars_{final_str[0]}d->{row['NLP first level'].capitalize()}->{row['NLP second level'].capitalize()}\"\n",
    "                )\n",
    "                final_outputs_one_row.append(\n",
    "                    f\"first_level_tags->pillars_{final_str[0]}d->{row['NLP first level'].capitalize()}\"\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            print(final_str)\n",
    "            raise (Exception(\"problem!\"))\n",
    "\n",
    "    return final_outputs_one_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hum_mapping_sheet['mapped_nlp'] = hum_mapping_sheet.apply(\n",
    "    lambda x: get_final_nlp_name(x), axis=1\n",
    ")\n",
    "\n",
    "nlp_tags_mapping_sheet = sorted(list(set(flatten(hum_mapping_sheet['mapped_nlp']))))\n",
    "len(nlp_tags_mapping_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_widgets = [\n",
    "    \"matrix2dWidget\",\n",
    "    \"multiselectWidget\",\n",
    "    \"no_common_matrix2dWidget\",\n",
    "    \"organigramWidget\",\n",
    "    \"selectWidget\",\n",
    "    \"raw\",\n",
    "    \"scaleWidget\",\n",
    "    \"no_common_multiselectWidget\",\n",
    "    \"matrix1dWidget\",\n",
    "]\n",
    "\n",
    "original_levels_cols = ['Original first level', 'Original second level', 'Original third level']\n",
    "date_regex = re.compile(r\"\\d\\d-\\d\\d-\\d\\d\\d\\d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337231/337231 [02:33<00:00, 2193.96it/s] \n"
     ]
    }
   ],
   "source": [
    "mapping_dict = defaultdict(list)# {one_kw: [f\"first_level_tags->Affected->{one_kw.capitalize()}\"] for one_kw in ['migrants', 'affected', 'non displaced', 'displaced']}\n",
    "\n",
    "too_many_rows, no_mapping = set(), set()\n",
    "\n",
    "nlp_all_outputs = []\n",
    "\n",
    "for one_output in tqdm(data[\"outputs\"].tolist()):\n",
    "\n",
    "    nlp_one_output = defaultdict(list)\n",
    "\n",
    "    ####### dates\n",
    "    dates_tmp = []\n",
    "\n",
    "    for one_date_widget_type in [\"dateRangeWidget\", \"dateWidget\"]:\n",
    "        if one_date_widget_type in one_output:\n",
    "            dates_tmp.extend(\n",
    "                [\n",
    "                    one_date\n",
    "                    for one_date in one_output[one_date_widget_type]\n",
    "                    if one_date is not None\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    ##### geo_locations\n",
    "    if \"geoWidget\" in one_output:\n",
    "        geo_location_output = [\n",
    "            geo_locations_dict.get(one_loc_id) for one_loc_id in one_output[\"geoWidget\"]\n",
    "        ]\n",
    "    else:\n",
    "        geo_location_output = []\n",
    "\n",
    "    ##### nlp mapping widgets\n",
    "    for one_widget_type in mapping_widgets:\n",
    "        if one_widget_type in one_output:\n",
    "            outputs_one_widget = one_output[one_widget_type]\n",
    "            \"\"\"if type(outputs_one_widget[0]) is list:\n",
    "                outputs_one_widget = flatten(outputs_one_widget)\"\"\"\n",
    "            # print(outputs_one_widget)\n",
    "            for item in outputs_one_widget:\n",
    "                \"\"\"item = (\n",
    "                    str(raw_item)\n",
    "                    .lower()\n",
    "                    .replace(\"->->\", \"->\")\n",
    "                    .replace(\"-> \", \"->\")\n",
    "                    .replace(\" ->\", \"->\")\n",
    "                    .replace(\"->none\", \"\")\n",
    "                    .replace(\"->n/a\", \"\")\n",
    "                    .replace(\"\\t\", \"\")\n",
    "                    .replace(\"•\", \"\")\n",
    "                    .replace(\"subpillars\", \"two_levels\")\n",
    "                    .replace(\"subsectors\", \"two_levels\")\n",
    "                    .replace(\"sectors->\", \"\")\n",
    "                    .strip()\n",
    "                )\"\"\"\n",
    "                if not str(item) in [\"nan\", \"none\", \"\", \"n/a\"]:\n",
    "                    if item.isdigit():\n",
    "                        geo_location_output.append(geo_locations_dict.get(int(item)))\n",
    "\n",
    "                    elif date_regex.match(item):\n",
    "                        dates_tmp.append(item)\n",
    "\n",
    "                    else:\n",
    "                        all_items = item.strip().split(\"->\")\n",
    "                        \"\"\"if len(all_items) == 2 and all_items[0] == \"sectors\":\n",
    "                            nlp_one_output[\"nlp_tags\"].append(\n",
    "                                f\"first_level_tags->{item}\"\n",
    "                            )\n",
    "\n",
    "                        else:\"\"\"\n",
    "                        last_item = all_items[-1]\n",
    "\n",
    "                        if item not in mapping_dict:\n",
    "                            if len(all_items) == 1:\n",
    "                                # secondary tags or isolated items\n",
    "                                mapping_row = hum_mapping_sheet[\n",
    "                                    hum_mapping_sheet.apply(\n",
    "                                        lambda x: any(\n",
    "                                            [\n",
    "                                                last_item == x[one_level_original]\n",
    "                                                for one_level_original in original_levels_cols\n",
    "                                            ]\n",
    "                                        ),\n",
    "                                        axis=1,\n",
    "                                    )\n",
    "                                ].copy()\n",
    "                            # subpillars, subsectors\n",
    "                            else:# len(all_items) == 2:\n",
    "                                second_last_item = all_items[-2]\n",
    "                                mapping_row = hum_mapping_sheet[\n",
    "                                    hum_mapping_sheet.apply(\n",
    "                                        lambda x: second_last_item\n",
    "                                        == x[\"Original first level\"]\n",
    "                                        and last_item == x[\"Original second level\"],\n",
    "                                        axis=1,\n",
    "                                    )\n",
    "                                ].copy()\n",
    "                            \"\"\"else:\n",
    "                                \n",
    "                                \n",
    "                                mapping_row = hum_mapping_sheet[\n",
    "                                    hum_mapping_sheet.apply(\n",
    "                                        lambda x: second_last_item\n",
    "                                        == x[\"Original first level\"]\n",
    "                                        and last_item == x[\"Original second level\"],\n",
    "                                        axis=1,\n",
    "                                    )\n",
    "                                ].copy()\"\"\"\n",
    "\n",
    "                            if len(mapping_row) == 1:\n",
    "                                one_mapped_item = mapping_row.iloc[0][\"mapped_nlp\"]\n",
    "                                \n",
    "                                nlp_one_output[\"nlp_tags\"].extend(one_mapped_item)\n",
    "                                mapping_dict[item] = one_mapped_item\n",
    "\n",
    "                            elif len(mapping_row) > 1:\n",
    "                                all_mapped_nlp = (\n",
    "                                    mapping_row[\"mapped_nlp\"].apply(str).tolist()\n",
    "                                )\n",
    "                                if len(set(all_mapped_nlp)) == 1:\n",
    "                                    one_mapped_item = mapping_row.iloc[0][\n",
    "                                        \"mapped_nlp\"\n",
    "                                    ]\n",
    "                                    \n",
    "                                    nlp_one_output[\"nlp_tags\"].extend(\n",
    "                                        one_mapped_item\n",
    "                                    )\n",
    "                                    mapping_dict[item] = one_mapped_item\n",
    "                                else:\n",
    "                                    first_level_mapped_row = hum_mapping_sheet[\n",
    "                                        hum_mapping_sheet.apply(\n",
    "                                            lambda x: x[\"Original first level\"]\n",
    "                                            == last_item\n",
    "                                            and str(x[\"Original second level\"])\n",
    "                                            == \"nan\",\n",
    "                                            axis=1,\n",
    "                                        )\n",
    "                                    ].copy()\n",
    "\n",
    "                                    if len(first_level_mapped_row) == 1:\n",
    "                                        one_mapped_item = (\n",
    "                                            first_level_mapped_row.iloc[0][\n",
    "                                                \"mapped_nlp\"\n",
    "                                            ]\n",
    "                                        )\n",
    "                                        \n",
    "                                        nlp_one_output[\"nlp_tags\"].extend(\n",
    "                                            one_mapped_item\n",
    "                                        )\n",
    "                                        mapping_dict[item] = one_mapped_item\n",
    "                                    else:\n",
    "\n",
    "                                        too_many_rows.add(item)\n",
    "                                        mapping_dict[item] = \"too_many_rows\"\n",
    "                            else:\n",
    "                                no_mapping.add(item)\n",
    "                                mapping_dict[item] = \"no_mapping\"\n",
    "\n",
    "                        elif mapping_dict[item] not in [\n",
    "                            \"no_mapping\",\n",
    "                            \"too_many_rows\",\n",
    "                        ]:\n",
    "                            nlp_one_output[\"nlp_tags\"].extend(mapping_dict[item])\n",
    "\n",
    "    nlp_one_output[\"geo_location\"] = [\n",
    "        one_loc for one_loc in geo_location_output if one_loc is not None\n",
    "    ]\n",
    "\n",
    "    if len(dates_tmp) > 0:\n",
    "        dates_output = dates_tmp[0]\n",
    "    else:\n",
    "        dates_output = \"-\"\n",
    "\n",
    "    nlp_one_output[\"excerpt_date\"] = dates_output\n",
    "\n",
    "    nlp_one_output[\"nlp_tags\"] = list(set(nlp_one_output[\"nlp_tags\"]))\n",
    "\n",
    "    nlp_all_outputs.append(nlp_one_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['excerpt_date'] = [one_excerpt_tags['excerpt_date'] for one_excerpt_tags in nlp_all_outputs]\n",
    "data['geo_location'] = [one_excerpt_tags['geo_location'] for one_excerpt_tags in nlp_all_outputs]\n",
    "data['nlp_tags'] = [one_excerpt_tags['nlp_tags'] for one_excerpt_tags in nlp_all_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#too short excerpts -> noise\n",
    "data = data[data.excerpt.apply(lambda x: len(str(x)))>25]    \n",
    "\n",
    "#too many level 2 tags -> noise\n",
    "data['second_level_tags'] = data['nlp_tags'].apply(lambda x: [item for item in x if any([kw in item for kw in ['subsectors', 'subpillars']])])\n",
    "data = data[data['second_level_tags'].apply(lambda x: len(x)>5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tags = dict(Counter(flatten(data.nlp_tags)))\n",
    "#sort descending value\n",
    "all_tags = dict( sorted(all_tags.items(), key=operator.itemgetter(1),reverse=True))\n",
    "len(all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: drop duplicates check preprocessing modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[General household information]Two most common behaviours adapted to prevent COVID-19 spreading, as reported by households Stopping handshakes or physical contact (48%), Keeping distance from people (42%).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                26\n",
       " Désinfection des ménages des cas confirmés.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                16\n",
       "Une réunion de coordination hebdomadaire sur la COVID-19 se tient entre le bureau OMS-Algérie et le bureau régional OMS (AFRO) tous les lundis.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              15\n",
       "[MUTLI-SECTORAL NEEDS]  100% of both IDP settlement and  non-IDP settlement households found with multi-sectoral needs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      14\n",
       " Poursuite de la réalisation des tests par les laboratoires : INSP, UCRC, LBMA et CICM ;  Poursuite de la réalisation des tests de dépistage pour les voyageurs par le laboratoire de l’INSP.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              12\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ..\n",
       "Ethiopia remains vulnerable to epidemic outbreaks due to poor living standards (especially in rural areas), chronic food insecurity, low vaccination rates, and a large number of IDPs who live in unhygienic environments.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   1\n",
       "The rapid assessment of water and sanitation conditions in five communities (Canaan, San Juan Acul, La Felicidad, El Chorro and E l Cedral) detected that the families do not have access to drinking water. Most obtain their water from a small number of low-flow artisan wells and springs some 30 or 40 minutes away on foot from dwellings and from the La Pasión River. The river rises and submerges at least two of the community springs and some of the wells every winter.                                                                                                                                                                                                                                                        1\n",
       "[données de juillet 2020 dans les 6 territoires ruraux de Beni, Lubero, Masisi, Nyiragongo, Rutshuru et Walikale, et 3 villes (Goma, Beni et Butembo) - 3 383 ménages ont été sondés] La proportion des ménages ayant consommation alimentaire pauvre est très élevée dans les territoires de Walikale (63 %) et Rutshuru (54 %) où plus de la moitié de la population aurait une alimentation insuffisante en quantité et en qualité.                                                                                                                                                                                                                                                                                                        1\n",
       "In the past two weeks, 14,010 patients (68 per cent female), including 4,658 children under five years (47 per cent female) received health services in ten UNICEF-supported health facilities. 1,611 pregnant women received at least one antenatal care consultation and 284 women received a postnatal care consultation. So far Cox’s Bazar District Hospital Special Care Newborn Unit (SCANU) cared for 830 newborns (45 per cent females), including 135 newborns in the last two weeks; while Teknaf Newborn Stabilization Unit (NSU) has treated 101 newborns since November 2017, including 8 in the last two weeks. These facilities are receiving referral cases from both host (almost 90 per cent) and Rohingya communities     1\n",
       "[9th March 2021, Bangladesh] Of the 13 new deceased, all men, 10 were from the Dhaka division, and one each from Barisal, Rangpur, and Chittagong divisions. All of them died while undergoing treatment at different hospitals.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1\n",
       "Name: excerpt, Length: 297898, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.excerpt.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (default, Dec 11 2020, 14:32:07) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a5ddf8e25d962f331e8059973cfd97c5aef9d0ccfdd243943e9f1f512e91043"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
