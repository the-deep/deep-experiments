{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These requirements are necessary if you launch this notebook from SageMaker instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install mlflow\\n!pip install pytorch-lightning\\n!pip install transformers\\n!pip install tqdm\\n!pip install sagemaker\\n\\n!pip install s3fs\\n!pip install smdebug'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!pip install mlflow\n",
    "!pip install pytorch-lightning\n",
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "!pip install sagemaker\n",
    "\n",
    "!pip install s3fs\n",
    "!pip install smdebug\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:49:30.843642Z",
     "start_time": "2021-06-01T14:49:30.663973Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.functional import accuracy, f1, auroc\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.core.decorators import auto_move_data\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from matplotlib import rc\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local constants, regarding the data, MLFlow server, paths, etc..: use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deep.constants import *\n",
    "from deep.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the data you want. We advise the `pandas` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:57:29.882333Z",
     "start_time": "2021-06-01T14:57:28.547379Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(\n",
    "    '..', '..', '..', \"data\", \"frameworks_data\", 'data_v0.6.2','generated_entries'\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"original_df = pd.read_csv(os.path.join(DATA_PATH, 'full_dataset.csv'))\n",
    "augmented_data = pd.read_csv(os.path.join(DATA_PATH, 'generated_text.csv'))\"\"\"\n",
    "\n",
    "\n",
    "tot_df = pd.read_csv(os.path.join(DATA_PATH, 'total_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_excel(os.path.join('..', '..', '..', '..', 'feedback_output.xlsx'))\n",
    "test_data = test_data[['Entry']].rename(columns={'Entry':'excerpt'})\n",
    "test_data = test_data[test_data.excerpt.apply(lambda x: 'NONE' != x.upper())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"augmented_data = pd.merge(\\n    right=original_df.drop(columns=['excerpt']),\\n    left=augmented_data[['entry_id', 'excerpt']],\\n    on='entry_id',\\n    how='right'\\n)\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"augmented_data = pd.merge(\n",
    "    right=original_df.drop(columns=['excerpt']),\n",
    "    left=augmented_data[['entry_id', 'excerpt']],\n",
    "    on='entry_id',\n",
    "    how='right'\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tot_df = pd.concat([original_df, augmented_data])'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"tot_df = pd.concat([original_df, augmented_data])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_df = tot_df[\n",
    "    ['entry_id', 'excerpt', 'lead_id',\n",
    "     'sectors',\n",
    "     #'severity',\n",
    "     #'demographic_groups',\n",
    "     #'subpillars_1d', \n",
    "     #'specific_needs_groups',\n",
    "     'subpillars_2d', \n",
    "     #'affected_groups',\n",
    "     'pillars_2d',\n",
    "     #'pillars_1d'\n",
    "     \n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T15:42:32.024647Z",
     "start_time": "2021-05-27T15:42:31.984694Z"
    }
   },
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:29:20.899415Z",
     "start_time": "2021-06-09T08:29:19.327852Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session(default_bucket=DEV_BUCKET.name)\n",
    "role = SAGEMAKER_ROLE\n",
    "role_arn = SAGEMAKER_ROLE_ARN\n",
    "tracking_uri = MLFLOW_SERVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to upload data to an S3 bucket. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLFLOW_SERVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:43.201910Z",
     "start_time": "2021-06-09T08:29:28.837139Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = True  # To make the computations faster, sample = True.\n",
    "\n",
    "if sample:\n",
    "    tot_df = tot_df.sample(n=50_000)\n",
    "    \n",
    "job_name = f\"pytorch-{formatted_time()}-all-models\"  # change it as you prefer\n",
    "input_path = DEV_BUCKET / 'training' / 'input_data' / job_name  # Do not change this\n",
    "\n",
    "train_path = str(input_path / 'train.pickle')\n",
    "val_path = str(input_path / 'val.pickle')\n",
    "\n",
    "\n",
    "tot_df.to_pickle(train_path, protocol=4)  # protocol 4 is necessary, since SageMaker uses python 3.6\n",
    "test_data.to_pickle(val_path, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:43.284096Z",
     "start_time": "2021-06-09T08:31:43.206457Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPU instances\n",
    "\n",
    "instances = [\n",
    "    'ml.p2.xlarge',\n",
    "    'ml.p3.2xlarge'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters are passed as command line arguments to the training script. \n",
    "\n",
    "You can add/change them as you like. It's important to keep the `tracking_uri` and the `experiment_name` which are used by MLFlow.\n",
    "\n",
    "The class `PyTorch` is part of the `SageMaker` python API. The parameters are important and you should probably not change most of them. The ones you may want to change are:\n",
    "\n",
    "- `instance_type`, specify the instance you want\n",
    "- `source_dir`, specify your script directory. Try to use global variable as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:43.458886Z",
     "start_time": "2021-06-09T08:31:43.304626Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "proportions_negative_examples_test = {\n",
    "    'sectors':0.16,\n",
    "    'subpillars_2d':0.3,\n",
    "    'pillars_2d':0.3,\n",
    "    'subpillars_1d': 0.69,\n",
    "    'pillars_1d':0.69,\n",
    "    'demographic_groups': 0.74,\n",
    "    'specific_needs_groups': 0.86,\n",
    "    'affected_groups': 0.35\n",
    "}\n",
    "factor_prop_tot_train = 0.05\n",
    "proportions_negative_examples_train = {\n",
    "    key:value*factor_prop_tot_train for key, value in proportions_negative_examples_test.items()\n",
    "}\n",
    "instance_type='ml.p3.2xlarge'\n",
    "\n",
    "hyperparameters={\n",
    "    'tracking_uri': MLFLOW_SERVER,\n",
    "    'experiment_name': \"pl-trials\",\n",
    "    'max_len': 256,\n",
    "    'epochs': 1,\n",
    "    'model_name': 'microsoft/xtremedistil-l6-h256-uncased',\n",
    "    'tokenizer_name': 'microsoft/xtremedistil-l6-h256-uncased',\n",
    "    'dropout_rate': 0.3,\n",
    "    'pred_threshold':0.4,\n",
    "    'output_length': 256,\n",
    "    'learning_rate': 5e-5,\n",
    "    #'training_names':'sectors,subpillars_2d,subpillars_1d,specific_needs_groups,affected_groups,demographic_groups',\n",
    "    'training_names':'sectors,pillars_2d,subpillars_2d',\n",
    "    #'training_names':'subpillars_1d,specific_needs_groups,demographic_groups',\n",
    "    #'training_names':'sectors,subpillars_2d,subpillars_1d,severity,specific_needs_groups,affected_groups,demographic_groups',\n",
    "    #'train_with_all_positive_examples':True,\n",
    "    \"model_mode\":\"train\",\n",
    "    \"proportions_negative_examples_test\": str(proportions_negative_examples_test),\n",
    "    \"proportions_negative_examples_train\": str(proportions_negative_examples_train),\n",
    "    \"instance_type\": instance_type,\n",
    "    \n",
    "    #\"numbers_augmentation\":\"with\"\n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point='train_mlflow.py',\n",
    "    source_dir=str('../../../scripts/training/selim/multiclass-lightning'),\n",
    "    output_path=str(DEV_BUCKET/'models/'),\n",
    "    code_location=str(input_path),\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    framework_version=\"1.8\",\n",
    "    py_version=\"py36\",\n",
    "    hyperparameters = hyperparameters,\n",
    "    job_name=job_name,\n",
    "    #distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": False}}}\n",
    "#     train_instance_count=2,\n",
    "#     train_instance_type=\"ml.c4.xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:43.482969Z",
     "start_time": "2021-06-09T08:31:43.459884Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit_arguments = {\n",
    "    'train': str(input_path),\n",
    "    'test': str(input_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:45.995868Z",
     "start_time": "2021-06-09T08:31:43.484212Z"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-25 14:14:55 Starting - Starting the training job...\n",
      "2021-10-25 14:15:35 Starting - Launching requested ML instancesProfilerReport-1635171269: InProgress\n",
      "...\n",
      "2021-10-25 14:16:17 Starting - Preparing the instances for training.........\n",
      "2021-10-25 14:17:58 Downloading - Downloading input data\n",
      "2021-10-25 14:17:58 Training - Downloading the training image...................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-10-25 14:21:40,268 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-10-25 14:21:40,291 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-10-25 14:21:46,520 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-10-25 14:21:46,958 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.8.2\n",
      "  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting tensorflow==2.4.0\n",
      "  Downloading tensorflow-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl (394.7 MB)\u001b[0m\n",
      "\u001b[34mCollecting pytorch-lightning==1.3.8\n",
      "  Downloading pytorch_lightning-1.3.8-py3-none-any.whl (813 kB)\u001b[0m\n",
      "\u001b[34mCollecting torchmetrics==0.4.1\n",
      "  Downloading torchmetrics-0.4.1-py3-none-any.whl (234 kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm==4.41.1\n",
      "  Downloading tqdm-4.41.1-py2.py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34mCollecting nlpaug==1.1.6\n",
      "  Downloading nlpaug-1.1.6-py3-none-any.whl (405 kB)\u001b[0m\n",
      "\u001b[34mCollecting nltk==3.2.5\n",
      "  Downloading nltk-3.2.5.tar.gz (1.2 MB)\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting mlflow==1.18.0\n",
      "  Downloading mlflow-1.18.0-py3-none-any.whl (14.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn==0.22.2.post1\n",
      "  Downloading scikit_learn-0.22.2.post1-cp36-cp36m-manylinux1_x86_64.whl (7.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker==2.49.1\n",
      "  Downloading sagemaker-2.49.1.tar.gz (421 kB)\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting s3fs==2021.07.0\n",
      "  Downloading s3fs-2021.7.0-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting smdebug==1.0.11\n",
      "  Downloading smdebug-1.0.11-py2.py3-none-any.whl (269 kB)\u001b[0m\n",
      "\u001b[34mCollecting scikit-multilearn==0.2.0\n",
      "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (2.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (4.8.1)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub==0.0.12\n",
      "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.10.23-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\u001b[0m\n",
      "\n",
      "2021-10-25 14:22:19 Training - Training image download completed. Training in progress.\u001b[34m  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (3.3.1)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\u001b[0m\n",
      "\u001b[34mCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting numpy>=1.17\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.4.0->-r requirements.txt (line 2)) (3.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.4.0->-r requirements.txt (line 2)) (0.36.2)\u001b[0m\n",
      "\u001b[34mCollecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp36-cp36m-manylinux2014_x86_64.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\u001b[0m\n",
      "\u001b[34mCollecting h5py~=2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.4.0->-r requirements.txt (line 2)) (0.2.0)\u001b[0m\n",
      "\u001b[34mCollecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 3)) (1.8.1)\u001b[0m\n",
      "\u001b[34mCollecting pyDeprecate==0.3.0\n",
      "  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow!=8.3.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 3)) (8.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 3)) (2021.10.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 3)) (0.18.2)\u001b[0m\n",
      "\u001b[34mCollecting gunicorn\n",
      "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\u001b[0m\n",
      "\u001b[34mCollecting sqlalchemy\n",
      "  Downloading SQLAlchemy-1.4.26-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: entrypoints in /opt/conda/lib/python3.6/site-packages (from mlflow==1.18.0->-r requirements.txt (line 8)) (0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from mlflow==1.18.0->-r requirements.txt (line 8)) (2021.3)\u001b[0m\n",
      "\u001b[34mCollecting querystring-parser\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.6/site-packages (from mlflow==1.18.0->-r requirements.txt (line 8)) (8.0.3)\u001b[0m\n",
      "\u001b[34mCollecting sqlparse>=0.3.1\n",
      "  Downloading sqlparse-0.4.2-py3-none-any.whl (42 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from mlflow==1.18.0->-r requirements.txt (line 8)) (1.1.5)\u001b[0m\n",
      "\u001b[34mCollecting databricks-cli>=0.8.7\n",
      "  Downloading databricks-cli-0.16.2.tar.gz (58 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting prometheus-flask-exporter\n",
      "  Downloading prometheus_flask_exporter-0.18.4-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting alembic<=1.4.1\n",
      "  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Flask in /opt/conda/lib/python3.6/site-packages (from mlflow==1.18.0->-r requirements.txt (line 8)) (2.0.2)\u001b[0m\n",
      "\u001b[34mCollecting docker>=4.0.0\n",
      "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitpython>=2.1.0\n",
      "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.6/site-packages (from mlflow==1.18.0->-r requirements.txt (line 8)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.22.2.post1->-r requirements.txt (line 9)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.22.2.post1->-r requirements.txt (line 9)) (1.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs in /opt/conda/lib/python3.6/site-packages (from sagemaker==2.49.1->-r requirements.txt (line 10)) (21.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3>=1.16.32 in /opt/conda/lib/python3.6/site-packages (from sagemaker==2.49.1->-r requirements.txt (line 10)) (1.18.63)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.6/site-packages (from sagemaker==2.49.1->-r requirements.txt (line 10)) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker==2.49.1->-r requirements.txt (line 10)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.6/site-packages (from sagemaker==2.49.1->-r requirements.txt (line 10)) (0.2.8)\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore>=1.0.1\n",
      "  Downloading aiobotocore-1.4.2.tar.gz (52 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting fsspec==2021.07.0\n",
      "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyinstrument>=3.1.3 in /opt/conda/lib/python3.6/site-packages (from smdebug==1.0.11->-r requirements.txt (line 12)) (3.4.2)\u001b[0m\n",
      "\u001b[34mCollecting botocore<1.20.107,>=1.20.106\n",
      "  Downloading botocore-1.20.106-py2.py3-none-any.whl (7.7 MB)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp>=3.3.1\n",
      "  Downloading aiohttp-3.7.4.post0-cp36-cp36m-manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.8.0-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting Mako\n",
      "  Downloading Mako-1.1.5-py2.py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34mCollecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.6/site-packages (from alembic<=1.4.1->mlflow==1.18.0->-r requirements.txt (line 8)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker==2.49.1->-r requirements.txt (line 10)) (0.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker==2.49.1->-r requirements.txt (line 10)) (0.10.0)\u001b[0m\n",
      "\u001b[34mCollecting boto3>=1.16.32\n",
      "  Downloading boto3-1.19.2-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.19.1-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.19.0-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.65-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.64-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.62-py3-none-any.whl (131 kB)\u001b[0m\n",
      "\u001b[34m  Downloading boto3-1.18.61-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.60-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.59-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.58-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.57-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.56-py3-none-any.whl (131 kB)\u001b[0m\n",
      "\u001b[34m  Downloading boto3-1.18.55-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.54-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.53-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.52-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.51-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.50-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.49-py3-none-any.whl (131 kB)\u001b[0m\n",
      "\u001b[34m  Downloading boto3-1.18.48-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.47-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.46-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.45-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.44-py3-none-any.whl (131 kB)\u001b[0m\n",
      "\u001b[34m  Downloading boto3-1.18.43-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.42-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.41-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.40-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.39-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.38-py3-none-any.whl (131 kB)\u001b[0m\n",
      "\u001b[34m  Downloading boto3-1.18.37-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.36-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.35-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.34-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.33-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.32-py3-none-any.whl (131 kB)\u001b[0m\n",
      "\u001b[34m  Downloading boto3-1.18.31-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.30-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.29-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.28-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.27-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.26-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.25-py3-none-any.whl (131 kB)\u001b[0m\n",
      "\u001b[34m  Downloading boto3-1.18.24-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.23-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.22-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.21-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.20-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.19-py3-none-any.whl (131 kB)\u001b[0m\n",
      "\u001b[34m  Downloading boto3-1.18.18-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.17-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.16-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.15-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.14-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.13-py3-none-any.whl (131 kB)\u001b[0m\n",
      "\u001b[34m  Downloading boto3-1.18.12-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.11-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.10-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.9-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.8-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.7-py3-none-any.whl (131 kB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m  Downloading boto3-1.18.6-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.5-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.4-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.3-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.2-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.18.1-py3-none-any.whl (131 kB)\u001b[0m\n",
      "\u001b[34m  Downloading boto3-1.18.0-py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.17.112-py2.py3-none-any.whl (131 kB)\u001b[0m\n",
      "\u001b[34mCollecting s3transfer<0.5.0,>=0.4.0\n",
      "  Downloading s3transfer-0.4.2-py2.py3-none-any.whl (79 kB)\u001b[0m\n",
      "\u001b[34mCollecting boto3>=1.16.32\n",
      "  Downloading boto3-1.17.111-py2.py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.17.110-py2.py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.17.109-py2.py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.17.108-py2.py3-none-any.whl (131 kB)\u001b[0m\n",
      "\u001b[34m  Downloading boto3-1.17.107-py2.py3-none-any.whl (131 kB)\n",
      "  Downloading boto3-1.17.106-py2.py3-none-any.whl (131 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.6/site-packages (from databricks-cli>=0.8.7->mlflow==1.18.0->-r requirements.txt (line 8)) (0.8.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.6/site-packages (from docker>=4.0.0->mlflow==1.18.0->-r requirements.txt (line 8)) (1.2.1)\u001b[0m\n",
      "\u001b[34mCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Downloading fsspec-2021.10.0-py3-none-any.whl (125 kB)\n",
      "  Downloading fsspec-2021.9.0-py3-none-any.whl (123 kB)\n",
      "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.8.2->-r requirements.txt (line 1)) (3.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.8.2->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyinstrument-cext>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from pyinstrument>=3.1.3->smdebug==1.0.11->-r requirements.txt (line 12)) (0.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.8.2->-r requirements.txt (line 1)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.8.2->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.8.2->-r requirements.txt (line 1)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.8.2->-r requirements.txt (line 1)) (2021.5.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.6/site-packages (from sqlalchemy->mlflow==1.18.0->-r requirements.txt (line 8)) (1.1.2)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow==2.4.0->-r requirements.txt (line 2)) (49.6.0.post20210108)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow==2.4.0->-r requirements.txt (line 2)) (2.0.2)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.3.0-py2.py3-none-any.whl (154 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.6/site-packages (from Flask->mlflow==1.18.0->-r requirements.txt (line 8)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.6/site-packages (from Flask->mlflow==1.18.0->-r requirements.txt (line 8)) (3.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker==2.49.1->-r requirements.txt (line 10)) (0.70.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker==2.49.1->-r requirements.txt (line 10)) (0.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker==2.49.1->-r requirements.txt (line 10)) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker==2.49.1->-r requirements.txt (line 10)) (1.6.6.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.6/site-packages (from prometheus-flask-exporter->mlflow==1.18.0->-r requirements.txt (line 8)) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5.0,>=2.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 11)) (4.0.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (159 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<4.0,>=3.0\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting idna-ssl>=1.0\n",
      "  Downloading idna-ssl-1.1.0.tar.gz (3.4 kB)\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (270 kB)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->-r requirements.txt (line 2)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.6/site-packages (from Jinja2>=3.0->Flask->mlflow==1.18.0->-r requirements.txt (line 8)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->-r requirements.txt (line 2)) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: nltk, sagemaker, aiobotocore, alembic, databricks-cli, termcolor, wrapt, idna-ssl\n",
      "  Building wheel for nltk (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.2.5-py3-none-any.whl size=1392140 sha256=eb9ae00a2d19920d094c5fac8a14f3c40a5d8a9f6e0bb22359c1fde65c3dcd95\n",
      "  Stored in directory: /root/.cache/pip/wheels/f2/7f/71/cb36468789a03b5e2908281c8e1ce093e6860258b6b61677d8\n",
      "  Building wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-2.49.1-py2.py3-none-any.whl size=591916 sha256=cbf1fd936c31396acb7f2deba36c54d9df9f478ff2e6ad79a34bb93146741545\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/af/ea/8ff5943a87155df5b184e54474fbf2b59b75e5c172854643c6\n",
      "  Building wheel for aiobotocore (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for aiobotocore (setup.py): finished with status 'done'\n",
      "  Created wheel for aiobotocore: filename=aiobotocore-1.4.2-py3-none-any.whl size=49910 sha256=e23e504ab14bbbcbd0e98a346e9c0f7bb9839b3eda408b9777773c32f9bd312a\n",
      "  Stored in directory: /root/.cache/pip/wheels/28/61/17/b5ccee30498ab9e21ed6ed0f65d71f79cd252017bc95b6fc00\n",
      "  Building wheel for alembic (setup.py): started\n",
      "  Building wheel for alembic (setup.py): finished with status 'done'\n",
      "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158155 sha256=3d833a71eb6835d0385a1b33db867dd49a4305f8806b33797349b9cf2a6273c6\n",
      "  Stored in directory: /root/.cache/pip/wheels/e9/7b/aa/e18c983d8236b141f85838ba0f8e4e4ae9bcf7f1e00ff726ec\n",
      "  Building wheel for databricks-cli (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
      "  Created wheel for databricks-cli: filename=databricks_cli-0.16.2-py3-none-any.whl size=106811 sha256=7be9b1925ea7c4ca1550ece88983bb55ade3f5200fb20870be5f8491acbc8f3a\n",
      "  Stored in directory: /root/.cache/pip/wheels/f5/cf/28/d354903b6e02a075cec67cfb2414321d2beed2cc428fe26478\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=500c69b427b24fc9a4f28da4c6d12eeee70a5da98377c5a57aac8d276a3f784b\n",
      "  Stored in directory: /root/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for wrapt (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=69748 sha256=cb8ddab3659bf943073ac9ff9fc9ae18e0c4bd4c9df2acada58bb7a7ce35afb3\n",
      "  Stored in directory: /root/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "  Building wheel for idna-ssl (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for idna-ssl (setup.py): finished with status 'done'\n",
      "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-py3-none-any.whl size=3160 sha256=e2bd7d21b093bf24fe79b01677851a38c31c4d5fb54efec67bc93074b890bd90\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/f5/9c/f8331a854f7a8739cf0e74c13854e4dd7b1af11b04fe1dde13\u001b[0m\n",
      "\u001b[34mSuccessfully built nltk sagemaker aiobotocore alembic databricks-cli termcolor wrapt idna-ssl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: typing-extensions, six, pyasn1-modules, oauthlib, multidict, cachetools, yarl, smmap, requests-oauthlib, numpy, idna-ssl, google-auth, botocore, async-timeout, wrapt, tqdm, tensorboard-plugin-wit, tensorboard-data-server, sqlalchemy, s3transfer, regex, python-editor, markdown, Mako, grpcio, google-auth-oauthlib, gitdb, fsspec, aioitertools, aiohttp, absl-py, torchmetrics, tokenizers, termcolor, tensorflow-estimator, tensorboard, sqlparse, sacremoses, querystring-parser, pyDeprecate, prometheus-flask-exporter, opt-einsum, keras-preprocessing, huggingface-hub, h5py, gunicorn, gitpython, gast, flatbuffers, docker, databricks-cli, boto3, astunparse, alembic, aiobotocore, transformers, tensorflow, smdebug, scikit-multilearn, scikit-learn, sagemaker, s3fs, pytorch-lightning, nltk, nlpaug, mlflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.1\n",
      "    Uninstalling numpy-1.19.1:\n",
      "      Successfully uninstalled numpy-1.19.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.21.63\n",
      "    Uninstalling botocore-1.21.63:\n",
      "      Successfully uninstalled botocore-1.21.63\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.61.2\n",
      "    Uninstalling tqdm-4.61.2:\n",
      "      Successfully uninstalled tqdm-4.61.2\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.5.0\n",
      "    Uninstalling s3transfer-0.5.0:\n",
      "      Successfully uninstalled s3transfer-0.5.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2021.10.1\n",
      "    Uninstalling fsspec-2021.10.1:\n",
      "      Successfully uninstalled fsspec-2021.10.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.8.0\n",
      "    Uninstalling h5py-2.8.0:\n",
      "      Successfully uninstalled h5py-2.8.0\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.18.63\n",
      "    Uninstalling boto3-1.18.63:\n",
      "      Successfully uninstalled boto3-1.18.63\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: smdebug\n",
      "    Found existing installation: smdebug 1.0.9\n",
      "    Uninstalling smdebug-1.0.9:\n",
      "      Successfully uninstalled smdebug-1.0.9\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.63.1\n",
      "    Uninstalling sagemaker-2.63.1:\n",
      "      Successfully uninstalled sagemaker-2.63.1\n",
      "  Attempting uninstall: s3fs\n",
      "    Found existing installation: s3fs 0.4.2\n",
      "    Uninstalling s3fs-0.4.2:\n",
      "      Successfully uninstalled s3fs-0.4.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed Mako-1.1.5 absl-py-0.15.0 aiobotocore-1.4.2 aiohttp-3.7.4.post0 aioitertools-0.8.0 alembic-1.4.1 astunparse-1.6.3 async-timeout-3.0.1 boto3-1.17.106 botocore-1.20.106 cachetools-4.2.4 databricks-cli-0.16.2 docker-5.0.3 flatbuffers-1.12 fsspec-2021.7.0 gast-0.3.3 gitdb-4.0.9 gitpython-3.1.18 google-auth-2.3.0 google-auth-oauthlib-0.4.6 grpcio-1.32.0 gunicorn-20.1.0 h5py-2.10.0 huggingface-hub-0.0.12 idna-ssl-1.1.0 keras-preprocessing-1.1.2 markdown-3.3.4 mlflow-1.18.0 multidict-5.2.0 nlpaug-1.1.6 nltk-3.2.5 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 prometheus-flask-exporter-0.18.4 pyDeprecate-0.3.0 pyasn1-modules-0.2.8 python-editor-1.0.4 pytorch-lightning-1.3.8 querystring-parser-1.2.4 regex-2021.10.23 requests-oauthlib-1.3.0 s3fs-2021.7.0 s3transfer-0.4.2 sacremoses-0.0.46 sagemaker-2.49.1 scikit-learn-0.22.2.post1 scikit-multilearn-0.2.0 six-1.15.0 smdebug-1.0.11 smmap-5.0.0 sqlalchemy-1.4.26 sqlparse-0.4.2 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.4.0 tensorflow-estimator-2.4.0 termcolor-1.1.0 tokenizers-0.10.3 torchmetrics-0.4.1 tqdm-4.41.1 transformers-4.8.2 typing-extensions-3.7.4.3 wrapt-1.12.1 yarl-1.7.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mawscli 1.20.63 requires botocore==1.21.63, but you have botocore 1.20.106 which is incompatible.\u001b[0m\n",
      "\u001b[34mawscli 1.20.63 requires s3transfer<0.6.0,>=0.5.0, but you have s3transfer 0.4.2 which is incompatible.\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "\u001b[34m2021-10-25 14:23:07,843 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dropout_rate\": 0.3,\n",
      "        \"experiment_name\": \"pl-trials\",\n",
      "        \"proportions_negative_examples_test\": \"{'sectors': 0.16, 'subpillars_2d': 0.3, 'pillars_2d': 0.3, 'subpillars_1d': 0.69, 'pillars_1d': 0.69, 'demographic_groups': 0.74, 'specific_needs_groups': 0.86, 'affected_groups': 0.35}\",\n",
      "        \"model_mode\": \"train\",\n",
      "        \"max_len\": 256,\n",
      "        \"training_names\": \"sectors,pillars_2d,subpillars_2d\",\n",
      "        \"model_name\": \"microsoft/xtremedistil-l6-h256-uncased\",\n",
      "        \"output_length\": 256,\n",
      "        \"proportions_negative_examples_train\": \"{'sectors': 0.008, 'subpillars_2d': 0.015, 'pillars_2d': 0.015, 'subpillars_1d': 0.034499999999999996, 'pillars_1d': 0.034499999999999996, 'demographic_groups': 0.037, 'specific_needs_groups': 0.043000000000000003, 'affected_groups': 0.017499999999999998}\",\n",
      "        \"tokenizer_name\": \"microsoft/xtremedistil-l6-h256-uncased\",\n",
      "        \"epochs\": 1,\n",
      "        \"instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"learning_rate\": 5e-05,\n",
      "        \"pred_threshold\": 0.4,\n",
      "        \"tracking_uri\": \"http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-2021-10-25-16-14-01-867-all-models\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-deep-experiments-dev/training/input_data/pytorch-2021-10-25-16-14-01-867-all-models/pytorch-2021-10-25-16-14-01-867-all-models/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_mlflow\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_mlflow.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dropout_rate\":0.3,\"epochs\":1,\"experiment_name\":\"pl-trials\",\"instance_type\":\"ml.p3.2xlarge\",\"learning_rate\":5e-05,\"max_len\":256,\"model_mode\":\"train\",\"model_name\":\"microsoft/xtremedistil-l6-h256-uncased\",\"output_length\":256,\"pred_threshold\":0.4,\"proportions_negative_examples_test\":\"{'sectors': 0.16, 'subpillars_2d': 0.3, 'pillars_2d': 0.3, 'subpillars_1d': 0.69, 'pillars_1d': 0.69, 'demographic_groups': 0.74, 'specific_needs_groups': 0.86, 'affected_groups': 0.35}\",\"proportions_negative_examples_train\":\"{'sectors': 0.008, 'subpillars_2d': 0.015, 'pillars_2d': 0.015, 'subpillars_1d': 0.034499999999999996, 'pillars_1d': 0.034499999999999996, 'demographic_groups': 0.037, 'specific_needs_groups': 0.043000000000000003, 'affected_groups': 0.017499999999999998}\",\"tokenizer_name\":\"microsoft/xtremedistil-l6-h256-uncased\",\"tracking_uri\":\"http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\",\"training_names\":\"sectors,pillars_2d,subpillars_2d\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_mlflow.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_mlflow\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-deep-experiments-dev/training/input_data/pytorch-2021-10-25-16-14-01-867-all-models/pytorch-2021-10-25-16-14-01-867-all-models/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dropout_rate\":0.3,\"epochs\":1,\"experiment_name\":\"pl-trials\",\"instance_type\":\"ml.p3.2xlarge\",\"learning_rate\":5e-05,\"max_len\":256,\"model_mode\":\"train\",\"model_name\":\"microsoft/xtremedistil-l6-h256-uncased\",\"output_length\":256,\"pred_threshold\":0.4,\"proportions_negative_examples_test\":\"{'sectors': 0.16, 'subpillars_2d': 0.3, 'pillars_2d': 0.3, 'subpillars_1d': 0.69, 'pillars_1d': 0.69, 'demographic_groups': 0.74, 'specific_needs_groups': 0.86, 'affected_groups': 0.35}\",\"proportions_negative_examples_train\":\"{'sectors': 0.008, 'subpillars_2d': 0.015, 'pillars_2d': 0.015, 'subpillars_1d': 0.034499999999999996, 'pillars_1d': 0.034499999999999996, 'demographic_groups': 0.037, 'specific_needs_groups': 0.043000000000000003, 'affected_groups': 0.017499999999999998}\",\"tokenizer_name\":\"microsoft/xtremedistil-l6-h256-uncased\",\"tracking_uri\":\"http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\",\"training_names\":\"sectors,pillars_2d,subpillars_2d\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-2021-10-25-16-14-01-867-all-models\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-deep-experiments-dev/training/input_data/pytorch-2021-10-25-16-14-01-867-all-models/pytorch-2021-10-25-16-14-01-867-all-models/source/sourcedir.tar.gz\",\"module_name\":\"train_mlflow\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_mlflow.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dropout_rate\",\"0.3\",\"--epochs\",\"1\",\"--experiment_name\",\"pl-trials\",\"--instance_type\",\"ml.p3.2xlarge\",\"--learning_rate\",\"5e-05\",\"--max_len\",\"256\",\"--model_mode\",\"train\",\"--model_name\",\"microsoft/xtremedistil-l6-h256-uncased\",\"--output_length\",\"256\",\"--pred_threshold\",\"0.4\",\"--proportions_negative_examples_test\",\"{'sectors': 0.16, 'subpillars_2d': 0.3, 'pillars_2d': 0.3, 'subpillars_1d': 0.69, 'pillars_1d': 0.69, 'demographic_groups': 0.74, 'specific_needs_groups': 0.86, 'affected_groups': 0.35}\",\"--proportions_negative_examples_train\",\"{'sectors': 0.008, 'subpillars_2d': 0.015, 'pillars_2d': 0.015, 'subpillars_1d': 0.034499999999999996, 'pillars_1d': 0.034499999999999996, 'demographic_groups': 0.037, 'specific_needs_groups': 0.043000000000000003, 'affected_groups': 0.017499999999999998}\",\"--tokenizer_name\",\"microsoft/xtremedistil-l6-h256-uncased\",\"--tracking_uri\",\"http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\",\"--training_names\",\"sectors,pillars_2d,subpillars_2d\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_DROPOUT_RATE=0.3\u001b[0m\n",
      "\u001b[34mSM_HP_EXPERIMENT_NAME=pl-trials\u001b[0m\n",
      "\u001b[34mSM_HP_PROPORTIONS_NEGATIVE_EXAMPLES_TEST={'sectors': 0.16, 'subpillars_2d': 0.3, 'pillars_2d': 0.3, 'subpillars_1d': 0.69, 'pillars_1d': 0.69, 'demographic_groups': 0.74, 'specific_needs_groups': 0.86, 'affected_groups': 0.35}\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_MODE=train\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_LEN=256\u001b[0m\n",
      "\u001b[34mSM_HP_TRAINING_NAMES=sectors,pillars_2d,subpillars_2d\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=microsoft/xtremedistil-l6-h256-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_LENGTH=256\u001b[0m\n",
      "\u001b[34mSM_HP_PROPORTIONS_NEGATIVE_EXAMPLES_TRAIN={'sectors': 0.008, 'subpillars_2d': 0.015, 'pillars_2d': 0.015, 'subpillars_1d': 0.034499999999999996, 'pillars_1d': 0.034499999999999996, 'demographic_groups': 0.037, 'specific_needs_groups': 0.043000000000000003, 'affected_groups': 0.017499999999999998}\u001b[0m\n",
      "\u001b[34mSM_HP_TOKENIZER_NAME=microsoft/xtremedistil-l6-h256-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_PRED_THRESHOLD=0.4\u001b[0m\n",
      "\u001b[34mSM_HP_TRACKING_URI=http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train_mlflow.py --dropout_rate 0.3 --epochs 1 --experiment_name pl-trials --instance_type ml.p3.2xlarge --learning_rate 5e-05 --max_len 256 --model_mode train --model_name microsoft/xtremedistil-l6-h256-uncased --output_length 256 --pred_threshold 0.4 --proportions_negative_examples_test {'sectors': 0.16, 'subpillars_2d': 0.3, 'pillars_2d': 0.3, 'subpillars_1d': 0.69, 'pillars_1d': 0.69, 'demographic_groups': 0.74, 'specific_needs_groups': 0.86, 'affected_groups': 0.35} --proportions_negative_examples_train {'sectors': 0.008, 'subpillars_2d': 0.015, 'pillars_2d': 0.015, 'subpillars_1d': 0.034499999999999996, 'pillars_1d': 0.034499999999999996, 'demographic_groups': 0.037, 'specific_needs_groups': 0.043000000000000003, 'affected_groups': 0.017499999999999998} --tokenizer_name microsoft/xtremedistil-l6-h256-uncased --tracking_uri http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/ --training_names sectors,pillars_2d,subpillars_2d\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2021-10-25 14:23:30.041 algo-1:85 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.231 algo-1:85 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.232 algo-1:85 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.232 algo-1:85 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.233 algo-1:85 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.233 algo-1:85 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.326 algo-1:85 INFO hook.py:594] name:model.l0.embeddings.word_embeddings.weight count_params:7813632\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.326 algo-1:85 INFO hook.py:594] name:model.l0.embeddings.position_embeddings.weight count_params:131072\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.327 algo-1:85 INFO hook.py:594] name:model.l0.embeddings.token_type_embeddings.weight count_params:512\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.327 algo-1:85 INFO hook.py:594] name:model.l0.embeddings.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.327 algo-1:85 INFO hook.py:594] name:model.l0.embeddings.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.327 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.0.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.327 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.0.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.327 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.0.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.327 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.0.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.327 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.0.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.327 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.0.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.327 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.0.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.327 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.0.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.327 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.0.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.328 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.0.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.328 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.0.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.328 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.0.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.328 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.0.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.328 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.0.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.328 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.0.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.328 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.0.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.328 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.1.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.328 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.1.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.328 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.1.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.328 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.1.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.328 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.1.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.328 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.1.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.328 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.1.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.329 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.1.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.329 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.1.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.329 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.1.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.329 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.1.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.329 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.1.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.329 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.1.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.329 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.1.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.329 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.1.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.329 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.1.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.329 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.2.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.329 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.2.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.329 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.2.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.329 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.2.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.329 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.2.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.330 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.2.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.330 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.2.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.330 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.2.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.330 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.2.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.330 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.2.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.330 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.2.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.330 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.2.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.330 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.2.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.330 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.2.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.330 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.2.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.330 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.2.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.330 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.3.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.330 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.3.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.330 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.3.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.331 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.3.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.331 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.3.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.331 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.3.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.331 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.3.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.331 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.3.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.331 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.3.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.331 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.3.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.331 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.3.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.331 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.3.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.331 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.3.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.331 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.3.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.331 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.3.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.332 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.3.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.332 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.4.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.332 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.4.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.332 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.4.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.332 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.4.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.332 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.4.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.332 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.4.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.332 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.4.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.332 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.4.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.332 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.4.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.332 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.4.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.332 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.4.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.333 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.4.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.333 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.4.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.333 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.4.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.333 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.4.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.333 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.4.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.333 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.5.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.333 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.5.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.333 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.5.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.333 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.5.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.333 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.5.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.333 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.5.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.333 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.5.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.333 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.5.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.333 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.5.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.334 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.5.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.334 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.5.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.334 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.5.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.334 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.5.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.334 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.5.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.334 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.5.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.334 algo-1:85 INFO hook.py:594] name:model.l0.encoder.layer.5.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.334 algo-1:85 INFO hook.py:594] name:model.l0.pooler.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.334 algo-1:85 INFO hook.py:594] name:model.l0.pooler.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.334 algo-1:85 INFO hook.py:594] name:model.l2.weight count_params:38400\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.334 algo-1:85 INFO hook.py:594] name:model.l2.bias count_params:150\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.334 algo-1:85 INFO hook.py:594] name:model.l3.weight count_params:150\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.334 algo-1:85 INFO hook.py:594] name:model.l3.bias count_params:150\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.334 algo-1:85 INFO hook.py:594] name:model.l5.weight count_params:1650\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.335 algo-1:85 INFO hook.py:594] name:model.l5.bias count_params:11\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.335 algo-1:85 INFO hook.py:596] Total Trainable Params: 12790591\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.335 algo-1:85 INFO hook.py:423] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-10-25 14:23:30.338 algo-1:85 INFO hook.py:486] Hook is writing from the hook with pid: 85\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#015Validation sanity check: 0it [00:00, ?it/s]#015Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]#015                                                              #015#015Training: 99it [00:00, ?it/s]#015Training:   0%|          | 0/800 [00:00<?, ?it/s]#015Epoch 0:   0%|          | 0/800 [00:00<?, ?it/s] #015Epoch 0:   4%|         | 30/800 [00:01<00:48, 15.75it/s]#015Epoch 0:   4%|         | 30/800 [00:01<00:48, 15.74it/s, loss=2.26, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.499]#015Epoch 0:   8%|         | 60/800 [00:03<00:46, 16.02it/s, loss=2.26, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.499]#015Epoch 0:   8%|         | 60/800 [00:03<00:46, 16.02it/s, loss=1.83, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.524]#015Epoch 0:  11%|        | 90/800 [00:05<00:43, 16.51it/s, loss=1.83, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.524]#015Epoch 0:  11%|        | 90/800 [00:05<00:43, 16.51it/s, loss=1.86, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.476]#015Epoch 0:  15%|        | 120/800 [00:07<00:41, 16.46it/s, loss=1.86, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.476]#015Epoch 0:  15%|        | 120/800 [00:07<00:41, 16.46it/s, loss=2.13, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.473]#015Epoch 0:  19%|        | 150/800 [00:09<00:39, 16.41it/s, loss=2.13, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.473]#015Epoch 0:  19%|        | 150/800 [00:09<00:39, 16.41it/s, loss=2, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.476]   #015Epoch 0:  22%|       | 180/800 [00:10<00:37, 16.49it/s, loss=2, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.476]#015Epoch 0:  22%|       | 180/800 [00:10<00:37, 16.49it/s, loss=2.14, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.476]#015Epoch 0:  26%|       | 210/800 [00:12<00:35, 16.46it/s, loss=2.14, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.476]#015Epoch 0:  26%|       | 210/800 [00:12<00:35, 16.46it/s, loss=1.93, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.506]#015Epoch 0:  30%|       | 240/800 [00:14<00:33, 16.51it/s, loss=1.93, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.506]#015Epoch 0:  30%|       | 240/800 [00:14<00:33, 16.51it/s, loss=1.77, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.508]#015Epoch 0:  34%|      | 270/800 [00:16<00:32, 16.48it/s, loss=1.77, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.508]#015Epoch 0:  34%|      | 270/800 [00:16<00:32, 16.48it/s, loss=2.05, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.474]#015Epoch 0:  38%|      | 300/800 [00:18<00:30, 16.44it/s, loss=2.05, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.474]#015Epoch 0:  38%|      | 300/800 [00:18<00:30, 16.44it/s, loss=2.06, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.493]#015Epoch 0:  41%|     | 330/800 [00:20<00:28, 16.46it/s, loss=2.06, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.493]#015Epoch 0:  41%|     | 330/800 [00:20<00:28, 16.46it/s, loss=1.99, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.476]#015Epoch 0:  45%|     | 360/800 [00:21<00:26, 16.47it/s, loss=1.99, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.476]#015Epoch 0:  45%|     | 360/800 [00:21<00:26, 16.47it/s, loss=1.96, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.475]#015Epoch 0:  49%|     | 390/800 [00:23<00:24, 16.53it/s, loss=1.96, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.475]#015Epoch 0:  49%|     | 390/800 [00:23<00:24, 16.53it/s, loss=1.76, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.474]#015Epoch 0:  52%|    | 420/800 [00:25<00:22, 16.54it/s, loss=1.76, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.474]#015Epoch 0:  52%|    | 420/800 [00:25<00:22, 16.54it/s, loss=1.83, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.524]#015Epoch 0:  56%|    | 450/800 [00:27<00:21, 16.55it/s, loss=1.83, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.524]#015Epoch 0:  56%|    | 450/800 [00:27<00:21, 16.55it/s, loss=1.86, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.477]#015Epoch 0:  60%|    | 480/800 [00:28<00:19, 16.61it/s, loss=1.86, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.477]#015Epoch 0:  60%|    | 480/800 [00:28<00:19, 16.61it/s, loss=1.94, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.476]#015Epoch 0:  64%|   | 510/800 [00:30<00:17, 16.61it/s, loss=1.94, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.476]#015Epoch 0:  64%|   | 510/800 [00:30<00:17, 16.61it/s, loss=2.02, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.475]#015Epoch 0:  68%|   | 540/800 [00:32<00:15, 16.65it/s, loss=2.02, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.475]#015Epoch 0:  68%|   | 540/800 [00:32<00:15, 16.65it/s, loss=1.78, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.475]#015Epoch 0:  71%|  | 570/800 [00:34<00:13, 16.63it/s, loss=1.78, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.475]#015Epoch 0:  71%|  | 570/800 [00:34<00:13, 16.63it/s, loss=1.94, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.475]#015Epoch 0:  75%|  | 600/800 [00:36<00:12, 16.61it/s, loss=1.94, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.475]#015Epoch 0:  75%|  | 600/800 [00:36<00:12, 16.61it/s, loss=1.63, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.478]#015Epoch 0:  79%|  | 630/800 [00:37<00:10, 16.62it/s, loss=1.63, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.478]#015Epoch 0:  79%|  | 630/800 [00:37<00:10, 16.62it/s, loss=1.79, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.482]#015Epoch 0:  82%| | 660/800 [00:39<00:08, 16.59it/s, loss=1.79, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.482]#015Epoch 0:  82%| | 660/800 [00:39<00:08, 16.59it/s, loss=1.77, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.491]#015Epoch 0:  86%| | 690/800 [00:41<00:06, 16.60it/s, loss=1.77, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.491]#015Epoch 0:  86%| | 690/800 [00:41<00:06, 16.60it/s, loss=1.66, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.524]#015Epoch 0:  90%| | 720/800 [00:43<00:04, 16.58it/s, loss=1.66, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.524]#015Epoch 0:  90%| | 720/800 [00:43<00:04, 16.58it/s, loss=1.91, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.534]#015Epoch 0:  94%|| 750/800 [00:44<00:02, 16.70it/s, loss=1.91, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.534]\u001b[0m\n",
      "\u001b[34m#015Validating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:   0%|          | 0/56 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  54%|    | 30/56 [00:01<00:01, 25.20it/s]#033[A#015Epoch 0:  98%|| 780/800 [00:46<00:01, 16.92it/s, loss=1.91, v_num=0, val_f1_epoch=0.0722, val_loss_epoch=0.679, train_f1=0.534]\u001b[0m\n",
      "\u001b[34m#015Validating: 100%|| 56/56 [00:02<00:00, 25.84it/s]#033[A#015Epoch 0: 100%|| 800/800 [00:47<00:00, 16.99it/s, loss=1.67, v_num=0, val_f1_epoch=0.488, val_loss_epoch=0.229, train_f1=0.476, val_f1_step=0.479, val_loss_step=0.228]\u001b[0m\n",
      "\u001b[34m#015                                                           #033[A#015Epoch 0: 100%|| 800/800 [00:57<00:00, 13.86it/s, loss=1.67, v_num=0, val_f1_epoch=0.488, val_loss_epoch=0.229, train_f1=0.476, val_f1_step=0.479, val_loss_step=0.228]#015Validation sanity check: 0it [00:00, ?it/s]#015Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]#015                                                              #015#015Training: 99it [00:00, ?it/s]#015Training:   0%|          | 0/843 [00:00<?, ?it/s]#015Epoch 0:   0%|          | 0/843 [00:00<?, ?it/s] #015Epoch 0:   4%|         | 30/843 [00:01<00:48, 16.66it/s]#015Epoch 0:   4%|         | 30/843 [00:01<00:48, 16.66it/s, loss=1.58, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.583]#015Epoch 0:   7%|         | 60/843 [00:03<00:46, 16.85it/s, loss=1.58, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.583]#015Epoch 0:   7%|         | 60/843 [00:03<00:46, 16.84it/s, loss=1.52, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.550]#015Epoch 0:  11%|         | 90/843 [00:05<00:43, 17.12it/s, loss=1.52, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.550]#015Epoch 0:  11%|         | 90/843 [00:05<00:43, 17.12it/s, loss=1.36, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.659]#015Epoch 0:  14%|        | 120/843 [00:07<00:42, 17.10it/s, loss=1.36, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.659]#015Epoch 0:  14%|        | 120/843 [00:07<00:42, 17.10it/s, loss=1.53, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.641]#015Epoch 0:  18%|        | 150/843 [00:08<00:40, 17.08it/s, loss=1.53, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.641]#015Epoch 0:  18%|        | 150/843 [00:08<00:40, 17.08it/s, loss=1.49, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.604]#015Epoch 0:  21%|       | 180/843 [00:10<00:38, 17.25it/s, loss=1.49, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.604]#015Epoch 0:  21%|       | 180/843 [00:10<00:38, 17.25it/s, loss=1.33, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.586]#015Epoch 0:  25%|       | 210/843 [00:12<00:36, 17.22it/s, loss=1.33, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.586]#015Epoch 0:  25%|       | 210/843 [00:12<00:36, 17.22it/s, loss=1.39, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.590]#015Epoch 0:  28%|       | 240/843 [00:13<00:34, 17.35it/s, loss=1.39, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.590]#015Epoch 0:  28%|       | 240/843 [00:13<00:34, 17.35it/s, loss=1.26, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.721]#015Epoch 0:  32%|      | 270/843 [00:15<00:32, 17.37it/s, loss=1.26, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.721]#015Epoch 0:  32%|      | 270/843 [00:15<00:32, 17.37it/s, loss=1.23, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.733]#015Epoch 0:  36%|      | 300/843 [00:17<00:31, 17.38it/s, loss=1.23, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.733]#015Epoch 0:  36%|      | 300/843 [00:17<00:31, 17.38it/s, loss=1.27, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.590]#015Epoch 0:  39%|      | 330/843 [00:18<00:29, 17.41it/s, loss=1.27, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.590]#015Epoch 0:  39%|      | 330/843 [00:18<00:29, 17.41it/s, loss=1.23, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.780]#015Epoch 0:  43%|     | 360/843 [00:20<00:27, 17.37it/s, loss=1.23, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.780]#015Epoch 0:  43%|     | 360/843 [00:20<00:27, 17.37it/s, loss=1.1, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.705] #015Epoch 0:  46%|     | 390/843 [00:22<00:25, 17.45it/s, loss=1.1, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.705]#015Epoch 0:  46%|     | 390/843 [00:22<00:25, 17.45it/s, loss=1.23, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.648]#015Epoch 0:  50%|     | 420/843 [00:24<00:24, 17.45it/s, loss=1.23, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.648]#015Epoch 0:  50%|     | 420/843 [00:24<00:24, 17.45it/s, loss=1.21, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.656]#015Epoch 0:  53%|    | 450/843 [00:25<00:22, 17.43it/s, loss=1.21, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.656]#015Epoch 0:  53%|    | 450/843 [00:25<00:22, 17.43it/s, loss=1.18, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.653]#015Epoch 0:  57%|    | 480/843 [00:27<00:20, 17.49it/s, loss=1.18, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.653]#015Epoch 0:  57%|    | 480/843 [00:27<00:20, 17.49it/s, loss=1.12, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.707]#015Epoch 0:  60%|    | 510/843 [00:29<00:19, 17.48it/s, loss=1.12, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.707]#015Epoch 0:  60%|    | 510/843 [00:29<00:19, 17.48it/s, loss=1.14, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.668]#015Epoch 0:  64%|   | 540/843 [00:30<00:17, 17.53it/s, loss=1.14, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.668]#015Epoch 0:  64%|   | 540/843 [00:30<00:17, 17.53it/s, loss=1.25, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.673]#015Epoch 0:  68%|   | 570/843 [00:32<00:15, 17.51it/s, loss=1.25, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.673]#015Epoch 0:  68%|   | 570/843 [00:32<00:15, 17.51it/s, loss=1.15, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.688]#015Epoch 0:  71%|   | 600/843 [00:34<00:13, 17.49it/s, loss=1.15, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.688]#015Epoch 0:  71%|   | 600/843 [00:34<00:13, 17.49it/s, loss=1.11, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.691]#015Epoch 0:  75%|  | 630/843 [00:35<00:12, 17.52it/s, loss=1.11, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.691]#015Epoch 0:  75%|  | 630/843 [00:35<00:12, 17.52it/s, loss=1.16, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.687]#015Epoch 0:  78%|  | 660/843 [00:37<00:10, 17.52it/s, loss=1.16, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.687]#015Epoch 0:  78%|  | 660/843 [00:37<00:10, 17.52it/s, loss=1.13, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.628]#015Epoch 0:  82%| | 690/843 [00:39<00:08, 17.55it/s, loss=1.13, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.628]#015Epoch 0:  82%| | 690/843 [00:39<00:08, 17.55it/s, loss=1.16, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.759]#015Epoch 0:  85%| | 720/843 [00:41<00:07, 17.52it/s, loss=1.16, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.759]#015Epoch 0:  85%| | 720/843 [00:41<00:07, 17.52it/s, loss=1.1, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.723] #015Epoch 0:  89%| | 750/843 [00:42<00:05, 17.49it/s, loss=1.1, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.723]#015Epoch 0:  89%| | 750/843 [00:42<00:05, 17.49it/s, loss=1.19, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.789]#015Epoch 0:  93%|| 780/843 [00:44<00:03, 17.60it/s, loss=1.19, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.789]\u001b[0m\n",
      "\u001b[34m#015Validating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:   0%|          | 0/68 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  44%|     | 30/68 [00:01<00:01, 24.54it/s]#033[A#015Epoch 0:  96%|| 810/843 [00:45<00:01, 17.78it/s, loss=1.19, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.789]\u001b[0m\n",
      "\u001b[34m#015Validating:  88%| | 60/68 [00:02<00:00, 25.05it/s]#033[A#015Epoch 0: 100%|| 840/843 [00:46<00:00, 17.99it/s, loss=1.19, v_num=0, val_f1_epoch=0.120, val_loss_epoch=0.719, train_f1=0.789]\u001b[0m\n",
      "\u001b[34m#015Validating: 100%|| 68/68 [00:02<00:00, 26.04it/s]#033[A#015Epoch 0: 100%|| 843/843 [00:47<00:00, 17.93it/s, loss=0.986, v_num=0, val_f1_epoch=0.647, val_loss_epoch=0.326, train_f1=0.702, val_f1_step=0.573, val_loss_step=0.341]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#015                                                           #033[A#015Epoch 0: 100%|| 843/843 [00:58<00:00, 14.29it/s, loss=0.986, v_num=0, val_f1_epoch=0.647, val_loss_epoch=0.326, train_f1=0.702, val_f1_step=0.573, val_loss_step=0.341]#015Validation sanity check: 0it [00:00, ?it/s]#015Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]#015                                                              #015#015Training: 99it [00:00, ?it/s]#015Training:   0%|          | 0/831 [00:00<?, ?it/s]#015Epoch 0:   0%|          | 0/831 [00:00<?, ?it/s] #015Epoch 0:   4%|         | 30/831 [00:01<00:48, 16.50it/s]#015Epoch 0:   4%|         | 30/831 [00:01<00:48, 16.50it/s, loss=5.85, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.514]#015Epoch 0:   7%|         | 60/831 [00:03<00:46, 16.62it/s, loss=5.85, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.514]#015Epoch 0:   7%|         | 60/831 [00:03<00:46, 16.62it/s, loss=3.18, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.477]#015Epoch 0:  11%|         | 90/831 [00:05<00:43, 16.92it/s, loss=3.18, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.477]#015Epoch 0:  11%|         | 90/831 [00:05<00:43, 16.92it/s, loss=2.79, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.479]#015Epoch 0:  14%|        | 120/831 [00:07<00:41, 16.93it/s, loss=2.79, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.479]#015Epoch 0:  14%|        | 120/831 [00:07<00:41, 16.93it/s, loss=2.16, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.486]#015Epoch 0:  18%|        | 150/831 [00:08<00:39, 17.07it/s, loss=2.16, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.486]#015Epoch 0:  18%|        | 150/831 [00:08<00:39, 17.07it/s, loss=1.74, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.489]#015Epoch 0:  22%|       | 180/831 [00:10<00:37, 17.30it/s, loss=1.74, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.489]#015Epoch 0:  22%|       | 180/831 [00:10<00:37, 17.30it/s, loss=2.13, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.478]#015Epoch 0:  25%|       | 210/831 [00:12<00:35, 17.34it/s, loss=2.13, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.478]#015Epoch 0:  25%|       | 210/831 [00:12<00:35, 17.34it/s, loss=2.03, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.487]#015Epoch 0:  29%|       | 240/831 [00:13<00:33, 17.44it/s, loss=2.03, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.487]#015Epoch 0:  29%|       | 240/831 [00:13<00:33, 17.44it/s, loss=2.06, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.490]#015Epoch 0:  32%|      | 270/831 [00:15<00:32, 17.39it/s, loss=2.06, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.490]#015Epoch 0:  32%|      | 270/831 [00:15<00:32, 17.39it/s, loss=2.09, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.480]#015Epoch 0:  36%|      | 300/831 [00:17<00:30, 17.40it/s, loss=2.09, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.480]#015Epoch 0:  36%|      | 300/831 [00:17<00:30, 17.40it/s, loss=2.17, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.479]#015Epoch 0:  40%|      | 330/831 [00:18<00:28, 17.49it/s, loss=2.17, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.479]#015Epoch 0:  40%|      | 330/831 [00:18<00:28, 17.49it/s, loss=1.91, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.479]#015Epoch 0:  43%|     | 360/831 [00:20<00:26, 17.48it/s, loss=1.91, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.479]#015Epoch 0:  43%|     | 360/831 [00:20<00:26, 17.48it/s, loss=2.43, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.478]#015Epoch 0:  47%|     | 390/831 [00:22<00:25, 17.52it/s, loss=2.43, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.478]#015Epoch 0:  47%|     | 390/831 [00:22<00:25, 17.52it/s, loss=2.31, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.520]#015Epoch 0:  51%|     | 420/831 [00:24<00:23, 17.46it/s, loss=2.31, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.520]#015Epoch 0:  51%|     | 420/831 [00:24<00:23, 17.46it/s, loss=1.93, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.496]#015Epoch 0:  54%|    | 450/831 [00:25<00:21, 17.45it/s, loss=1.93, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.496]#015Epoch 0:  54%|    | 450/831 [00:25<00:21, 17.45it/s, loss=2.22, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.480]#015Epoch 0:  58%|    | 480/831 [00:27<00:20, 17.51it/s, loss=2.22, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.480]#015Epoch 0:  58%|    | 480/831 [00:27<00:20, 17.51it/s, loss=2.14, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.500]#015Epoch 0:  61%|   | 510/831 [00:29<00:18, 17.51it/s, loss=2.14, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.500]#015Epoch 0:  61%|   | 510/831 [00:29<00:18, 17.51it/s, loss=2.64, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.497]#015Epoch 0:  65%|   | 540/831 [00:30<00:16, 17.52it/s, loss=2.64, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.497]#015Epoch 0:  65%|   | 540/831 [00:30<00:16, 17.52it/s, loss=2.2, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.518] #015Epoch 0:  69%|   | 570/831 [00:32<00:14, 17.47it/s, loss=2.2, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.518]#015Epoch 0:  69%|   | 570/831 [00:32<00:14, 17.47it/s, loss=2.04, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.494]#015Epoch 0:  72%|  | 600/831 [00:34<00:13, 17.44it/s, loss=2.04, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.494]#015Epoch 0:  72%|  | 600/831 [00:34<00:13, 17.44it/s, loss=2.1, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.497] #015Epoch 0:  76%|  | 630/831 [00:36<00:11, 17.45it/s, loss=2.1, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.497]#015Epoch 0:  76%|  | 630/831 [00:36<00:11, 17.45it/s, loss=2.17, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.481]#015Epoch 0:  79%|  | 660/831 [00:37<00:09, 17.43it/s, loss=2.17, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.481]#015Epoch 0:  79%|  | 660/831 [00:37<00:09, 17.43it/s, loss=2.04, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.479]#015Epoch 0:  83%| | 690/831 [00:39<00:08, 17.44it/s, loss=2.04, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.479]#015Epoch 0:  83%| | 690/831 [00:39<00:08, 17.44it/s, loss=2.35, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.510]#015Epoch 0:  87%| | 720/831 [00:41<00:06, 17.43it/s, loss=2.35, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.510]#015Epoch 0:  87%| | 720/831 [00:41<00:06, 17.43it/s, loss=2.37, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.530]#015Epoch 0:  90%| | 750/831 [00:43<00:04, 17.43it/s, loss=2.37, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.530]#015Epoch 0:  90%| | 750/831 [00:43<00:04, 17.43it/s, loss=2.05, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.496]#015Epoch 0:  94%|| 780/831 [00:43<00:02, 17.77it/s, loss=2.05, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.496]\u001b[0m\n",
      "\u001b[34m#015Validating: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:   0%|          | 0/67 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating:  45%|     | 30/67 [00:01<00:01, 24.81it/s]#033[A#015Epoch 0:  97%|| 810/831 [00:45<00:01, 17.96it/s, loss=2.05, v_num=0, val_f1_epoch=0.0519, val_loss_epoch=0.697, train_f1=0.496]\u001b[0m\n",
      "\u001b[34m#015Validating:  90%| | 60/67 [00:02<00:00, 25.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Validating: 100%|| 67/67 [00:02<00:00, 25.67it/s]#033[A#015Epoch 0: 100%|| 831/831 [00:46<00:00, 17.84it/s, loss=2.53, v_num=0, val_f1_epoch=0.486, val_loss_epoch=0.177, train_f1=0.543, val_f1_step=0.486, val_loss_step=0.191]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#015                                                           #033[A#015Epoch 0: 100%|| 831/831 [00:55<00:00, 14.84it/s, loss=2.53, v_num=0, val_f1_epoch=0.486, val_loss_epoch=0.177, train_f1=0.543, val_f1_step=0.486, val_loss_step=0.191]#015Epoch 0: 100%|| 843/843 [03:32<00:00,  3.96it/s, loss=0.986, v_num=0, val_f1_epoch=0.647, val_loss_epoch=0.326, train_f1=0.702, val_f1_step=0.573, val_loss_step=0.341]\u001b[0m\n",
      "\u001b[34m#015Epoch 0: 100%|| 831/831 [01:28<00:00,  9.44it/s, loss=2.53, v_num=0, val_f1_epoch=0.486, val_loss_epoch=0.177, train_f1=0.543, val_f1_step=0.486, val_loss_step=0.191]\u001b[0m\n",
      "\u001b[34m#015Epoch 0: 100%|| 800/800 [05:31<00:00,  2.41it/s, loss=1.67, v_num=0, val_f1_epoch=0.488, val_loss_epoch=0.229, train_f1=0.476, val_f1_step=0.479, val_loss_step=0.228]\u001b[0m\n",
      "\u001b[34m2021-10-25 14:23:13.394844: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/decorators.py:65: LightningDeprecationWarning: The `@auto_move_data` decorator is deprecated in v1.3 and will be removed in v1.5. Please use `trainer.predict` instead for inference. The decorator was applied to `forward`\n",
      "  \"The `@auto_move_data` decorator is deprecated in v1.3 and will be removed in v1.5.\"\u001b[0m\n",
      "\u001b[34mINFO:root:reading, preprocessing data\u001b[0m\n",
      "\u001b[34m2021/10/25 14:23:22 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\u001b[0m\n",
      "\u001b[34mINFO:numexpr.utils:NumExpr defaulting to 8 threads.\u001b[0m\n",
      "\u001b[34mGPU available: True, used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/525 [00:00<?, ?B/s]#015Downloading: 100%|| 525/525 [00:00<00:00, 545kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading: 100%|| 232k/232k [00:00<00:00, 35.9MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/51.0M [00:00<?, ?B/s]#015Downloading:  11%|         | 5.55M/51.0M [00:00<00:00, 55.5MB/s]#015Downloading:  20%|        | 10.3M/51.0M [00:00<00:00, 52.7MB/s]#015Downloading:  30%|       | 15.2M/51.0M [00:00<00:00, 51.6MB/s]#015Downloading:  40%|      | 20.2M/51.0M [00:00<00:00, 51.1MB/s]#015Downloading:  49%|     | 25.2M/51.0M [00:00<00:00, 50.9MB/s]#015Downloading:  59%|    | 30.2M/51.0M [00:00<00:00, 50.6MB/s]#015Downloading:  69%|   | 35.3M/51.0M [00:00<00:00, 50.6MB/s]#015Downloading:  79%|  | 40.4M/51.0M [00:00<00:00, 50.8MB/s]#015Downloading:  89%| | 45.6M/51.0M [00:00<00:00, 51.2MB/s]#015Downloading: 100%|| 50.9M/51.0M [00:01<00:00, 51.6MB/s]#015Downloading: 100%|| 51.0M/51.0M [00:01<00:00, 50.8MB/s]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[0m\n",
      "\u001b[34mFIT Profiler Report\n",
      "\u001b[0m\n",
      "\u001b[34mAction                             #011|  Mean duration (s)#011|Num calls      #011|  Total time (s) #011|  Percentage %   #011|\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mTotal                              #011|  -              #011|_              #011|  8.8255         #011|  100 %          #011|\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mevaluation_step_and_end            #011|  1.1545         #011|2              #011|  2.3089         #011|  26.162         #011|\u001b[0m\n",
      "\u001b[34mvalidation_step                    #011|  1.1541         #011|2              #011|  2.3082         #011|  26.153         #011|\u001b[0m\n",
      "\u001b[34mcache_result                       #011|  2.3831e-05     #011|15             #011|  0.00035747     #011|  0.0040504      #011|\u001b[0m\n",
      "\u001b[34mon_validation_batch_start          #011|  5.7543e-05     #011|2              #011|  0.00011509     #011|  0.001304       #011|\u001b[0m\n",
      "\u001b[34mvalidation_step_end                #011|  1.7963e-05     #011|2              #011|  3.5926e-05     #011|  0.00040707     #011|\u001b[0m\n",
      "\u001b[34mon_validation_batch_end            #011|  1.6482e-05     #011|2              #011|  3.2965e-05     #011|  0.00037352     #011|\u001b[0m\n",
      "\u001b[34mon_before_accelerator_backend_setup#011|  3.0949e-05     #011|1              #011|  3.0949e-05     #011|  0.00035068     #011|\u001b[0m\n",
      "\u001b[34mon_validation_epoch_end            #011|  2.1412e-05     #011|1              #011|  2.1412e-05     #011|  0.00024261     #011|\u001b[0m\n",
      "\u001b[34m2021-10-25 14:29:47,540 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mon_validation_start                #011|  1.7067e-05     #011|1              #011|  1.7067e-05     #011|  0.00019338     #011|\u001b[0m\n",
      "\u001b[34mon_epoch_end                       #011|  1.5391e-05     #011|1              #011|  1.5391e-05     #011|  0.00017439     #011|\u001b[0m\n",
      "\u001b[34mon_validation_end                  #011|  1.4484e-05     #011|1              #011|  1.4484e-05     #011|  0.00016411     #011|\u001b[0m\n",
      "\u001b[34mon_val_dataloader                  #011|  1.4122e-05     #011|1              #011|  1.4122e-05     #011|  0.00016001     #011|\u001b[0m\n",
      "\u001b[34mon_epoch_start                     #011|  1.3507e-05     #011|1              #011|  1.3507e-05     #011|  0.00015304     #011|\u001b[0m\n",
      "\u001b[34mon_validation_epoch_start          #011|  1.1914e-05     #011|1              #011|  1.1914e-05     #011|  0.00013499     #011|\n",
      "\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]#015Finding best initial lr:   1%|          | 1/100 [00:00<00:28,  3.49it/s]#015Finding best initial lr:   3%|         | 3/100 [00:00<00:21,  4.54it/s]#015Finding best initial lr:   5%|         | 5/100 [00:00<00:16,  5.82it/s]#015Finding best initial lr:   7%|         | 7/100 [00:00<00:12,  7.25it/s]#015Finding best initial lr:   9%|         | 9/100 [00:00<00:10,  8.75it/s]#015Finding best initial lr:  11%|         | 11/100 [00:00<00:08, 10.23it/s]#015Finding best initial lr:  13%|        | 13/100 [00:01<00:07, 11.57it/s]#015Finding best initial lr:  15%|        | 15/100 [00:01<00:06, 12.72it/s]#015Finding best initial lr:  17%|        | 17/100 [00:01<00:06, 13.49it/s]#015Finding best initial lr:  19%|        | 19/100 [00:01<00:05, 14.52it/s]#015Finding best initial lr:  21%|        | 21/100 [00:01<00:05, 15.35it/s]#015Finding best initial lr:  23%|       | 23/100 [00:01<00:04, 15.99it/s]#015Finding best initial lr:  25%|       | 25/100 [00:01<00:04, 16.45it/s]#015Finding best initial lr:  27%|       | 27/100 [00:01<00:04, 16.80it/s]#015Finding best initial lr:  29%|       | 29/100 [00:01<00:04, 17.06it/s]#015Finding best initial lr:  31%|       | 31/100 [00:02<00:04, 17.09it/s]#015Finding best initial lr:  33%|      | 33/100 [00:02<00:03, 17.09it/s]#015Finding best initial lr:  35%|      | 35/100 [00:02<00:03, 17.10it/s]#015Finding best initial lr:  37%|      | 37/100 [00:02<00:03, 17.11it/s]#015Finding best initial lr:  39%|      | 39/100 [00:02<00:03, 17.28it/s]#015Finding best initial lr:  41%|      | 41/100 [00:02<00:03, 17.40it/s]#015Finding best initial lr:  43%|     | 43/100 [00:02<00:03, 17.49it/s]#015Finding best initial lr:  45%|     | 45/100 [00:02<00:03, 17.55it/s]#015Finding best initial lr:  47%|     | 47/100 [00:02<00:03, 17.62it/s]#015Finding best initial lr:  49%|     | 49/100 [00:03<00:02, 17.67it/s]#015Finding best initial lr:  51%|     | 51/100 [00:03<00:03, 14.62it/s]#015Finding best initial lr:  53%|    | 53/100 [00:03<00:03, 15.40it/s]#015Finding best initial lr:  55%|    | 55/100 [00:03<00:02, 16.00it/s]#015Finding best initial lr:  57%|    | 57/100 [00:03<00:02, 16.49it/s]#015Finding best initial lr:  59%|    | 59/100 [00:03<00:02, 16.81it/s]#015Finding best initial lr:  61%|    | 61/100 [00:03<00:02, 17.07it/s]#015Finding best initial lr:  63%|   | 63/100 [00:03<00:02, 17.27it/s]#015Finding best initial lr:  65%|   | 65/100 [00:04<00:02, 17.43it/s]#015Finding best initial lr:  67%|   | 67/100 [00:04<00:01, 17.41it/s]#015Finding best initial lr:  69%|   | 69/100 [00:04<00:01, 17.48it/s]#015Finding best initial lr:  71%|   | 71/100 [00:04<00:01, 17.57it/s]#015Finding best initial lr:  73%|  | 73/100 [00:04<00:01, 17.63it/s]#015Finding best initial lr:  75%|  | 75/100 [00:04<00:01, 17.68it/s]#015Finding best initial lr:  77%|  | 77/100 [00:04<00:01, 17.01it/s]#015Finding best initial lr:  79%|  | 79/100 [00:04<00:01, 16.76it/s]#015Finding best initial lr:  81%|  | 81/100 [00:04<00:01, 16.84it/s]#015Finding best initial lr:  83%| | 83/100 [00:05<00:01, 16.89it/s]#015Finding best initial lr:  85%| | 85/100 [00:05<00:00, 16.73it/s]#015Finding best initial lr:  87%| | 87/100 [00:05<00:00, 16.80it/s]#015Finding best initial lr:  89%| | 89/100 [00:05<00:00, 16.84it/s]#015Finding best initial lr:  91%| | 91/100 [00:05<00:00, 16.88it/s]#015Finding best initial lr:  93%|| 93/100 [00:05<00:00, 16.93it/s]#015Finding best initial lr:  95%|| 95/100 [00:05<00:00, 16.96it/s]#015Finding best initial lr:  97%|| 97/100 [00:05<00:00, 16.95it/s]#015Finding best initial lr:  99%|| 99/100 [00:06<00:00, 16.96it/s]FIT Profiler Report\n",
      "\u001b[0m\n",
      "\u001b[34mAction                             #011|  Mean duration (s)#011|Num calls      #011|  Total time (s) #011|  Percentage %   #011|\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mTotal                              #011|  -              #011|_              #011|  15.155         #011|  100 %          #011|\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mrun_training_epoch                 #011|  6.3185         #011|1              #011|  6.3185         #011|  41.692         #011|\u001b[0m\n",
      "\u001b[34mrun_training_batch                 #011|  0.056869       #011|100            #011|  5.6869         #011|  37.525         #011|\u001b[0m\n",
      "\u001b[34moptimizer_step_and_closure_0       #011|  0.056421       #011|100            #011|  5.6421         #011|  37.228         #011|\u001b[0m\n",
      "\u001b[34mtraining_step_and_backward         #011|  0.046886       #011|100            #011|  4.6886         #011|  30.937         #011|\u001b[0m\n",
      "\u001b[34mevaluation_step_and_end            #011|  1.1545         #011|2              #011|  2.3089         #011|  15.235         #011|\u001b[0m\n",
      "\u001b[34mvalidation_step                    #011|  1.1541         #011|2              #011|  2.3082         #011|  15.23          #011|\u001b[0m\n",
      "\u001b[34mbackward                           #011|  0.022764       #011|100            #011|  2.2764         #011|  15.021         #011|\u001b[0m\n",
      "\u001b[34mmodel_forward                      #011|  0.022526       #011|100            #011|  2.2526         #011|  14.864         #011|\u001b[0m\n",
      "\u001b[34mtraining_step                      #011|  0.022121       #011|100            #011|  2.2121         #011|  14.596         #011|\u001b[0m\n",
      "\u001b[34mget_train_batch                    #011|  0.0034099      #011|100            #011|  0.34099        #011|  2.25           #011|\u001b[0m\n",
      "\u001b[34mon_train_batch_end                 #011|  0.00014716     #011|100            #011|  0.014716       #011|  0.097104       #011|\u001b[0m\n",
      "\u001b[34mcache_result                       #011|  2.1378e-05     #011|422            #011|  0.0090216      #011|  0.059528       #011|\u001b[0m\n",
      "\u001b[34mon_train_start                     #011|  0.003999       #011|1              #011|  0.003999       #011|  0.026387       #011|\u001b[0m\n",
      "\u001b[34mon_batch_start                     #011|  3.2108e-05     #011|100            #011|  0.0032108      #011|  0.021186       #011|\u001b[0m\n",
      "\u001b[34mon_after_backward                  #011|  2.1647e-05     #011|100            #011|  0.0021647      #011|  0.014284       #011|\u001b[0m\n",
      "\u001b[34mon_batch_end                       #011|  1.6055e-05     #011|100            #011|  0.0016055      #011|  0.010594       #011|\u001b[0m\n",
      "\u001b[34mon_before_zero_grad                #011|  1.4982e-05     #011|100            #011|  0.0014982      #011|  0.009886       #011|\u001b[0m\n",
      "\u001b[34mon_train_batch_start               #011|  1.3527e-05     #011|100            #011|  0.0013527      #011|  0.0089253      #011|\u001b[0m\n",
      "\u001b[34mtraining_step_end                  #011|  1.1876e-05     #011|100            #011|  0.0011876      #011|  0.0078362      #011|\u001b[0m\n",
      "\u001b[34mon_train_epoch_end                 #011|  0.00023724     #011|1              #011|  0.00023724     #011|  0.0015654      #011|\u001b[0m\n",
      "\u001b[34mon_validation_batch_start          #011|  5.7543e-05     #011|2              #011|  0.00011509     #011|  0.00075938     #011|\u001b[0m\n",
      "\u001b[34mvalidation_step_end                #011|  1.7963e-05     #011|2              #011|  3.5926e-05     #011|  0.00023705     #011|\u001b[0m\n",
      "\u001b[34mon_epoch_end                       #011|  1.7497e-05     #011|2              #011|  3.4995e-05     #011|  0.00023091     #011|\u001b[0m\n",
      "\u001b[34mon_validation_batch_end            #011|  1.6482e-05     #011|2              #011|  3.2965e-05     #011|  0.00021752     #011|\u001b[0m\n",
      "\u001b[34mon_epoch_start                     #011|  1.5495e-05     #011|2              #011|  3.0989e-05     #011|  0.00020448     #011|\u001b[0m\n",
      "\u001b[34mon_before_accelerator_backend_setup#011|  3.0949e-05     #011|1              #011|  3.0949e-05     #011|  0.00020421     #011|\u001b[0m\n",
      "\u001b[34mon_train_end                       #011|  2.7234e-05     #011|1              #011|  2.7234e-05     #011|  0.0001797      #011|\u001b[0m\n",
      "\u001b[34mon_validation_epoch_end            #011|  2.1412e-05     #011|1              #011|  2.1412e-05     #011|  0.00014128     #011|\u001b[0m\n",
      "\u001b[34mon_validation_start                #011|  1.7067e-05     #011|1              #011|  1.7067e-05     #011|  0.00011261     #011|\u001b[0m\n",
      "\u001b[34mon_train_dataloader                #011|  1.5055e-05     #011|1              #011|  1.5055e-05     #011|  9.9339e-05     #011|\u001b[0m\n",
      "\u001b[34mon_validation_end                  #011|  1.4484e-05     #011|1              #011|  1.4484e-05     #011|  9.5571e-05     #011|\u001b[0m\n",
      "\u001b[34mon_val_dataloader                  #011|  1.4122e-05     #011|1              #011|  1.4122e-05     #011|  9.3182e-05     #011|\u001b[0m\n",
      "\u001b[34mon_train_epoch_start               #011|  1.3171e-05     #011|1              #011|  1.3171e-05     #011|  8.6907e-05     #011|\u001b[0m\n",
      "\u001b[34mon_validation_epoch_start          #011|  1.1914e-05     #011|1              #011|  1.1914e-05     #011|  7.8613e-05     #011|\n",
      "\u001b[0m\n",
      "\u001b[34mRestored states from the checkpoint file at /opt/ml/code/lr_find_temp_model.ckpt\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr: 100%|| 100/100 [00:06<00:00, 15.24it/s]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "  \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/callback_hook.py:101: LightningDeprecationWarning: The signature of `Callback.on_train_epoch_end` has changed in v1.3. `outputs` parameter has been removed. Support for the old signature will be removed in v1.5\n",
      "  \"The signature of `Callback.on_train_epoch_end` has changed in v1.3.\"\u001b[0m\n",
      "\u001b[34mEpoch 0, global step 744: val_loss reached 0.22831 (best 0.22831), saving model to \"/opt/ml/model/model_sectors.ckpt\" as top 1\n",
      "\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/55 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  2%|         | 1/55 [00:00<00:06,  8.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  7%|         | 4/55 [00:00<00:04, 11.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 13%|        | 7/55 [00:00<00:03, 13.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 18%|        | 10/55 [00:00<00:02, 16.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 24%|       | 13/55 [00:00<00:02, 18.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 29%|       | 16/55 [00:00<00:01, 20.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 35%|      | 19/55 [00:00<00:01, 22.15it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 40%|      | 22/55 [00:00<00:01, 23.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 45%|     | 25/55 [00:00<00:01, 23.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 51%|     | 28/55 [00:01<00:01, 24.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 56%|    | 31/55 [00:01<00:00, 25.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 62%|   | 34/55 [00:01<00:00, 26.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 67%|   | 37/55 [00:01<00:00, 26.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 73%|  | 40/55 [00:01<00:00, 27.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 78%|  | 43/55 [00:01<00:00, 27.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 84%| | 46/55 [00:01<00:00, 26.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 91%| | 50/55 [00:01<00:00, 27.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 96%|| 53/55 [00:02<00:00, 27.00it/s]#033[A#01556it [00:02, 26.22it/s]                        \n",
      "\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/55 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  2%|         | 1/55 [00:00<00:06,  8.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  7%|         | 4/55 [00:00<00:04, 10.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 13%|        | 7/55 [00:00<00:03, 13.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 18%|        | 10/55 [00:00<00:02, 15.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 24%|       | 13/55 [00:00<00:02, 18.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 29%|       | 16/55 [00:00<00:01, 20.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 35%|      | 19/55 [00:00<00:01, 21.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 40%|      | 22/55 [00:00<00:01, 23.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 45%|     | 25/55 [00:00<00:01, 24.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 51%|     | 28/55 [00:01<00:01, 25.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 56%|    | 31/55 [00:01<00:00, 26.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 62%|   | 34/55 [00:01<00:00, 26.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 67%|   | 37/55 [00:01<00:00, 26.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 73%|  | 40/55 [00:01<00:00, 26.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 78%|  | 43/55 [00:01<00:00, 27.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 84%| | 46/55 [00:01<00:00, 27.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 89%| | 49/55 [00:01<00:00, 26.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 95%|| 52/55 [00:01<00:00, 27.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|| 55/55 [00:02<00:00, 27.15it/s]#033[A#01556it [00:02, 26.16it/s]                        \u001b[0m\n",
      "\u001b[34mGPU available: True, used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[0m\n",
      "\u001b[34mFIT Profiler Report\n",
      "\u001b[0m\n",
      "\u001b[34mAction                             #011|  Mean duration (s)#011|Num calls      #011|  Total time (s) #011|  Percentage %   #011|\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mTotal                              #011|  -              #011|_              #011|  1.1101         #011|  100 %          #011|\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mevaluation_step_and_end            #011|  0.022274       #011|2              #011|  0.044548       #011|  4.0128         #011|\u001b[0m\n",
      "\u001b[34mvalidation_step                    #011|  0.02191        #011|2              #011|  0.043821       #011|  3.9473         #011|\u001b[0m\n",
      "\u001b[34mcache_result                       #011|  2.3708e-05     #011|15             #011|  0.00035561     #011|  0.032033       #011|\u001b[0m\n",
      "\u001b[34mon_validation_batch_start          #011|  5.8772e-05     #011|2              #011|  0.00011754     #011|  0.010588       #011|\u001b[0m\n",
      "\u001b[34mvalidation_step_end                #011|  1.8821e-05     #011|2              #011|  3.7643e-05     #011|  0.0033908      #011|\u001b[0m\n",
      "\u001b[34mon_validation_batch_end            #011|  1.6249e-05     #011|2              #011|  3.2499e-05     #011|  0.0029274      #011|\u001b[0m\n",
      "\u001b[34mon_before_accelerator_backend_setup#011|  2.5819e-05     #011|1              #011|  2.5819e-05     #011|  0.0023257      #011|\u001b[0m\n",
      "\u001b[34mon_validation_epoch_end            #011|  2.1522e-05     #011|1              #011|  2.1522e-05     #011|  0.0019387      #011|\u001b[0m\n",
      "\u001b[34mon_validation_start                #011|  2.1214e-05     #011|1              #011|  2.1214e-05     #011|  0.0019109      #011|\u001b[0m\n",
      "\u001b[34mon_val_dataloader                  #011|  1.7522e-05     #011|1              #011|  1.7522e-05     #011|  0.0015783      #011|\u001b[0m\n",
      "\u001b[34mon_validation_end                  #011|  1.6257e-05     #011|1              #011|  1.6257e-05     #011|  0.0014644      #011|\u001b[0m\n",
      "\u001b[34mon_epoch_start                     #011|  1.584e-05      #011|1              #011|  1.584e-05      #011|  0.0014268      #011|\u001b[0m\n",
      "\u001b[34mon_epoch_end                       #011|  1.4501e-05     #011|1              #011|  1.4501e-05     #011|  0.0013062      #011|\u001b[0m\n",
      "\u001b[34mon_validation_epoch_start          #011|  1.2075e-05     #011|1              #011|  1.2075e-05     #011|  0.0010877      #011|\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:   2%|         | 2/100 [00:00<00:06, 14.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:   4%|         | 4/100 [00:00<00:06, 15.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:   6%|         | 6/100 [00:00<00:05, 15.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:   8%|         | 8/100 [00:00<00:05, 16.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  10%|         | 10/100 [00:00<00:05, 16.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  12%|        | 12/100 [00:00<00:05, 17.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  14%|        | 14/100 [00:00<00:04, 17.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  16%|        | 16/100 [00:00<00:04, 17.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  18%|        | 18/100 [00:01<00:04, 17.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  20%|        | 20/100 [00:01<00:04, 17.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  22%|       | 22/100 [00:01<00:04, 17.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  24%|       | 24/100 [00:01<00:04, 17.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  26%|       | 26/100 [00:01<00:04, 16.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  28%|       | 28/100 [00:01<00:04, 17.19it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  30%|       | 30/100 [00:01<00:04, 17.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  32%|      | 32/100 [00:01<00:03, 17.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  34%|      | 34/100 [00:01<00:03, 17.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  36%|      | 36/100 [00:02<00:03, 17.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  38%|      | 38/100 [00:02<00:03, 17.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  40%|      | 40/100 [00:02<00:03, 17.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  42%|     | 42/100 [00:02<00:03, 17.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  44%|     | 44/100 [00:02<00:03, 17.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  46%|     | 46/100 [00:02<00:03, 17.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  48%|     | 48/100 [00:02<00:02, 17.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  50%|     | 50/100 [00:02<00:02, 17.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  52%|    | 52/100 [00:03<00:03, 14.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  54%|    | 54/100 [00:03<00:03, 15.00it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  56%|    | 56/100 [00:03<00:02, 15.00it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  58%|    | 58/100 [00:03<00:02, 15.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  60%|    | 60/100 [00:03<00:02, 16.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  62%|   | 62/100 [00:03<00:02, 16.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  64%|   | 64/100 [00:03<00:02, 16.97it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  66%|   | 66/100 [00:03<00:01, 17.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  68%|   | 68/100 [00:03<00:01, 17.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  70%|   | 70/100 [00:04<00:01, 17.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  72%|  | 72/100 [00:04<00:01, 17.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  74%|  | 74/100 [00:04<00:01, 17.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  76%|  | 76/100 [00:04<00:01, 17.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  78%|  | 78/100 [00:04<00:01, 17.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  80%|  | 80/100 [00:04<00:01, 17.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  82%| | 82/100 [00:04<00:01, 17.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  84%| | 84/100 [00:04<00:00, 17.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  86%| | 86/100 [00:05<00:00, 17.74it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  88%| | 88/100 [00:05<00:00, 17.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  90%| | 90/100 [00:05<00:00, 17.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  92%|| 92/100 [00:05<00:00, 17.72it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  94%|| 94/100 [00:05<00:00, 17.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  96%|| 96/100 [00:05<00:00, 17.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  98%|| 98/100 [00:05<00:00, 17.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr: 100%|| 100/100 [00:05<00:00, 17.82it/s]#033[AFIT Profiler Report\n",
      "\u001b[0m\n",
      "\u001b[34mAction                             #011|  Mean duration (s)#011|Num calls      #011|  Total time (s) #011|  Percentage %   #011|\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mTotal                              #011|  -              #011|_              #011|  7.131          #011|  100 %          #011|\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mrun_training_epoch                 #011|  6.0126         #011|1              #011|  6.0126         #011|  84.317         #011|\u001b[0m\n",
      "\u001b[34mrun_training_batch                 #011|  0.053702       #011|100            #011|  5.3702         #011|  75.308         #011|\u001b[0m\n",
      "\u001b[34moptimizer_step_and_closure_0       #011|  0.053288       #011|100            #011|  5.3288         #011|  74.727         #011|\u001b[0m\n",
      "\u001b[34mtraining_step_and_backward         #011|  0.043461       #011|100            #011|  4.3461         #011|  60.946         #011|\u001b[0m\n",
      "\u001b[34mbackward                           #011|  0.022872       #011|100            #011|  2.2872         #011|  32.074         #011|\u001b[0m\n",
      "\u001b[34mmodel_forward                      #011|  0.018929       #011|100            #011|  1.8929         #011|  26.545         #011|\u001b[0m\n",
      "\u001b[34mtraining_step                      #011|  0.018625       #011|100            #011|  1.8625         #011|  26.118         #011|\u001b[0m\n",
      "\u001b[34mget_train_batch                    #011|  0.0033512      #011|100            #011|  0.33512        #011|  4.6995         #011|\u001b[0m\n",
      "\u001b[34mevaluation_step_and_end            #011|  0.022274       #011|2              #011|  0.044548       #011|  0.6247         #011|\u001b[0m\n",
      "\u001b[34mvalidation_step                    #011|  0.02191        #011|2              #011|  0.043821       #011|  0.61451        #011|\u001b[0m\n",
      "\u001b[34mon_train_batch_end                 #011|  0.00017864     #011|100            #011|  0.017864       #011|  0.25051        #011|\u001b[0m\n",
      "\u001b[34mcache_result                       #011|  2.2642e-05     #011|422            #011|  0.0095549      #011|  0.13399        #011|\u001b[0m\n",
      "\u001b[34mon_train_start                     #011|  0.0040783      #011|1              #011|  0.0040783      #011|  0.057191       #011|\u001b[0m\n",
      "\u001b[34mon_batch_start                     #011|  3.3167e-05     #011|100            #011|  0.0033167      #011|  0.046511       #011|\u001b[0m\n",
      "\u001b[34mon_after_backward                  #011|  2.3263e-05     #011|100            #011|  0.0023263      #011|  0.032622       #011|\u001b[0m\n",
      "\u001b[34mon_batch_end                       #011|  1.682e-05      #011|100            #011|  0.001682       #011|  0.023587       #011|\u001b[0m\n",
      "\u001b[34mon_before_zero_grad                #011|  1.5705e-05     #011|100            #011|  0.0015705      #011|  0.022024       #011|\u001b[0m\n",
      "\u001b[34mon_train_batch_start               #011|  1.4627e-05     #011|100            #011|  0.0014627      #011|  0.020512       #011|\u001b[0m\n",
      "\u001b[34mtraining_step_end                  #011|  1.2883e-05     #011|100            #011|  0.0012883      #011|  0.018067       #011|\u001b[0m\n",
      "\u001b[34mon_train_epoch_end                 #011|  0.00023621     #011|1              #011|  0.00023621     #011|  0.0033124      #011|\u001b[0m\n",
      "\u001b[34mon_validation_batch_start          #011|  5.8772e-05     #011|2              #011|  0.00011754     #011|  0.0016484      #011|\u001b[0m\n",
      "\u001b[34mvalidation_step_end                #011|  1.8821e-05     #011|2              #011|  3.7643e-05     #011|  0.00052788     #011|\u001b[0m\n",
      "\u001b[34mon_epoch_end                       #011|  1.7317e-05     #011|2              #011|  3.4634e-05     #011|  0.00048568     #011|\u001b[0m\n",
      "\u001b[34mon_epoch_start                     #011|  1.6496e-05     #011|2              #011|  3.2991e-05     #011|  0.00046264     #011|\u001b[0m\n",
      "\u001b[34mon_validation_batch_end            #011|  1.6249e-05     #011|2              #011|  3.2499e-05     #011|  0.00045574     #011|\u001b[0m\n",
      "\u001b[34mon_train_end                       #011|  3.0424e-05     #011|1              #011|  3.0424e-05     #011|  0.00042664     #011|\u001b[0m\n",
      "\u001b[34mon_before_accelerator_backend_setup#011|  2.5819e-05     #011|1              #011|  2.5819e-05     #011|  0.00036207     #011|\u001b[0m\n",
      "\u001b[34mon_validation_epoch_end            #011|  2.1522e-05     #011|1              #011|  2.1522e-05     #011|  0.00030181     #011|\u001b[0m\n",
      "\u001b[34mon_validation_start                #011|  2.1214e-05     #011|1              #011|  2.1214e-05     #011|  0.00029749     #011|\u001b[0m\n",
      "\u001b[34mon_val_dataloader                  #011|  1.7522e-05     #011|1              #011|  1.7522e-05     #011|  0.00024572     #011|\u001b[0m\n",
      "\u001b[34mon_train_dataloader                #011|  1.6855e-05     #011|1              #011|  1.6855e-05     #011|  0.00023636     #011|\u001b[0m\n",
      "\u001b[34mon_validation_end                  #011|  1.6257e-05     #011|1              #011|  1.6257e-05     #011|  0.00022798     #011|\u001b[0m\n",
      "\u001b[34mon_train_epoch_start               #011|  1.3725e-05     #011|1              #011|  1.3725e-05     #011|  0.00019247     #011|\u001b[0m\n",
      "\u001b[34mon_validation_epoch_start          #011|  1.2075e-05     #011|1              #011|  1.2075e-05     #011|  0.00016933     #011|\n",
      "\u001b[0m\n",
      "\u001b[34mRestored states from the checkpoint file at /opt/ml/code/lr_find_temp_model.ckpt\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr: 100%|| 100/100 [00:19<00:00, 17.82it/s]#033[A/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "  \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n",
      "\u001b[0m\n",
      "\u001b[34m#015                                                                          #015#033[AEpoch 0, global step 775: val_loss reached 0.34121 (best 0.34121), saving model to \"/opt/ml/model/model_pillars_2d.ckpt\" as top 1\n",
      "\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/67 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  1%|         | 1/67 [00:00<00:07,  9.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  6%|         | 4/67 [00:00<00:05, 11.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 10%|         | 7/67 [00:00<00:04, 13.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 15%|        | 10/67 [00:00<00:03, 16.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 19%|        | 13/67 [00:00<00:02, 18.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 24%|       | 16/67 [00:00<00:02, 20.17it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 28%|       | 19/67 [00:00<00:02, 21.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 33%|      | 22/67 [00:00<00:01, 22.95it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 37%|      | 25/67 [00:01<00:01, 24.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 42%|     | 28/67 [00:01<00:01, 24.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 46%|     | 31/67 [00:01<00:01, 25.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 51%|     | 34/67 [00:01<00:01, 26.17it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 55%|    | 37/67 [00:01<00:01, 26.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 60%|    | 40/67 [00:01<00:00, 27.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 64%|   | 43/67 [00:01<00:00, 26.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 69%|   | 46/67 [00:01<00:00, 26.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 73%|  | 49/67 [00:01<00:00, 26.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 78%|  | 52/67 [00:01<00:00, 27.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 82%| | 55/67 [00:02<00:00, 27.37it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 87%| | 58/67 [00:02<00:00, 27.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 91%| | 61/67 [00:02<00:00, 26.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 96%|| 64/67 [00:02<00:00, 26.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|| 67/67 [00:02<00:00, 25.37it/s]#033[A#01568it [00:02, 25.66it/s]                        \n",
      "\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/69 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  1%|         | 1/69 [00:00<00:07,  9.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  6%|         | 4/69 [00:00<00:05, 11.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 10%|         | 7/69 [00:00<00:04, 13.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 14%|        | 10/69 [00:00<00:03, 16.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 19%|        | 13/69 [00:00<00:03, 18.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 23%|       | 16/69 [00:00<00:02, 19.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 28%|       | 19/69 [00:00<00:02, 21.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 32%|      | 22/69 [00:00<00:02, 22.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 36%|      | 25/69 [00:01<00:01, 23.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 41%|      | 28/69 [00:01<00:01, 24.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 45%|     | 31/69 [00:01<00:01, 25.17it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 49%|     | 34/69 [00:01<00:01, 25.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 54%|    | 37/69 [00:01<00:01, 26.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 58%|    | 40/69 [00:01<00:01, 25.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 62%|   | 43/69 [00:01<00:00, 26.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 67%|   | 46/69 [00:01<00:00, 26.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 71%|   | 49/69 [00:01<00:00, 26.41it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 75%|  | 52/69 [00:02<00:00, 26.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 80%|  | 55/69 [00:02<00:00, 25.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 84%| | 58/69 [00:02<00:00, 26.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 88%| | 61/69 [00:02<00:00, 26.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 93%|| 64/69 [00:02<00:00, 26.95it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 97%|| 67/69 [00:02<00:00, 27.64it/s]#033[A#01570it [00:02, 25.67it/s]                        \u001b[0m\n",
      "\u001b[34mGPU available: True, used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[0m\n",
      "\u001b[34mFIT Profiler Report\n",
      "\u001b[0m\n",
      "\u001b[34mAction                             #011|  Mean duration (s)#011|Num calls      #011|  Total time (s) #011|  Percentage %   #011|\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mTotal                              #011|  -              #011|_              #011|  1.1837         #011|  100 %          #011|\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mevaluation_step_and_end            #011|  0.022519       #011|2              #011|  0.045039       #011|  3.8049         #011|\u001b[0m\n",
      "\u001b[34mvalidation_step                    #011|  0.022129       #011|2              #011|  0.044258       #011|  3.739          #011|\u001b[0m\n",
      "\u001b[34mcache_result                       #011|  2.3824e-05     #011|15             #011|  0.00035737     #011|  0.03019        #011|\u001b[0m\n",
      "\u001b[34mon_validation_batch_start          #011|  6.7073e-05     #011|2              #011|  0.00013415     #011|  0.011333       #011|\u001b[0m\n",
      "\u001b[34mvalidation_step_end                #011|  1.7379e-05     #011|2              #011|  3.4759e-05     #011|  0.0029365      #011|\u001b[0m\n",
      "\u001b[34mon_validation_batch_end            #011|  1.6217e-05     #011|2              #011|  3.2434e-05     #011|  0.00274        #011|\u001b[0m\n",
      "\u001b[34mon_before_accelerator_backend_setup#011|  2.6788e-05     #011|1              #011|  2.6788e-05     #011|  0.0022631      #011|\u001b[0m\n",
      "\u001b[34mon_validation_epoch_end            #011|  2.3057e-05     #011|1              #011|  2.3057e-05     #011|  0.0019479      #011|\u001b[0m\n",
      "\u001b[34mon_validation_start                #011|  2.1185e-05     #011|1              #011|  2.1185e-05     #011|  0.0017897      #011|\u001b[0m\n",
      "\u001b[34mon_validation_end                  #011|  1.5973e-05     #011|1              #011|  1.5973e-05     #011|  0.0013494      #011|\u001b[0m\n",
      "\u001b[34mon_epoch_start                     #011|  1.5891e-05     #011|1              #011|  1.5891e-05     #011|  0.0013425      #011|\u001b[0m\n",
      "\u001b[34mon_epoch_end                       #011|  1.5407e-05     #011|1              #011|  1.5407e-05     #011|  0.0013016      #011|\u001b[0m\n",
      "\u001b[34mon_val_dataloader                  #011|  1.5213e-05     #011|1              #011|  1.5213e-05     #011|  0.0012852      #011|\u001b[0m\n",
      "\u001b[34mon_validation_epoch_start          #011|  1.2228e-05     #011|1              #011|  1.2228e-05     #011|  0.001033       #011|\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:   2%|         | 2/100 [00:00<00:06, 14.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:   4%|         | 4/100 [00:00<00:06, 15.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:   6%|         | 6/100 [00:00<00:05, 15.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:   8%|         | 8/100 [00:00<00:05, 16.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  10%|         | 10/100 [00:00<00:05, 16.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  12%|        | 12/100 [00:00<00:05, 17.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  14%|        | 14/100 [00:00<00:05, 17.15it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  16%|        | 16/100 [00:00<00:04, 17.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  18%|        | 18/100 [00:01<00:04, 17.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  20%|        | 20/100 [00:01<00:04, 17.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  22%|       | 22/100 [00:01<00:04, 17.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  24%|       | 24/100 [00:01<00:04, 17.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  26%|       | 26/100 [00:01<00:04, 18.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  28%|       | 28/100 [00:01<00:03, 18.12it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  30%|       | 30/100 [00:01<00:03, 18.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  32%|      | 32/100 [00:01<00:03, 18.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  34%|      | 34/100 [00:01<00:03, 17.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  36%|      | 36/100 [00:02<00:03, 17.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  38%|      | 38/100 [00:02<00:03, 18.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  40%|      | 40/100 [00:02<00:03, 18.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  42%|     | 42/100 [00:02<00:03, 18.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  44%|     | 44/100 [00:02<00:03, 18.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  46%|     | 46/100 [00:02<00:02, 18.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  48%|     | 48/100 [00:02<00:02, 18.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  50%|     | 50/100 [00:02<00:02, 18.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  52%|    | 52/100 [00:02<00:03, 15.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  54%|    | 54/100 [00:03<00:02, 15.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  56%|    | 56/100 [00:03<00:02, 16.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  58%|    | 58/100 [00:03<00:02, 17.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  60%|    | 60/100 [00:03<00:02, 17.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  62%|   | 62/100 [00:03<00:02, 17.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  64%|   | 64/100 [00:03<00:01, 18.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  66%|   | 66/100 [00:03<00:01, 18.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  68%|   | 68/100 [00:03<00:01, 18.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  70%|   | 70/100 [00:03<00:01, 18.29it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  72%|  | 72/100 [00:04<00:01, 18.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  74%|  | 74/100 [00:04<00:01, 18.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  76%|  | 76/100 [00:04<00:01, 18.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  78%|  | 78/100 [00:04<00:01, 18.41it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  80%|  | 80/100 [00:04<00:01, 18.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  82%| | 82/100 [00:04<00:00, 18.49it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  84%| | 84/100 [00:04<00:00, 18.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  86%| | 86/100 [00:04<00:00, 18.54it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  88%| | 88/100 [00:04<00:00, 18.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  90%| | 90/100 [00:05<00:00, 18.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  92%|| 92/100 [00:05<00:00, 17.98it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  94%|| 94/100 [00:05<00:00, 17.93it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  96%|| 96/100 [00:05<00:00, 17.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr:  98%|| 98/100 [00:05<00:00, 17.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr: 100%|| 100/100 [00:05<00:00, 17.81it/s]#033[AFIT Profiler Report\n",
      "\u001b[0m\n",
      "\u001b[34mAction                             #011|  Mean duration (s)#011|Num calls      #011|  Total time (s) #011|  Percentage %   #011|\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mTotal                              #011|  -              #011|_              #011|  7.0119         #011|  100 %          #011|\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mrun_training_epoch                 #011|  5.8188         #011|1              #011|  5.8188         #011|  82.985         #011|\u001b[0m\n",
      "\u001b[34mrun_training_batch                 #011|  0.051844       #011|100            #011|  5.1844         #011|  73.938         #011|\u001b[0m\n",
      "\u001b[34moptimizer_step_and_closure_0       #011|  0.051446       #011|100            #011|  5.1446         #011|  73.37          #011|\u001b[0m\n",
      "\u001b[34mtraining_step_and_backward         #011|  0.042022       #011|100            #011|  4.2022         #011|  59.929         #011|\u001b[0m\n",
      "\u001b[34mbackward                           #011|  0.022543       #011|100            #011|  2.2543         #011|  32.149         #011|\u001b[0m\n",
      "\u001b[34mmodel_forward                      #011|  0.017882       #011|100            #011|  1.7882         #011|  25.502         #011|\u001b[0m\n",
      "\u001b[34mtraining_step                      #011|  0.017589       #011|100            #011|  1.7589         #011|  25.085         #011|\u001b[0m\n",
      "\u001b[34mget_train_batch                    #011|  0.0032951      #011|100            #011|  0.32951        #011|  4.6993         #011|\u001b[0m\n",
      "\u001b[34mevaluation_step_and_end            #011|  0.022519       #011|2              #011|  0.045039       #011|  0.64232        #011|\u001b[0m\n",
      "\u001b[34mvalidation_step                    #011|  0.022129       #011|2              #011|  0.044258       #011|  0.63119        #011|\u001b[0m\n",
      "\u001b[34mon_train_batch_end                 #011|  0.00017289     #011|100            #011|  0.017289       #011|  0.24657        #011|\u001b[0m\n",
      "\u001b[34mcache_result                       #011|  2.1978e-05     #011|422            #011|  0.0092745      #011|  0.13227        #011|\u001b[0m\n",
      "\u001b[34mon_train_start                     #011|  0.0051668      #011|1              #011|  0.0051668      #011|  0.073686       #011|\u001b[0m\n",
      "\u001b[34mon_batch_start                     #011|  3.1887e-05     #011|100            #011|  0.0031887      #011|  0.045476       #011|\u001b[0m\n",
      "\u001b[34mon_after_backward                  #011|  2.2051e-05     #011|100            #011|  0.0022051      #011|  0.031448       #011|\u001b[0m\n",
      "\u001b[34mon_batch_end                       #011|  1.6448e-05     #011|100            #011|  0.0016448      #011|  0.023457       #011|\u001b[0m\n",
      "\u001b[34mon_before_zero_grad                #011|  1.5481e-05     #011|100            #011|  0.0015481      #011|  0.022078       #011|\u001b[0m\n",
      "\u001b[34mon_train_batch_start               #011|  1.4092e-05     #011|100            #011|  0.0014092      #011|  0.020098       #011|\u001b[0m\n",
      "\u001b[34mtraining_step_end                  #011|  1.2367e-05     #011|100            #011|  0.0012367      #011|  0.017637       #011|\u001b[0m\n",
      "\u001b[34mon_train_epoch_end                 #011|  0.00024543     #011|1              #011|  0.00024543     #011|  0.0035001      #011|\u001b[0m\n",
      "\u001b[34mon_validation_batch_start          #011|  6.7073e-05     #011|2              #011|  0.00013415     #011|  0.0019131      #011|\u001b[0m\n",
      "\u001b[34mon_epoch_end                       #011|  1.7974e-05     #011|2              #011|  3.5949e-05     #011|  0.00051268     #011|\u001b[0m\n",
      "\u001b[34mvalidation_step_end                #011|  1.7379e-05     #011|2              #011|  3.4759e-05     #011|  0.00049571     #011|\u001b[0m\n",
      "\u001b[34mon_epoch_start                     #011|  1.7318e-05     #011|2              #011|  3.4635e-05     #011|  0.00049394     #011|\u001b[0m\n",
      "\u001b[34mon_validation_batch_end            #011|  1.6217e-05     #011|2              #011|  3.2434e-05     #011|  0.00046256     #011|\u001b[0m\n",
      "\u001b[34mon_train_end                       #011|  2.8331e-05     #011|1              #011|  2.8331e-05     #011|  0.00040404     #011|\u001b[0m\n",
      "\u001b[34mon_before_accelerator_backend_setup#011|  2.6788e-05     #011|1              #011|  2.6788e-05     #011|  0.00038204     #011|\u001b[0m\n",
      "\u001b[34mon_validation_epoch_end            #011|  2.3057e-05     #011|1              #011|  2.3057e-05     #011|  0.00032883     #011|\u001b[0m\n",
      "\u001b[34mon_validation_start                #011|  2.1185e-05     #011|1              #011|  2.1185e-05     #011|  0.00030213     #011|\u001b[0m\n",
      "\u001b[34mon_train_dataloader                #011|  1.7936e-05     #011|1              #011|  1.7936e-05     #011|  0.00025579     #011|\u001b[0m\n",
      "\u001b[34mon_validation_end                  #011|  1.5973e-05     #011|1              #011|  1.5973e-05     #011|  0.0002278      #011|\u001b[0m\n",
      "\u001b[34mon_train_epoch_start               #011|  1.544e-05      #011|1              #011|  1.544e-05      #011|  0.0002202      #011|\u001b[0m\n",
      "\u001b[34mon_val_dataloader                  #011|  1.5213e-05     #011|1              #011|  1.5213e-05     #011|  0.00021696     #011|\u001b[0m\n",
      "\u001b[34mon_validation_epoch_start          #011|  1.2228e-05     #011|1              #011|  1.2228e-05     #011|  0.00017439     #011|\n",
      "\u001b[0m\n",
      "\u001b[34mRestored states from the checkpoint file at /opt/ml/code/lr_find_temp_model.ckpt\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr: 100%|| 100/100 [00:23<00:00, 17.81it/s]#033[A/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/deprecated_api.py:152: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "  \"`Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\"\n",
      "\u001b[0m\n",
      "\u001b[34m#015                                                                          #015#033[AEpoch 0, global step 764: val_loss reached 0.19101 (best 0.19101), saving model to \"/opt/ml/model/model_subpillars_2d.ckpt\" as top 1\n",
      "\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/66 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  2%|         | 1/66 [00:00<00:09,  6.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  6%|         | 4/66 [00:00<00:07,  8.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 11%|         | 7/66 [00:00<00:05, 10.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 15%|        | 10/66 [00:00<00:04, 13.12it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 20%|        | 13/66 [00:00<00:03, 15.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 24%|       | 16/66 [00:00<00:02, 16.85it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 29%|       | 19/66 [00:00<00:02, 18.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 33%|      | 22/66 [00:00<00:02, 20.54it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 38%|      | 25/66 [00:01<00:01, 22.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 42%|     | 28/66 [00:01<00:01, 23.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 47%|     | 31/66 [00:01<00:01, 24.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 52%|    | 34/66 [00:01<00:01, 24.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 56%|    | 37/66 [00:01<00:01, 25.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 61%|    | 40/66 [00:01<00:00, 26.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 65%|   | 43/66 [00:01<00:00, 26.50it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 70%|   | 46/66 [00:01<00:00, 26.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 74%|  | 49/66 [00:01<00:00, 26.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 79%|  | 52/66 [00:02<00:00, 26.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 83%| | 55/66 [00:02<00:00, 26.63it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 88%| | 58/66 [00:02<00:00, 26.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 92%|| 61/66 [00:02<00:00, 26.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 97%|| 64/66 [00:02<00:00, 26.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m#01567it [00:02, 26.57it/s]                        #033[A#01567it [00:02, 24.81it/s]\n",
      "\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/77 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  1%|         | 1/77 [00:00<00:08,  8.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  5%|         | 4/77 [00:00<00:06, 10.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  9%|         | 7/77 [00:00<00:05, 13.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 13%|        | 10/77 [00:00<00:04, 15.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 17%|        | 13/77 [00:00<00:03, 17.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 21%|        | 16/77 [00:00<00:03, 19.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 25%|       | 19/77 [00:00<00:02, 20.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 29%|       | 22/77 [00:00<00:02, 21.75it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 32%|      | 25/77 [00:01<00:02, 22.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 36%|      | 28/77 [00:01<00:02, 23.42it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 40%|      | 31/77 [00:01<00:01, 24.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 44%|     | 34/77 [00:01<00:01, 24.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 48%|     | 37/77 [00:01<00:01, 24.72it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 52%|    | 40/77 [00:01<00:01, 25.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 56%|    | 43/77 [00:01<00:01, 25.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 60%|    | 46/77 [00:01<00:01, 24.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 64%|   | 49/77 [00:01<00:01, 25.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 68%|   | 52/77 [00:02<00:00, 25.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 71%|  | 55/77 [00:02<00:00, 25.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 75%|  | 58/77 [00:02<00:00, 25.95it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 79%|  | 61/77 [00:02<00:00, 26.12it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 83%| | 64/77 [00:02<00:00, 26.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 87%| | 67/77 [00:02<00:00, 26.81it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 91%| | 70/77 [00:02<00:00, 26.72it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 95%|| 73/77 [00:02<00:00, 26.44it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 99%|| 76/77 [00:03<00:00, 26.29it/s]#033[A#01578it [00:03, 24.99it/s]                        \u001b[0m\n",
      "\u001b[34m#015Finding best initial lr: 100%|| 100/100 [04:22<00:00,  2.63s/it]\u001b[0m\n",
      "\u001b[34m#015Finding best initial lr: 100%|| 100/100 [02:16<00:00,  1.37s/it]\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-10-25 14:30:21 Uploading - Uploading generated training model\n",
      "2021-10-25 14:31:01 Completed - Training job completed\n",
      "ProfilerReport-1635171269: IssuesFound\n",
      "Training seconds: 784\n",
      "Billable seconds: 784\n"
     ]
    }
   ],
   "source": [
    "# Fit the estimator\n",
    "\n",
    "estimator.fit(fit_arguments, job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b5ea04f4f930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert (1==2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code used for deploying and testing models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.pytorch.load_model(\n",
    "    's3://deep-mlflow-artifact/16/21a5ece6091b4ddf8b223e78159ce1c7/artifacts/pytorch_model_all',\n",
    "    map_location=torch.device('cpu')   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_data['excerpt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.deploy(\n",
    "    'testcpu-pytorch-trained-gpu',\n",
    "    's3://deep-mlflow-artifact/16/21a5ece6091b4ddf8b223e78159ce1c7/artifacts/pytorch_model_all',\n",
    "    execution_role_arn=SAGEMAKER_ROLE_ARN,\n",
    "    image_url=\"961104659532.dkr.ecr.us-east-1.amazonaws.com/mlflow-pyfunc:latest\",\n",
    "    region_name=\"us-east-1\",\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    synchronous=False,\n",
    "    archive=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
