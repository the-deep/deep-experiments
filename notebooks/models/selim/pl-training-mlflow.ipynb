{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These requirements are necessary if you launch this notebook from SageMaker instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install mlflow\\n!pip install pytorch-lightning\\n!pip install transformers\\n!pip install tqdm\\n!pip install sagemaker\\n\\n!pip install s3fs\\n!pip install smdebug'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!pip install mlflow\n",
    "!pip install pytorch-lightning\n",
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "!pip install sagemaker\n",
    "\n",
    "!pip install s3fs\n",
    "!pip install smdebug\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:49:30.843642Z",
     "start_time": "2021-06-01T14:49:30.663973Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local constants, regarding the data, MLFlow server, paths, etc..: use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deep.constants import *\n",
    "from deep.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the data you want. We advise the `pandas` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T14:57:29.882333Z",
     "start_time": "2021-06-01T14:57:28.547379Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/selim/anaconda3/envs/deepl/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = os.path.join(\n",
    "    '..', '..', '..', \"data\", \"frameworks_data\", 'data_v0.7.1'\n",
    ")\n",
    "\n",
    "train_val_df = pd.read_csv(os.path.join(DATA_PATH, 'new_columns_train_val.csv')).drop_duplicates()\n",
    "#tot_df = tot_df[tot_df]\n",
    "\n",
    "test_df = pd.read_csv(os.path.join(DATA_PATH, 'new_columns_test_v0.7.1.csv'))[['excerpt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from ast import literal_eval\\n\\ntrain_val_df['target'] = train_val_df['target'].apply(\\n    lambda x: [item for item in literal_eval(x) if 'first_level' in item]\\n)\\ntrain_val_df.to_csv(os.path.join(DATA_PATH, 'tmp_train_val.csv'))\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from ast import literal_eval\n",
    "\n",
    "train_val_df['target'] = train_val_df['target'].apply(\n",
    "    lambda x: [item for item in literal_eval(x) if 'first_level' in item]\n",
    ")\n",
    "train_val_df.to_csv(os.path.join(DATA_PATH, 'tmp_train_val.csv'))\n",
    "\n",
    "\"\"\"\n",
    "#train_val_df = pd.read_csv(os.path.join(DATA_PATH, 'tmp_train_val.csv'))\n",
    "#train_val_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "len(list(set(flatten(train_val_df['target'].apply(literal_eval)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['excerpt', 'entry_id', 'target']\n",
    "train_val_df = train_val_df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tot_df = pd.concat(\\n    [\\n        tot_df,\\n        pd.read_csv(os.path.join(DATA_PATH, 'augmented_subpillars.csv'))\\n    ]\\n)\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"tot_df = pd.concat(\n",
    "    [\n",
    "        tot_df,\n",
    "        pd.read_csv(os.path.join(DATA_PATH, 'augmented_subpillars.csv'))\n",
    "    ]\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['excerpt', 'entry_id', 'target'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(422160, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T15:42:32.024647Z",
     "start_time": "2021-05-27T15:42:31.984694Z"
    }
   },
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:29:20.899415Z",
     "start_time": "2021-06-09T08:29:19.327852Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session(default_bucket=DEV_BUCKET.name)\n",
    "role = SAGEMAKER_ROLE\n",
    "role_arn = SAGEMAKER_ROLE_ARN\n",
    "tracking_uri = MLFLOW_SERVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to upload data to an S3 bucket. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLFLOW_SERVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:43.201910Z",
     "start_time": "2021-06-09T08:29:28.837139Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = False  # To make the computations faster, sample = True.\n",
    "\n",
    "if sample:\n",
    "    train_val_df = train_val_df.sample(n=20_000)\n",
    "    \n",
    "job_name = f\"pytorch-{formatted_time()}-all-models\"  # change it as you prefer\n",
    "input_path = DEV_BUCKET / 'training' / 'input_data' / job_name  # Do not change this\n",
    "\n",
    "train_path = str(input_path / 'train.pickle')\n",
    "val_path = str(input_path / 'val.pickle')\n",
    "\n",
    "train_val_df.to_pickle(train_path, protocol=4)  # protocol 4 is necessary, since SageMaker uses python 3.6\n",
    "test_df.to_pickle(val_path, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:43.284096Z",
     "start_time": "2021-06-09T08:31:43.206457Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPU instances\n",
    "\n",
    "instances = [\n",
    "    'ml.p2.xlarge',\n",
    "    'ml.p3.2xlarge'\n",
    "]\n",
    "\n",
    "# CPU instances\n",
    "instances = [\n",
    "    'ml.c4.2xlarge',\n",
    "    'ml.c4.4xlarge',\n",
    "    'ml.c5n.2xlarge'\n",
    "]\n",
    "\n",
    "# https://aws.amazon.com/sagemaker/pricing/instance-types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters are passed as command line arguments to the training script. \n",
    "\n",
    "You can add/change them as you like. It's important to keep the `tracking_uri` and the `experiment_name` which are used by MLFlow.\n",
    "\n",
    "The class `PyTorch` is part of the `SageMaker` python API. The parameters are important and you should probably not change most of them. The ones you may want to change are:\n",
    "\n",
    "- `instance_type`, specify the instance you want\n",
    "- `source_dir`, specify your script directory. Try to use global variable as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:43.458886Z",
     "start_time": "2021-06-09T08:31:43.304626Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "instance_type = \"ml.p3.2xlarge\"\n",
    "\n",
    "hyperparameters = {\n",
    "    \"tracking_uri\": MLFLOW_SERVER,\n",
    "    \"experiment_name\": \"pl-new-architecture\",\n",
    "    \"max_len\": 128,\n",
    "    \"epochs\": 2,\n",
    "    \"model_name\": \"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\n",
    "    \"tokenizer_name\": \"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"output_length\": 384,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"instance_type\": instance_type,\n",
    "    \"f_beta\": 0.7,\n",
    "    \"nb_repetitions\": 1,\n",
    "    \"run_name\": \"all_tags_final\",\n",
    "    \"train_batch_size\": 32,\n",
    "    \"val_batch_size\": 64,\n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train_mlflow.py\",\n",
    "    source_dir=str(\n",
    "        \"../../../scripts/training/selim/multiclass-lightning/MultitaskAllInOne\"\n",
    "    ),\n",
    "    output_path=str(DEV_BUCKET / \"models/\"),\n",
    "    code_location=str(input_path),\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    framework_version=\"1.8\",\n",
    "    py_version=\"py3\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    job_name=job_name,\n",
    "    #     train_instance_count=2,\n",
    "    #     train_instance_type=\"ml.c4.xlarge\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:43.482969Z",
     "start_time": "2021-06-09T08:31:43.459884Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit_arguments = {\n",
    "    'train': str(input_path),\n",
    "    'test': str(input_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T08:31:45.995868Z",
     "start_time": "2021-06-09T08:31:43.484212Z"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-07 11:39:20 Starting - Starting the training job...ProfilerReport-1657193958: InProgress\n",
      "...\n",
      "2022-07-07 11:40:17 Starting - Preparing the instances for training......\n",
      "2022-07-07 11:41:21 Downloading - Downloading input data...\n",
      "2022-07-07 11:41:58 Training - Downloading the training image........................\n",
      "2022-07-07 11:46:03 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-07-07 11:46:06,328 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-07-07 11:46:06,351 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-07-07 11:46:06,358 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-07-07 11:46:06,808 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.8.2\n",
      "  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting tensorflow==2.4.0\n",
      "  Downloading tensorflow-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl (394.7 MB)\u001b[0m\n",
      "\u001b[34mCollecting pytorch-lightning==1.3.8\n",
      "  Downloading pytorch_lightning-1.3.8-py3-none-any.whl (813 kB)\u001b[0m\n",
      "\u001b[34mCollecting torchmetrics==0.4.1\n",
      "  Downloading torchmetrics-0.4.1-py3-none-any.whl (234 kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm==4.41.1\n",
      "  Downloading tqdm-4.41.1-py2.py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34mCollecting mlflow==1.23.0\n",
      "  Downloading mlflow-1.23.0-py3-none-any.whl (15.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn==0.22.2.post1\u001b[0m\n",
      "\u001b[34m  Downloading scikit_learn-0.22.2.post1-cp36-cp36m-manylinux1_x86_64.whl (7.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker==2.49.1\n",
      "  Downloading sagemaker-2.49.1.tar.gz (421 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3fs==2021.07.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 9)) (2021.7.0)\u001b[0m\n",
      "\u001b[34mCollecting smdebug==1.0.11\n",
      "  Downloading smdebug-1.0.11-py2.py3-none-any.whl (269 kB)\u001b[0m\n",
      "\u001b[34mCollecting scikit-multilearn==0.2.0\n",
      "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle==2.0.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 12)) (2.0.0)\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.6.5\n",
      "  Downloading deepspeed-0.6.5.tar.gz (567 kB)\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece==0.1.96\n",
      "  Downloading sentencepiece-0.1.96-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (2.26.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub==0.0.12\n",
      "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (4.8.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (3.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.8.2->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.6.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.4.0->-r requirements.txt (line 2)) (0.2.0)\u001b[0m\n",
      "\u001b[34mCollecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.4.0->-r requirements.txt (line 2)) (0.36.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.6/site-packages (from tensorflow==2.4.0->-r requirements.txt (line 2)) (3.19.1)\u001b[0m\n",
      "\u001b[34mCollecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\u001b[0m\n",
      "\u001b[34mCollecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\u001b[0m\n",
      "\u001b[34mCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\u001b[0m\n",
      "\u001b[34mCollecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting h5py~=2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\u001b[0m\n",
      "\u001b[34mCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp36-cp36m-manylinux2014_x86_64.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy>=1.17\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting pyDeprecate==0.3.0\n",
      "  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow!=8.3.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 3)) (8.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 3)) (0.18.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 3)) (2021.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 3)) (1.8.1)\u001b[0m\n",
      "\u001b[34mCollecting prometheus-flask-exporter\n",
      "  Downloading prometheus_flask_exporter-0.20.2-py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting sqlparse>=0.3.1\n",
      "  Downloading sqlparse-0.4.2-py3-none-any.whl (42 kB)\u001b[0m\n",
      "\u001b[34mCollecting alembic\n",
      "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\u001b[0m\n",
      "\u001b[34mCollecting docker>=4.0.0\n",
      "  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from mlflow==1.23.0->-r requirements.txt (line 6)) (1.5.4)\u001b[0m\n",
      "\u001b[34mCollecting sqlalchemy\n",
      "  Downloading SQLAlchemy-1.4.39-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Flask in /opt/conda/lib/python3.6/site-packages (from mlflow==1.23.0->-r requirements.txt (line 6)) (2.0.2)\u001b[0m\n",
      "\u001b[34mCollecting databricks-cli>=0.8.7\n",
      "  Downloading databricks-cli-0.17.0.tar.gz (81 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from mlflow==1.23.0->-r requirements.txt (line 6)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: entrypoints in /opt/conda/lib/python3.6/site-packages (from mlflow==1.23.0->-r requirements.txt (line 6)) (0.3)\u001b[0m\n",
      "\u001b[34mCollecting gitpython>=2.1.0\n",
      "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from mlflow==1.23.0->-r requirements.txt (line 6)) (2021.3)\u001b[0m\n",
      "\u001b[34mCollecting gunicorn\n",
      "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\u001b[0m\n",
      "\u001b[34mCollecting querystring-parser\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.6/site-packages (from mlflow==1.23.0->-r requirements.txt (line 6)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.22.2.post1->-r requirements.txt (line 7)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs in /opt/conda/lib/python3.6/site-packages (from sagemaker==2.49.1->-r requirements.txt (line 8)) (21.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3>=1.16.32 in /opt/conda/lib/python3.6/site-packages (from sagemaker==2.49.1->-r requirements.txt (line 8)) (1.20.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.6/site-packages (from sagemaker==2.49.1->-r requirements.txt (line 8)) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker==2.49.1->-r requirements.txt (line 8)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.6/site-packages (from sagemaker==2.49.1->-r requirements.txt (line 8)) (0.2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiobotocore>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from s3fs==2021.07.0->-r requirements.txt (line 9)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyinstrument>=3.1.3 in /opt/conda/lib/python3.6/site-packages (from smdebug==1.0.11->-r requirements.txt (line 10)) (3.4.2)\u001b[0m\n",
      "\u001b[34mCollecting hjson\n",
      "  Downloading hjson-3.0.2-py3-none-any.whl (54 kB)\u001b[0m\n",
      "\u001b[34mCollecting ninja\n",
      "  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.6/site-packages (from deepspeed==0.6.5->-r requirements.txt (line 13)) (5.8.0)\u001b[0m\n",
      "\u001b[34mCollecting py-cpuinfo\n",
      "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp>=3.3.1 in /opt/conda/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 9)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.23.25,>=1.23.24 in /opt/conda/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 9)) (1.23.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aioitertools>=0.5.1 in /opt/conda/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 9)) (0.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker==2.49.1->-r requirements.txt (line 8)) (0.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker==2.49.1->-r requirements.txt (line 8)) (0.10.0)\u001b[0m\n",
      "\u001b[34mCollecting pyjwt>=1.7.0\n",
      "  Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.1.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.6/site-packages (from databricks-cli>=0.8.7->mlflow==1.23.0->-r requirements.txt (line 6)) (0.8.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.6/site-packages (from docker>=4.0.0->mlflow==1.23.0->-r requirements.txt (line 6)) (1.2.3)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.8.2->-r requirements.txt (line 1)) (3.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.8.2->-r requirements.txt (line 1)) (3.0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyinstrument-cext>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from pyinstrument>=3.1.3->smdebug==1.0.11->-r requirements.txt (line 10)) (0.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.8.2->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.8.2->-r requirements.txt (line 1)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.8.2->-r requirements.txt (line 1)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.8.2->-r requirements.txt (line 1)) (2021.5.30)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow==2.4.0->-r requirements.txt (line 2)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard~=2.4->tensorflow==2.4.0->-r requirements.txt (line 2)) (58.0.4)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.9.0-py2.py3-none-any.whl (167 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.6/site-packages (from alembic->mlflow==1.23.0->-r requirements.txt (line 6)) (5.4.0)\u001b[0m\n",
      "\u001b[34mCollecting Mako\n",
      "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.6/site-packages (from sqlalchemy->mlflow==1.23.0->-r requirements.txt (line 6)) (1.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.6/site-packages (from Flask->mlflow==1.23.0->-r requirements.txt (line 6)) (3.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.6/site-packages (from Flask->mlflow==1.23.0->-r requirements.txt (line 6)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->mlflow==1.23.0->-r requirements.txt (line 6)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker==2.49.1->-r requirements.txt (line 8)) (1.6.6.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker==2.49.1->-r requirements.txt (line 8)) (0.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker==2.49.1->-r requirements.txt (line 8)) (0.70.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker==2.49.1->-r requirements.txt (line 8)) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.6/site-packages (from prometheus-flask-exporter->mlflow==1.23.0->-r requirements.txt (line 6)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna-ssl>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 9)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 9)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 9)) (0.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 9)) (1.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 9)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 9)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs==2021.07.0->-r requirements.txt (line 9)) (5.2.0)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->-r requirements.txt (line 2)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.6/site-packages (from Jinja2>=3.0->Flask->mlflow==1.23.0->-r requirements.txt (line 6)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->-r requirements.txt (line 2)) (0.4.8)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker, deepspeed, databricks-cli, termcolor, wrapt, py-cpuinfo, sacremoses\n",
      "  Building wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-2.49.1-py2.py3-none-any.whl size=591938 sha256=ba332c0b9a4a4113f0ee17cead40ff64f2bb2c1cd050909ace8aa84a1bff70e8\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/af/ea/8ff5943a87155df5b184e54474fbf2b59b75e5c172854643c6\n",
      "  Building wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for deepspeed (setup.py): finished with status 'done'\n",
      "  Created wheel for deepspeed: filename=deepspeed-0.6.5-py3-none-any.whl size=563645 sha256=a34f1ba51c8b2a655e490a91a22116a87079387c3181e3da6561f7b3f687fb52\n",
      "  Stored in directory: /root/.cache/pip/wheels/7b/c7/f1/3bb3800985b70bd891e6ab5d4a9594eab2c792656bc6721df0\n",
      "  Building wheel for databricks-cli (setup.py): started\n",
      "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
      "  Created wheel for databricks-cli: filename=databricks_cli-0.17.0-py3-none-any.whl size=141959 sha256=9f8eeb566ac0d2fe240c98c3f4b6ff8a84d53a8c1a1afb91058eedc7135f6c8d\n",
      "  Stored in directory: /root/.cache/pip/wheels/54/50/3d/a8dbe746d2ab14f930d52509e2d881cbcc6c78db88207e25a2\n",
      "  Building wheel for termcolor (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=c1d72da4184ff4e137a3a020d0c95d700678c82170b024cab67a5a6bf8241ab3\n",
      "  Stored in directory: /root/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for wrapt (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=69767 sha256=6fa3afce9028f74b75ba7ccf96e75b8efe1a229065c89dbbd48ad3b8f780cadb\n",
      "  Stored in directory: /root/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "  Building wheel for py-cpuinfo (setup.py): started\n",
      "  Building wheel for py-cpuinfo (setup.py): finished with status 'done'\n",
      "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=8bf888d8c50cede97d0d6f84c168bc27fa1f0a6219be5a992bfdad23760162f1\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/e1/d9/9b782b170e5272d6500cee4d29dd6c724598b22dc399d81d01\n",
      "  Building wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=a17074cce8b924c8b4c1d7e5f06ac49a3c9bc1de38ce1dde6dbc5ff31ec2f4d2\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/64/31/e9900a234b23fb3e9dc565d6114a9d6ff84a72dbdd356502b4\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker deepspeed databricks-cli termcolor wrapt py-cpuinfo sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: typing-extensions, six, pyasn1-modules, oauthlib, cachetools, smmap, requests-oauthlib, numpy, google-auth, wrapt, tqdm, tensorboard-plugin-wit, tensorboard-data-server, sqlalchemy, regex, pyjwt, markdown, Mako, grpcio, google-auth-oauthlib, gitdb, absl-py, torchmetrics, tokenizers, termcolor, tensorflow-estimator, tensorboard, sqlparse, sacremoses, querystring-parser, pyDeprecate, py-cpuinfo, prometheus-flask-exporter, opt-einsum, ninja, keras-preprocessing, huggingface-hub, hjson, h5py, gunicorn, gitpython, gast, flatbuffers, docker, databricks-cli, astunparse, alembic, transformers, tensorflow, smdebug, sentencepiece, scikit-multilearn, scikit-learn, sagemaker, pytorch-lightning, mlflow, deepspeed\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.1\n",
      "    Uninstalling numpy-1.19.1:\n",
      "      Successfully uninstalled numpy-1.19.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.13.3\n",
      "    Uninstalling wrapt-1.13.3:\n",
      "      Successfully uninstalled wrapt-1.13.3\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.61.2\n",
      "    Uninstalling tqdm-4.61.2:\n",
      "      Successfully uninstalled tqdm-4.61.2\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.8.0\n",
      "    Uninstalling h5py-2.8.0:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled h5py-2.8.0\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: smdebug\n",
      "    Found existing installation: smdebug 1.0.9\n",
      "    Uninstalling smdebug-1.0.9:\n",
      "      Successfully uninstalled smdebug-1.0.9\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.72.0\u001b[0m\n",
      "\u001b[34m    Uninstalling sagemaker-2.72.0:\n",
      "      Successfully uninstalled sagemaker-2.72.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed Mako-1.1.6 absl-py-0.15.0 alembic-1.7.7 astunparse-1.6.3 cachetools-4.2.4 databricks-cli-0.17.0 deepspeed-0.6.5 docker-5.0.3 flatbuffers-1.12 gast-0.3.3 gitdb-4.0.9 gitpython-3.1.18 google-auth-2.9.0 google-auth-oauthlib-0.4.6 grpcio-1.32.0 gunicorn-20.1.0 h5py-2.10.0 hjson-3.0.2 huggingface-hub-0.0.12 keras-preprocessing-1.1.2 markdown-3.3.7 mlflow-1.23.0 ninja-1.10.2.3 numpy-1.19.5 oauthlib-3.2.0 opt-einsum-3.3.0 prometheus-flask-exporter-0.20.2 py-cpuinfo-8.0.0 pyDeprecate-0.3.0 pyasn1-modules-0.2.8 pyjwt-2.4.0 pytorch-lightning-1.3.8 querystring-parser-1.2.4 regex-2022.6.2 requests-oauthlib-1.3.1 sacremoses-0.0.53 sagemaker-2.49.1 scikit-learn-0.22.2.post1 scikit-multilearn-0.2.0 sentencepiece-0.1.96 six-1.15.0 smdebug-1.0.11 smmap-5.0.0 sqlalchemy-1.4.39 sqlparse-0.4.2 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.4.0 tensorflow-estimator-2.4.0 termcolor-1.1.0 tokenizers-0.10.3 torchmetrics-0.4.1 tqdm-4.41.1 transformers-4.8.2 typing-extensions-3.7.4.3 wrapt-1.12.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-07-07 11:47:13,119 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 2,\n",
      "        \"experiment_name\": \"pl-new-architecture\",\n",
      "        \"f_beta\": 0.7,\n",
      "        \"instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"learning_rate\": 1e-05,\n",
      "        \"max_len\": 128,\n",
      "        \"model_name\": \"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\n",
      "        \"nb_repetitions\": 1,\n",
      "        \"output_length\": 384,\n",
      "        \"run_name\": \"all_tags_final\",\n",
      "        \"tokenizer_name\": \"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\n",
      "        \"tracking_uri\": \"http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\",\n",
      "        \"train_batch_size\": 32,\n",
      "        \"val_batch_size\": 64,\n",
      "        \"weight_decay\": 0.03\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-2022-07-07-13-38-48-334-all-models\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-deep-experiments-dev/training/input_data/pytorch-2022-07-07-13-38-48-334-all-models/pytorch-2022-07-07-13-38-48-334-all-models/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_mlflow\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_mlflow.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":2,\"experiment_name\":\"pl-new-architecture\",\"f_beta\":0.7,\"instance_type\":\"ml.p3.2xlarge\",\"learning_rate\":1e-05,\"max_len\":128,\"model_name\":\"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\"nb_repetitions\":1,\"output_length\":384,\"run_name\":\"all_tags_final\",\"tokenizer_name\":\"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\"tracking_uri\":\"http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\",\"train_batch_size\":32,\"val_batch_size\":64,\"weight_decay\":0.03}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_mlflow.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_mlflow\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-deep-experiments-dev/training/input_data/pytorch-2022-07-07-13-38-48-334-all-models/pytorch-2022-07-07-13-38-48-334-all-models/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":2,\"experiment_name\":\"pl-new-architecture\",\"f_beta\":0.7,\"instance_type\":\"ml.p3.2xlarge\",\"learning_rate\":1e-05,\"max_len\":128,\"model_name\":\"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\"nb_repetitions\":1,\"output_length\":384,\"run_name\":\"all_tags_final\",\"tokenizer_name\":\"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\"tracking_uri\":\"http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\",\"train_batch_size\":32,\"val_batch_size\":64,\"weight_decay\":0.03},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-2022-07-07-13-38-48-334-all-models\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-deep-experiments-dev/training/input_data/pytorch-2022-07-07-13-38-48-334-all-models/pytorch-2022-07-07-13-38-48-334-all-models/source/sourcedir.tar.gz\",\"module_name\":\"train_mlflow\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_mlflow.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"2\",\"--experiment_name\",\"pl-new-architecture\",\"--f_beta\",\"0.7\",\"--instance_type\",\"ml.p3.2xlarge\",\"--learning_rate\",\"1e-05\",\"--max_len\",\"128\",\"--model_name\",\"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\"--nb_repetitions\",\"1\",\"--output_length\",\"384\",\"--run_name\",\"all_tags_final\",\"--tokenizer_name\",\"nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\",\"--tracking_uri\",\"http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\",\"--train_batch_size\",\"32\",\"--val_batch_size\",\"64\",\"--weight_decay\",\"0.03\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_EXPERIMENT_NAME=pl-new-architecture\u001b[0m\n",
      "\u001b[34mSM_HP_F_BETA=0.7\u001b[0m\n",
      "\u001b[34mSM_HP_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=1e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_LEN=128\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\u001b[0m\n",
      "\u001b[34mSM_HP_NB_REPETITIONS=1\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_LENGTH=384\u001b[0m\n",
      "\u001b[34mSM_HP_RUN_NAME=all_tags_final\u001b[0m\n",
      "\u001b[34mSM_HP_TOKENIZER_NAME=nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large\u001b[0m\n",
      "\u001b[34mSM_HP_TRACKING_URI=http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_VAL_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.03\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train_mlflow.py --epochs 2 --experiment_name pl-new-architecture --f_beta 0.7 --instance_type ml.p3.2xlarge --learning_rate 1e-05 --max_len 128 --model_name nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large --nb_repetitions 1 --output_length 384 --run_name all_tags_final --tokenizer_name nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large --tracking_uri http://mlflow-deep-387470f3-1883319727.us-east-1.elb.amazonaws.com/ --train_batch_size 32 --val_batch_size 64 --weight_decay 0.03\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:47:16,641] [WARNING] [partition_parameters.py:61:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.\u001b[0m\n",
      "\u001b[34m#015Validation sanity check: 0it [00:00, ?it/s]#015Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s][2022-07-07 11:48:18.418 algo-1:134 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:18.456 algo-1:134 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:18.457 algo-1:134 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:18.457 algo-1:134 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:18.458 algo-1:134 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:18.458 algo-1:134 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.263 algo-1:134 INFO hook.py:594] name:model.common_backbone.embeddings.word_embeddings.weight count_params:96000768\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.263 algo-1:134 INFO hook.py:594] name:model.common_backbone.embeddings.position_embeddings.weight count_params:197376\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.263 algo-1:134 INFO hook.py:594] name:model.common_backbone.embeddings.token_type_embeddings.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.264 algo-1:134 INFO hook.py:594] name:model.common_backbone.embeddings.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.264 algo-1:134 INFO hook.py:594] name:model.common_backbone.embeddings.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.264 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.0.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.264 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.0.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.264 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.0.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.264 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.0.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.265 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.0.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.265 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.0.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.265 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.0.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.265 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.0.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.265 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.0.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.265 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.0.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.265 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.0.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.265 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.0.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.265 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.0.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.265 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.0.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.265 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.0.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.266 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.0.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.266 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.1.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.266 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.1.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.266 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.1.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.266 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.1.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.266 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.1.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.266 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.1.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.266 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.1.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.267 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.1.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.267 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.1.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.267 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.1.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.267 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.1.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.267 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.1.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.267 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.1.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.267 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.1.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.267 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.1.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.267 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.1.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.267 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.2.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.267 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.2.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.268 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.2.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.268 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.2.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.268 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.2.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.268 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.2.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.268 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.2.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.268 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.2.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.268 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.2.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.268 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.2.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.268 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.2.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.268 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.2.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.269 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.2.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.269 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.2.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.269 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.2.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.269 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.2.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.269 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.3.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.269 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.3.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.269 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.3.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.269 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.3.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.270 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.3.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.270 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.3.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.270 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.3.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.270 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.3.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.270 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.3.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.270 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.3.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.270 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.3.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.270 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.3.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.270 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.3.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.270 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.3.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.271 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.3.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.271 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.3.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.271 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.4.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.271 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.4.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.271 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.4.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.271 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.4.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.271 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.4.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.271 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.4.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.271 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.4.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.271 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.4.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.271 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.4.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.271 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.4.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.272 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.4.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.272 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.4.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.272 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.4.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.272 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.4.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.272 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.4.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.272 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.4.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.272 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.5.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.272 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.5.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.272 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.5.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.272 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.5.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.273 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.5.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.273 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.5.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.273 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.5.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.273 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.5.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.273 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.5.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.273 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.5.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.273 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.5.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.273 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.5.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.273 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.5.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.273 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.5.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.273 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.5.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.274 algo-1:134 INFO hook.py:594] name:model.common_backbone.encoder.layer.5.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.274 algo-1:134 INFO hook.py:594] name:model.common_backbone.pooler.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.274 algo-1:134 INFO hook.py:594] name:model.common_backbone.pooler.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.274 algo-1:134 INFO hook.py:594] name:model.last_layer.0.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.274 algo-1:134 INFO hook.py:594] name:model.last_layer.0.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.274 algo-1:134 INFO hook.py:594] name:model.last_layer.0.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.274 algo-1:134 INFO hook.py:594] name:model.last_layer.0.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.274 algo-1:134 INFO hook.py:594] name:model.last_layer.0.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.274 algo-1:134 INFO hook.py:594] name:model.last_layer.0.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.274 algo-1:134 INFO hook.py:594] name:model.last_layer.0.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.274 algo-1:134 INFO hook.py:594] name:model.last_layer.0.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.275 algo-1:134 INFO hook.py:594] name:model.last_layer.0.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.275 algo-1:134 INFO hook.py:594] name:model.last_layer.0.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.275 algo-1:134 INFO hook.py:594] name:model.last_layer.0.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.275 algo-1:134 INFO hook.py:594] name:model.last_layer.0.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.275 algo-1:134 INFO hook.py:594] name:model.last_layer.0.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.275 algo-1:134 INFO hook.py:594] name:model.last_layer.0.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.275 algo-1:134 INFO hook.py:594] name:model.last_layer.0.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.275 algo-1:134 INFO hook.py:594] name:model.last_layer.0.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.275 algo-1:134 INFO hook.py:594] name:model.last_layer.1.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.275 algo-1:134 INFO hook.py:594] name:model.last_layer.1.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.275 algo-1:134 INFO hook.py:594] name:model.last_layer.1.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.276 algo-1:134 INFO hook.py:594] name:model.last_layer.1.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.276 algo-1:134 INFO hook.py:594] name:model.last_layer.1.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.276 algo-1:134 INFO hook.py:594] name:model.last_layer.1.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.276 algo-1:134 INFO hook.py:594] name:model.last_layer.1.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.276 algo-1:134 INFO hook.py:594] name:model.last_layer.1.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.276 algo-1:134 INFO hook.py:594] name:model.last_layer.1.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.276 algo-1:134 INFO hook.py:594] name:model.last_layer.1.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.276 algo-1:134 INFO hook.py:594] name:model.last_layer.1.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.277 algo-1:134 INFO hook.py:594] name:model.last_layer.1.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.277 algo-1:134 INFO hook.py:594] name:model.last_layer.1.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.277 algo-1:134 INFO hook.py:594] name:model.last_layer.1.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.277 algo-1:134 INFO hook.py:594] name:model.last_layer.1.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.277 algo-1:134 INFO hook.py:594] name:model.last_layer.1.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.277 algo-1:134 INFO hook.py:594] name:model.last_layer.2.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.278 algo-1:134 INFO hook.py:594] name:model.last_layer.2.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.278 algo-1:134 INFO hook.py:594] name:model.last_layer.2.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.278 algo-1:134 INFO hook.py:594] name:model.last_layer.2.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.278 algo-1:134 INFO hook.py:594] name:model.last_layer.2.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.278 algo-1:134 INFO hook.py:594] name:model.last_layer.2.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.278 algo-1:134 INFO hook.py:594] name:model.last_layer.2.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.278 algo-1:134 INFO hook.py:594] name:model.last_layer.2.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.278 algo-1:134 INFO hook.py:594] name:model.last_layer.2.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.279 algo-1:134 INFO hook.py:594] name:model.last_layer.2.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.279 algo-1:134 INFO hook.py:594] name:model.last_layer.2.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.279 algo-1:134 INFO hook.py:594] name:model.last_layer.2.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.279 algo-1:134 INFO hook.py:594] name:model.last_layer.2.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.279 algo-1:134 INFO hook.py:594] name:model.last_layer.2.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.279 algo-1:134 INFO hook.py:594] name:model.last_layer.2.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.279 algo-1:134 INFO hook.py:594] name:model.last_layer.2.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.279 algo-1:134 INFO hook.py:594] name:model.last_layer.3.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.279 algo-1:134 INFO hook.py:594] name:model.last_layer.3.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.279 algo-1:134 INFO hook.py:594] name:model.last_layer.3.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.279 algo-1:134 INFO hook.py:594] name:model.last_layer.3.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.279 algo-1:134 INFO hook.py:594] name:model.last_layer.3.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.279 algo-1:134 INFO hook.py:594] name:model.last_layer.3.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.280 algo-1:134 INFO hook.py:594] name:model.last_layer.3.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.280 algo-1:134 INFO hook.py:594] name:model.last_layer.3.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.280 algo-1:134 INFO hook.py:594] name:model.last_layer.3.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.280 algo-1:134 INFO hook.py:594] name:model.last_layer.3.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.280 algo-1:134 INFO hook.py:594] name:model.last_layer.3.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.280 algo-1:134 INFO hook.py:594] name:model.last_layer.3.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.280 algo-1:134 INFO hook.py:594] name:model.last_layer.3.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.280 algo-1:134 INFO hook.py:594] name:model.last_layer.3.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.280 algo-1:134 INFO hook.py:594] name:model.last_layer.3.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.281 algo-1:134 INFO hook.py:594] name:model.last_layer.3.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.281 algo-1:134 INFO hook.py:594] name:model.last_layer.4.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.281 algo-1:134 INFO hook.py:594] name:model.last_layer.4.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.281 algo-1:134 INFO hook.py:594] name:model.last_layer.4.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.281 algo-1:134 INFO hook.py:594] name:model.last_layer.4.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.281 algo-1:134 INFO hook.py:594] name:model.last_layer.4.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.281 algo-1:134 INFO hook.py:594] name:model.last_layer.4.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.281 algo-1:134 INFO hook.py:594] name:model.last_layer.4.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.281 algo-1:134 INFO hook.py:594] name:model.last_layer.4.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.281 algo-1:134 INFO hook.py:594] name:model.last_layer.4.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.281 algo-1:134 INFO hook.py:594] name:model.last_layer.4.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.281 algo-1:134 INFO hook.py:594] name:model.last_layer.4.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.282 algo-1:134 INFO hook.py:594] name:model.last_layer.4.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.282 algo-1:134 INFO hook.py:594] name:model.last_layer.4.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.282 algo-1:134 INFO hook.py:594] name:model.last_layer.4.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.282 algo-1:134 INFO hook.py:594] name:model.last_layer.4.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.282 algo-1:134 INFO hook.py:594] name:model.last_layer.4.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.282 algo-1:134 INFO hook.py:594] name:model.last_layer.5.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.282 algo-1:134 INFO hook.py:594] name:model.last_layer.5.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.282 algo-1:134 INFO hook.py:594] name:model.last_layer.5.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.282 algo-1:134 INFO hook.py:594] name:model.last_layer.5.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.282 algo-1:134 INFO hook.py:594] name:model.last_layer.5.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.282 algo-1:134 INFO hook.py:594] name:model.last_layer.5.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.282 algo-1:134 INFO hook.py:594] name:model.last_layer.5.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.282 algo-1:134 INFO hook.py:594] name:model.last_layer.5.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.282 algo-1:134 INFO hook.py:594] name:model.last_layer.5.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.283 algo-1:134 INFO hook.py:594] name:model.last_layer.5.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.283 algo-1:134 INFO hook.py:594] name:model.last_layer.5.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.283 algo-1:134 INFO hook.py:594] name:model.last_layer.5.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.283 algo-1:134 INFO hook.py:594] name:model.last_layer.5.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.283 algo-1:134 INFO hook.py:594] name:model.last_layer.5.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.283 algo-1:134 INFO hook.py:594] name:model.last_layer.5.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.283 algo-1:134 INFO hook.py:594] name:model.last_layer.5.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.283 algo-1:134 INFO hook.py:594] name:model.last_layer.6.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.283 algo-1:134 INFO hook.py:594] name:model.last_layer.6.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.283 algo-1:134 INFO hook.py:594] name:model.last_layer.6.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.283 algo-1:134 INFO hook.py:594] name:model.last_layer.6.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.283 algo-1:134 INFO hook.py:594] name:model.last_layer.6.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.283 algo-1:134 INFO hook.py:594] name:model.last_layer.6.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.284 algo-1:134 INFO hook.py:594] name:model.last_layer.6.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.284 algo-1:134 INFO hook.py:594] name:model.last_layer.6.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.284 algo-1:134 INFO hook.py:594] name:model.last_layer.6.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.284 algo-1:134 INFO hook.py:594] name:model.last_layer.6.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.284 algo-1:134 INFO hook.py:594] name:model.last_layer.6.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.284 algo-1:134 INFO hook.py:594] name:model.last_layer.6.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.284 algo-1:134 INFO hook.py:594] name:model.last_layer.6.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.284 algo-1:134 INFO hook.py:594] name:model.last_layer.6.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.284 algo-1:134 INFO hook.py:594] name:model.last_layer.6.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.285 algo-1:134 INFO hook.py:594] name:model.last_layer.6.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.285 algo-1:134 INFO hook.py:594] name:model.last_layer.7.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.285 algo-1:134 INFO hook.py:594] name:model.last_layer.7.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.285 algo-1:134 INFO hook.py:594] name:model.last_layer.7.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.285 algo-1:134 INFO hook.py:594] name:model.last_layer.7.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.285 algo-1:134 INFO hook.py:594] name:model.last_layer.7.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.285 algo-1:134 INFO hook.py:594] name:model.last_layer.7.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.286 algo-1:134 INFO hook.py:594] name:model.last_layer.7.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.286 algo-1:134 INFO hook.py:594] name:model.last_layer.7.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.286 algo-1:134 INFO hook.py:594] name:model.last_layer.7.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.286 algo-1:134 INFO hook.py:594] name:model.last_layer.7.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.286 algo-1:134 INFO hook.py:594] name:model.last_layer.7.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.286 algo-1:134 INFO hook.py:594] name:model.last_layer.7.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.286 algo-1:134 INFO hook.py:594] name:model.last_layer.7.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.286 algo-1:134 INFO hook.py:594] name:model.last_layer.7.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.286 algo-1:134 INFO hook.py:594] name:model.last_layer.7.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.286 algo-1:134 INFO hook.py:594] name:model.last_layer.7.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.286 algo-1:134 INFO hook.py:594] name:model.last_layer.8.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.286 algo-1:134 INFO hook.py:594] name:model.last_layer.8.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.286 algo-1:134 INFO hook.py:594] name:model.last_layer.8.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.286 algo-1:134 INFO hook.py:594] name:model.last_layer.8.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.286 algo-1:134 INFO hook.py:594] name:model.last_layer.8.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.286 algo-1:134 INFO hook.py:594] name:model.last_layer.8.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.287 algo-1:134 INFO hook.py:594] name:model.last_layer.8.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.287 algo-1:134 INFO hook.py:594] name:model.last_layer.8.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.287 algo-1:134 INFO hook.py:594] name:model.last_layer.8.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.287 algo-1:134 INFO hook.py:594] name:model.last_layer.8.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.287 algo-1:134 INFO hook.py:594] name:model.last_layer.8.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.287 algo-1:134 INFO hook.py:594] name:model.last_layer.8.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.287 algo-1:134 INFO hook.py:594] name:model.last_layer.8.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.287 algo-1:134 INFO hook.py:594] name:model.last_layer.8.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.287 algo-1:134 INFO hook.py:594] name:model.last_layer.8.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.287 algo-1:134 INFO hook.py:594] name:model.last_layer.8.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.287 algo-1:134 INFO hook.py:594] name:model.last_layer.9.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.287 algo-1:134 INFO hook.py:594] name:model.last_layer.9.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.287 algo-1:134 INFO hook.py:594] name:model.last_layer.9.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.287 algo-1:134 INFO hook.py:594] name:model.last_layer.9.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.287 algo-1:134 INFO hook.py:594] name:model.last_layer.9.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.287 algo-1:134 INFO hook.py:594] name:model.last_layer.9.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.287 algo-1:134 INFO hook.py:594] name:model.last_layer.9.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.288 algo-1:134 INFO hook.py:594] name:model.last_layer.9.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.288 algo-1:134 INFO hook.py:594] name:model.last_layer.9.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.288 algo-1:134 INFO hook.py:594] name:model.last_layer.9.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.288 algo-1:134 INFO hook.py:594] name:model.last_layer.9.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.288 algo-1:134 INFO hook.py:594] name:model.last_layer.9.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.288 algo-1:134 INFO hook.py:594] name:model.last_layer.9.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.288 algo-1:134 INFO hook.py:594] name:model.last_layer.9.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.288 algo-1:134 INFO hook.py:594] name:model.last_layer.9.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.288 algo-1:134 INFO hook.py:594] name:model.last_layer.9.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.288 algo-1:134 INFO hook.py:594] name:model.last_layer.10.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.289 algo-1:134 INFO hook.py:594] name:model.last_layer.10.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.289 algo-1:134 INFO hook.py:594] name:model.last_layer.10.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.289 algo-1:134 INFO hook.py:594] name:model.last_layer.10.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.289 algo-1:134 INFO hook.py:594] name:model.last_layer.10.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.289 algo-1:134 INFO hook.py:594] name:model.last_layer.10.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.289 algo-1:134 INFO hook.py:594] name:model.last_layer.10.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.289 algo-1:134 INFO hook.py:594] name:model.last_layer.10.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.289 algo-1:134 INFO hook.py:594] name:model.last_layer.10.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.289 algo-1:134 INFO hook.py:594] name:model.last_layer.10.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.289 algo-1:134 INFO hook.py:594] name:model.last_layer.10.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.289 algo-1:134 INFO hook.py:594] name:model.last_layer.10.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.289 algo-1:134 INFO hook.py:594] name:model.last_layer.10.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.290 algo-1:134 INFO hook.py:594] name:model.last_layer.10.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.290 algo-1:134 INFO hook.py:594] name:model.last_layer.10.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.290 algo-1:134 INFO hook.py:594] name:model.last_layer.10.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.290 algo-1:134 INFO hook.py:594] name:model.last_layer.11.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.290 algo-1:134 INFO hook.py:594] name:model.last_layer.11.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.290 algo-1:134 INFO hook.py:594] name:model.last_layer.11.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.290 algo-1:134 INFO hook.py:594] name:model.last_layer.11.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.290 algo-1:134 INFO hook.py:594] name:model.last_layer.11.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.290 algo-1:134 INFO hook.py:594] name:model.last_layer.11.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.290 algo-1:134 INFO hook.py:594] name:model.last_layer.11.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.290 algo-1:134 INFO hook.py:594] name:model.last_layer.11.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.290 algo-1:134 INFO hook.py:594] name:model.last_layer.11.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.291 algo-1:134 INFO hook.py:594] name:model.last_layer.11.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.291 algo-1:134 INFO hook.py:594] name:model.last_layer.11.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.291 algo-1:134 INFO hook.py:594] name:model.last_layer.11.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.291 algo-1:134 INFO hook.py:594] name:model.last_layer.11.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.291 algo-1:134 INFO hook.py:594] name:model.last_layer.11.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.291 algo-1:134 INFO hook.py:594] name:model.last_layer.11.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.291 algo-1:134 INFO hook.py:594] name:model.last_layer.11.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.291 algo-1:134 INFO hook.py:594] name:model.last_layer.12.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.291 algo-1:134 INFO hook.py:594] name:model.last_layer.12.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.291 algo-1:134 INFO hook.py:594] name:model.last_layer.12.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.291 algo-1:134 INFO hook.py:594] name:model.last_layer.12.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.291 algo-1:134 INFO hook.py:594] name:model.last_layer.12.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.292 algo-1:134 INFO hook.py:594] name:model.last_layer.12.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.292 algo-1:134 INFO hook.py:594] name:model.last_layer.12.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.292 algo-1:134 INFO hook.py:594] name:model.last_layer.12.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.292 algo-1:134 INFO hook.py:594] name:model.last_layer.12.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.292 algo-1:134 INFO hook.py:594] name:model.last_layer.12.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.292 algo-1:134 INFO hook.py:594] name:model.last_layer.12.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.292 algo-1:134 INFO hook.py:594] name:model.last_layer.12.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.292 algo-1:134 INFO hook.py:594] name:model.last_layer.12.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.292 algo-1:134 INFO hook.py:594] name:model.last_layer.12.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.292 algo-1:134 INFO hook.py:594] name:model.last_layer.12.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.292 algo-1:134 INFO hook.py:594] name:model.last_layer.12.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.293 algo-1:134 INFO hook.py:594] name:model.last_layer.13.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.293 algo-1:134 INFO hook.py:594] name:model.last_layer.13.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.293 algo-1:134 INFO hook.py:594] name:model.last_layer.13.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.293 algo-1:134 INFO hook.py:594] name:model.last_layer.13.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.293 algo-1:134 INFO hook.py:594] name:model.last_layer.13.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.293 algo-1:134 INFO hook.py:594] name:model.last_layer.13.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.293 algo-1:134 INFO hook.py:594] name:model.last_layer.13.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.293 algo-1:134 INFO hook.py:594] name:model.last_layer.13.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.293 algo-1:134 INFO hook.py:594] name:model.last_layer.13.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.293 algo-1:134 INFO hook.py:594] name:model.last_layer.13.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.293 algo-1:134 INFO hook.py:594] name:model.last_layer.13.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.293 algo-1:134 INFO hook.py:594] name:model.last_layer.13.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.293 algo-1:134 INFO hook.py:594] name:model.last_layer.13.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.294 algo-1:134 INFO hook.py:594] name:model.last_layer.13.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.294 algo-1:134 INFO hook.py:594] name:model.last_layer.13.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.294 algo-1:134 INFO hook.py:594] name:model.last_layer.13.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.294 algo-1:134 INFO hook.py:594] name:model.last_layer.14.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.294 algo-1:134 INFO hook.py:594] name:model.last_layer.14.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.294 algo-1:134 INFO hook.py:594] name:model.last_layer.14.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.294 algo-1:134 INFO hook.py:594] name:model.last_layer.14.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.294 algo-1:134 INFO hook.py:594] name:model.last_layer.14.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.294 algo-1:134 INFO hook.py:594] name:model.last_layer.14.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.294 algo-1:134 INFO hook.py:594] name:model.last_layer.14.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.294 algo-1:134 INFO hook.py:594] name:model.last_layer.14.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.294 algo-1:134 INFO hook.py:594] name:model.last_layer.14.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.295 algo-1:134 INFO hook.py:594] name:model.last_layer.14.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.295 algo-1:134 INFO hook.py:594] name:model.last_layer.14.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.295 algo-1:134 INFO hook.py:594] name:model.last_layer.14.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.295 algo-1:134 INFO hook.py:594] name:model.last_layer.14.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.295 algo-1:134 INFO hook.py:594] name:model.last_layer.14.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.295 algo-1:134 INFO hook.py:594] name:model.last_layer.14.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.295 algo-1:134 INFO hook.py:594] name:model.last_layer.14.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.295 algo-1:134 INFO hook.py:594] name:model.last_layer.15.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.295 algo-1:134 INFO hook.py:594] name:model.last_layer.15.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.295 algo-1:134 INFO hook.py:594] name:model.last_layer.15.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.296 algo-1:134 INFO hook.py:594] name:model.last_layer.15.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.296 algo-1:134 INFO hook.py:594] name:model.last_layer.15.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.296 algo-1:134 INFO hook.py:594] name:model.last_layer.15.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.296 algo-1:134 INFO hook.py:594] name:model.last_layer.15.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.296 algo-1:134 INFO hook.py:594] name:model.last_layer.15.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.296 algo-1:134 INFO hook.py:594] name:model.last_layer.15.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.296 algo-1:134 INFO hook.py:594] name:model.last_layer.15.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.296 algo-1:134 INFO hook.py:594] name:model.last_layer.15.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.296 algo-1:134 INFO hook.py:594] name:model.last_layer.15.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.296 algo-1:134 INFO hook.py:594] name:model.last_layer.15.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.297 algo-1:134 INFO hook.py:594] name:model.last_layer.15.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.297 algo-1:134 INFO hook.py:594] name:model.last_layer.15.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.297 algo-1:134 INFO hook.py:594] name:model.last_layer.15.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.297 algo-1:134 INFO hook.py:594] name:model.last_layer.16.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.297 algo-1:134 INFO hook.py:594] name:model.last_layer.16.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.297 algo-1:134 INFO hook.py:594] name:model.last_layer.16.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.297 algo-1:134 INFO hook.py:594] name:model.last_layer.16.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.297 algo-1:134 INFO hook.py:594] name:model.last_layer.16.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.297 algo-1:134 INFO hook.py:594] name:model.last_layer.16.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.297 algo-1:134 INFO hook.py:594] name:model.last_layer.16.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.298 algo-1:134 INFO hook.py:594] name:model.last_layer.16.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.298 algo-1:134 INFO hook.py:594] name:model.last_layer.16.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.298 algo-1:134 INFO hook.py:594] name:model.last_layer.16.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.298 algo-1:134 INFO hook.py:594] name:model.last_layer.16.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.298 algo-1:134 INFO hook.py:594] name:model.last_layer.16.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.298 algo-1:134 INFO hook.py:594] name:model.last_layer.16.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.298 algo-1:134 INFO hook.py:594] name:model.last_layer.16.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.298 algo-1:134 INFO hook.py:594] name:model.last_layer.16.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.298 algo-1:134 INFO hook.py:594] name:model.last_layer.16.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.299 algo-1:134 INFO hook.py:594] name:model.last_layer.17.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.299 algo-1:134 INFO hook.py:594] name:model.last_layer.17.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.299 algo-1:134 INFO hook.py:594] name:model.last_layer.17.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.299 algo-1:134 INFO hook.py:594] name:model.last_layer.17.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.299 algo-1:134 INFO hook.py:594] name:model.last_layer.17.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.299 algo-1:134 INFO hook.py:594] name:model.last_layer.17.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.299 algo-1:134 INFO hook.py:594] name:model.last_layer.17.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.299 algo-1:134 INFO hook.py:594] name:model.last_layer.17.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.300 algo-1:134 INFO hook.py:594] name:model.last_layer.17.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.300 algo-1:134 INFO hook.py:594] name:model.last_layer.17.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.300 algo-1:134 INFO hook.py:594] name:model.last_layer.17.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.300 algo-1:134 INFO hook.py:594] name:model.last_layer.17.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.300 algo-1:134 INFO hook.py:594] name:model.last_layer.17.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.300 algo-1:134 INFO hook.py:594] name:model.last_layer.17.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.300 algo-1:134 INFO hook.py:594] name:model.last_layer.17.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.300 algo-1:134 INFO hook.py:594] name:model.last_layer.17.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.301 algo-1:134 INFO hook.py:594] name:model.last_layer.18.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.301 algo-1:134 INFO hook.py:594] name:model.last_layer.18.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.301 algo-1:134 INFO hook.py:594] name:model.last_layer.18.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.301 algo-1:134 INFO hook.py:594] name:model.last_layer.18.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.301 algo-1:134 INFO hook.py:594] name:model.last_layer.18.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.301 algo-1:134 INFO hook.py:594] name:model.last_layer.18.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.301 algo-1:134 INFO hook.py:594] name:model.last_layer.18.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.301 algo-1:134 INFO hook.py:594] name:model.last_layer.18.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.302 algo-1:134 INFO hook.py:594] name:model.last_layer.18.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.302 algo-1:134 INFO hook.py:594] name:model.last_layer.18.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.302 algo-1:134 INFO hook.py:594] name:model.last_layer.18.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.302 algo-1:134 INFO hook.py:594] name:model.last_layer.18.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.302 algo-1:134 INFO hook.py:594] name:model.last_layer.18.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.302 algo-1:134 INFO hook.py:594] name:model.last_layer.18.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.302 algo-1:134 INFO hook.py:594] name:model.last_layer.18.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.302 algo-1:134 INFO hook.py:594] name:model.last_layer.18.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.302 algo-1:134 INFO hook.py:594] name:model.last_layer.19.attention.self.query.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.302 algo-1:134 INFO hook.py:594] name:model.last_layer.19.attention.self.query.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.303 algo-1:134 INFO hook.py:594] name:model.last_layer.19.attention.self.key.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.303 algo-1:134 INFO hook.py:594] name:model.last_layer.19.attention.self.key.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.303 algo-1:134 INFO hook.py:594] name:model.last_layer.19.attention.self.value.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.303 algo-1:134 INFO hook.py:594] name:model.last_layer.19.attention.self.value.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.303 algo-1:134 INFO hook.py:594] name:model.last_layer.19.attention.output.dense.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.303 algo-1:134 INFO hook.py:594] name:model.last_layer.19.attention.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.303 algo-1:134 INFO hook.py:594] name:model.last_layer.19.attention.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.303 algo-1:134 INFO hook.py:594] name:model.last_layer.19.attention.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.303 algo-1:134 INFO hook.py:594] name:model.last_layer.19.intermediate.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.303 algo-1:134 INFO hook.py:594] name:model.last_layer.19.intermediate.dense.bias count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.303 algo-1:134 INFO hook.py:594] name:model.last_layer.19.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.304 algo-1:134 INFO hook.py:594] name:model.last_layer.19.output.dense.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.304 algo-1:134 INFO hook.py:594] name:model.last_layer.19.output.LayerNorm.weight count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.304 algo-1:134 INFO hook.py:594] name:model.last_layer.19.output.LayerNorm.bias count_params:384\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.304 algo-1:134 INFO hook.py:594] name:model.output_layer.0.weight count_params:6144\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.304 algo-1:134 INFO hook.py:594] name:model.output_layer.0.bias count_params:8\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.304 algo-1:134 INFO hook.py:594] name:model.output_layer.1.weight count_params:3840\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.304 algo-1:134 INFO hook.py:594] name:model.output_layer.1.bias count_params:5\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.304 algo-1:134 INFO hook.py:594] name:model.output_layer.2.weight count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.305 algo-1:134 INFO hook.py:594] name:model.output_layer.2.bias count_params:4\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.305 algo-1:134 INFO hook.py:594] name:model.output_layer.3.weight count_params:4608\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.305 algo-1:134 INFO hook.py:594] name:model.output_layer.3.bias count_params:6\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.305 algo-1:134 INFO hook.py:594] name:model.output_layer.4.weight count_params:4608\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.305 algo-1:134 INFO hook.py:594] name:model.output_layer.4.bias count_params:6\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.305 algo-1:134 INFO hook.py:594] name:model.output_layer.5.weight count_params:4608\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.306 algo-1:134 INFO hook.py:594] name:model.output_layer.5.bias count_params:6\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.306 algo-1:134 INFO hook.py:594] name:model.output_layer.6.weight count_params:3840\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.306 algo-1:134 INFO hook.py:594] name:model.output_layer.6.bias count_params:5\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.306 algo-1:134 INFO hook.py:594] name:model.output_layer.7.weight count_params:9216\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.306 algo-1:134 INFO hook.py:594] name:model.output_layer.7.bias count_params:12\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.306 algo-1:134 INFO hook.py:594] name:model.output_layer.8.weight count_params:2304\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.306 algo-1:134 INFO hook.py:594] name:model.output_layer.8.bias count_params:3\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.306 algo-1:134 INFO hook.py:594] name:model.output_layer.9.weight count_params:2304\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.306 algo-1:134 INFO hook.py:594] name:model.output_layer.9.bias count_params:3\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.306 algo-1:134 INFO hook.py:594] name:model.output_layer.10.weight count_params:5376\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.306 algo-1:134 INFO hook.py:594] name:model.output_layer.10.bias count_params:7\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.307 algo-1:134 INFO hook.py:594] name:model.output_layer.11.weight count_params:5376\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.307 algo-1:134 INFO hook.py:594] name:model.output_layer.11.bias count_params:7\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.307 algo-1:134 INFO hook.py:594] name:model.output_layer.12.weight count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.307 algo-1:134 INFO hook.py:594] name:model.output_layer.12.bias count_params:4\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.307 algo-1:134 INFO hook.py:594] name:model.output_layer.13.weight count_params:2304\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.307 algo-1:134 INFO hook.py:594] name:model.output_layer.13.bias count_params:3\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.307 algo-1:134 INFO hook.py:594] name:model.output_layer.14.weight count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.307 algo-1:134 INFO hook.py:594] name:model.output_layer.14.bias count_params:4\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.307 algo-1:134 INFO hook.py:594] name:model.output_layer.15.weight count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.307 algo-1:134 INFO hook.py:594] name:model.output_layer.15.bias count_params:4\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.307 algo-1:134 INFO hook.py:594] name:model.output_layer.16.weight count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.308 algo-1:134 INFO hook.py:594] name:model.output_layer.16.bias count_params:4\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.308 algo-1:134 INFO hook.py:594] name:model.output_layer.17.weight count_params:3840\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.308 algo-1:134 INFO hook.py:594] name:model.output_layer.17.bias count_params:5\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.309 algo-1:134 INFO hook.py:594] name:model.output_layer.18.weight count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.309 algo-1:134 INFO hook.py:594] name:model.output_layer.18.bias count_params:4\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.309 algo-1:134 INFO hook.py:594] name:model.output_layer.19.weight count_params:2304\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.309 algo-1:134 INFO hook.py:594] name:model.output_layer.19.bias count_params:3\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.309 algo-1:134 INFO hook.py:596] Total Trainable Params: 142562407\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.309 algo-1:134 INFO hook.py:423] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-07-07 11:48:19.312 algo-1:134 INFO hook.py:486] Hook is writing from the hook with pid: 134\u001b[0m\n",
      "\n",
      "2022-07-07 11:51:04 Stopping - Stopping the training job\u001b[34m#015                                                              #015#015Training: 0it [00:00, ?it/s]#015Training:   0%|          | 0/10474 [00:00<?, ?it/s]#015Epoch 0:   0%|          | 0/10474 [00:00<?, ?it/s] #015Epoch 0:   0%|          | 40/10474 [00:16<1:11:53,  2.42it/s]#015Epoch 0:   0%|          | 40/10474 [00:16<1:11:53,  2.42it/s, loss=2.19e+03, val_loss=4.55e+3]#015Epoch 0:   1%|          | 80/10474 [00:32<1:11:11,  2.43it/s, loss=2.19e+03, val_loss=4.55e+3]#015Epoch 0:   1%|          | 80/10474 [00:32<1:11:11,  2.43it/s, loss=1.86e+03, val_loss=4.55e+3]#015Epoch 0:   1%|          | 120/10474 [00:49<1:11:29,  2.41it/s, loss=1.86e+03, val_loss=4.55e+3]#015Epoch 0:   1%|          | 120/10474 [00:49<1:11:29,  2.41it/s, loss=1.62e+03, val_loss=4.55e+3]#015Epoch 0:   2%|▏         | 160/10474 [01:06<1:11:01,  2.42it/s, loss=1.62e+03, val_loss=4.55e+3]#015Epoch 0:   2%|▏         | 160/10474 [01:06<1:11:01,  2.42it/s, loss=1.5e+03, val_loss=4.55e+3] #015Epoch 0:   2%|▏         | 200/10474 [01:22<1:10:35,  2.43it/s, loss=1.5e+03, val_loss=4.55e+3]#015Epoch 0:   2%|▏         | 200/10474 [01:22<1:10:35,  2.43it/s, loss=1.41e+03, val_loss=4.55e+3]#015Epoch 0:   2%|▏         | 240/10474 [01:38<1:10:14,  2.43it/s, loss=1.41e+03, val_loss=4.55e+3]#015Epoch 0:   2%|▏         | 240/10474 [01:38<1:10:14,  2.43it/s, loss=1.32e+03, val_loss=4.55e+3]#015Epoch 0:   3%|▎         | 280/10474 [01:55<1:10:13,  2.42it/s, loss=1.32e+03, val_loss=4.55e+3]#015Epoch 0:   3%|▎         | 280/10474 [01:55<1:10:13,  2.42it/s, loss=1.25e+03, val_loss=4.55e+3]#015Epoch 0:   3%|▎         | 320/10474 [02:12<1:09:58,  2.42it/s, loss=1.25e+03, val_loss=4.55e+3]#015Epoch 0:   3%|▎         | 320/10474 [02:12<1:09:58,  2.42it/s, loss=1.19e+03, val_loss=4.55e+3]#015Epoch 0:   3%|▎         | 360/10474 [02:29<1:09:46,  2.42it/s, loss=1.19e+03, val_loss=4.55e+3]#015Epoch 0:   3%|▎         | 360/10474 [02:29<1:09:46,  2.42it/s, loss=1.15e+03, val_loss=4.55e+3]#015Epoch 0:   4%|▍         | 400/10474 [02:45<1:09:38,  2.41it/s, loss=1.15e+03, val_loss=4.55e+3]#015Epoch 0:   4%|▍         | 400/10474 [02:45<1:09:38,  2.41it/s, loss=1.11e+03, val_loss=4.55e+3]#015Epoch 0:   4%|▍         | 440/10474 [03:03<1:09:49,  2.39it/s, loss=1.11e+03, val_loss=4.55e+3]#015Epoch 0:   4%|▍         | 440/10474 [03:03<1:09:49,  2.39it/s, loss=1.08e+03, val_loss=4.55e+3]#015Epoch 0:   5%|▍         | 480/10474 [03:21<1:09:57,  2.38it/s, loss=1.08e+03, val_loss=4.55e+3]#015Epoch 0:   5%|▍         | 480/10474 [03:21<1:09:57,  2.38it/s, loss=1.05e+03, val_loss=4.55e+3]#015Epoch 0:   5%|▍         | 520/10474 [03:50<1:13:29,  2.26it/s, loss=1.05e+03, val_loss=4.55e+3]#015Epoch 0:   5%|▍         | 520/10474 [03:50<1:13:29,  2.26it/s, loss=1.02e+03, val_loss=4.55e+3]#015Epoch 0:   5%|▌         | 560/10474 [04:08<1:13:18,  2.25it/s, loss=1.02e+03, val_loss=4.55e+3]#015Epoch 0:   5%|▌         | 560/10474 [04:08<1:13:18,  2.25it/s, loss=995, val_loss=4.55e+3]     #015Epoch 0:   6%|▌         | 600/10474 [04:26<1:13:06,  2.25it/s, loss=995, val_loss=4.55e+3]#015Epoch 0:   6%|▌         | 600/10474 [04:26<1:13:06,  2.25it/s, loss=963, val_loss=4.55e+3]\u001b[0m\n",
      "\n",
      "2022-07-07 11:53:22 Uploading - Uploading generated training model\n",
      "2022-07-07 11:53:22 Stopped - Training job stopped\n",
      "ProfilerReport-1657193958: Stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job ended with status 'Stopped' rather than 'Completed'. This could mean the job timed out or stopped early for some other reason: Consider checking whether it completed as you expect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 710\n",
      "Billable seconds: 710\n"
     ]
    }
   ],
   "source": [
    "# Fit the estimator\n",
    "estimator.fit(fit_arguments, job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a5ddf8e25d962f331e8059973cfd97c5aef9d0ccfdd243943e9f1f512e91043"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('deepl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
