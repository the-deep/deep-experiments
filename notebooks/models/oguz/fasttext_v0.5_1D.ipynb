{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.getcwd() + \"../../../../\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def strip_whitespaces(arr):\n",
    "    return [\n",
    "        item.replace(' ', '_')\n",
    "        for item in arr\n",
    "    ]\n",
    "\n",
    "def read_dataset(path):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # evaluate lists\n",
    "    df[\"pillars_1d\"] = df[\"pillars_1d\"].apply(literal_eval)\n",
    "    df[\"subpillars_1d\"] = df[\"subpillars_1d\"].apply(literal_eval)\n",
    "\n",
    "    # remove whitespaces\n",
    "    df[\"pillars_1d\"] = df[\"pillars_1d\"].apply(strip_whitespaces)\n",
    "    df[\"subpillars_1d\"] = df[\"subpillars_1d\"].apply(strip_whitespaces)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_unique(df, field):\n",
    "    unique = set()\n",
    "    occurrences = list()\n",
    "    for pils in df[field]:\n",
    "        unique.update(pils)\n",
    "        occurrences.extend(pils)\n",
    "\n",
    "    print(Counter(occurrences).most_common())\n",
    "    return list(unique)\n",
    "\n",
    "def preprocess_dataset(df, pillars, subpillars):\n",
    "    # add columns for pillars\n",
    "    for pillar in pillars:\n",
    "        df[pillar] = 0\n",
    "\n",
    "    # add columns for subpillars\n",
    "    for subpillar in subpillars:\n",
    "        df[subpillar] = 0\n",
    "\n",
    "    # add rows for pillars + subpillars \n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0]):    \n",
    "        for pillar in row[\"pillars_1d\"]:\n",
    "            df.loc[idx, pillar] = 1\n",
    "        for subpillar in row[\"subpillars_1d\"]:\n",
    "            df.loc[idx, subpillar] = 1\n",
    "    return df\n"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T14:01:37.213628Z",
     "start_time": "2021-05-31T14:01:35.197651Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_train = read_dataset(\"../../../data/frameworks_data/data_v0.5/data_v0.5_train.csv\")\n",
    "df_val = read_dataset(\"../../../data/frameworks_data/data_v0.5/data_v0.5_val.csv\")\n",
    "\n",
    "print(df_train.columns)\n",
    "\n",
    "pillars = get_unique(df_train, \"pillars_1d\")\n",
    "subpillars = get_unique(df_train, \"subpillars_1d\")\n",
    "\n",
    "df_train = preprocess_dataset(df_train, pillars, subpillars)\n",
    "df_val = preprocess_dataset(df_val, pillars, subpillars)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_train.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "# fasttext.FastText.eprint = lambda x: None # suppress warnings\n",
    "\n",
    "def prepare_ground_truth(df, subpillars):\n",
    "    X = df['excerpt'].tolist()\n",
    "    y = np.stack([df[sp].to_numpy() for sp in subpillars])\n",
    "    y = np.transpose(y)\n",
    "    y = [ y[i, :].tolist() for i in range(y.shape[0]) ]\n",
    "    return X, y\n",
    "\n",
    "def prepare_fasttext_input(X, y, subpillars, path):\n",
    "    with open(path, 'w') as f:\n",
    "        for xi, yi in tqdm(zip(X, y), total=len(X)):\n",
    "            yi = [f'__label__{subpillars[i]}' for i, label in enumerate(yi) if label == 1]\n",
    "            f.write(' '.join(yi + [xi]) + '\\n')\n",
    "\n",
    "def train(input, **hparams):\n",
    "    return fasttext.train_supervised(\n",
    "        input=path,\n",
    "        **hparams)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "models = []\n",
    "preds_train, preds_val = [], []\n",
    "gt_train, gt_val = [], []\n",
    "\n",
    "hparams = {\n",
    "    'lr': 0.1,\n",
    "    'epoch': 5,\n",
    "    'wordNgrams': 5,\n",
    "    'bucket': 2000000,\n",
    "    'dim': 256,\n",
    "    'loss': 'ova'\n",
    "}\n",
    "\n",
    "os.makedirs(\"ftdata\", exist_ok=True)\n",
    "for pillar in pillars:\n",
    "    # get associated subpillars \n",
    "    sp = [ s for s in subpillars if (pillar + '->') in s ]\n",
    "\n",
    "    print('Pillar: ', pillar)\n",
    "    print('Subpillars: ', sp)\n",
    "\n",
    "    # prepare input to training\n",
    "    print('Preparing ground truth...')\n",
    "    X, y = prepare_ground_truth(df_train, sp)\n",
    "\n",
    "    path = f\"ftdata/train_{pillar}.txt\"\n",
    "    if not os.path.exists(path):\n",
    "        print('Preparing input text file...')\n",
    "        prepare_fasttext_input(X, y, sp, path)\n",
    "    else:\n",
    "        print('Using existing text file...')\n",
    "\n",
    "    # train model\n",
    "    print('Training model...')\n",
    "    model = train(path, **hparams)\n",
    "\n",
    "    # predictions on train set\n",
    "    print('Predicting on the train set...')\n",
    "    preds = model.predict(X, k=-1)\n",
    "    preds_train.append(preds)\n",
    "\n",
    "    # match ground truth to model label ordering\n",
    "    labels = [ label[9:] for label in model.get_labels() ]\n",
    "    y = np.array(y)\n",
    "    y_gt = np.zeros_like(y)\n",
    "    for i, label in enumerate(labels):\n",
    "        y_gt[:, i] = y[:, sp.index(label)]\n",
    "    gt_train.append(y_gt)\n",
    "\n",
    "    # predictions on val set\n",
    "    print('Predicting on the test set...')\n",
    "    X, y = prepare_ground_truth(df_val, sp)\n",
    "    preds = model.predict(X, k=-1)\n",
    "    preds_val.append(preds)\n",
    "\n",
    "    # match ground truth to model label ordering\n",
    "    y = np.array(y)\n",
    "    y_gt = np.zeros_like(y)\n",
    "    for i, label in enumerate(labels):\n",
    "        y_gt[:, i] = y[:, sp.index(label)]\n",
    "    gt_val.append(y_gt)\n",
    "\n",
    "    # save the model\n",
    "    models.append(model)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T14:03:52.322115Z",
     "start_time": "2021-05-31T14:03:52.201251Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate(gt, pred):\n",
    "    gt = np.asarray(gt)\n",
    "    pred = np.asarray(pred) > 0.5\n",
    "\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(gt, pred)\n",
    "    print('Precision: ', precision)\n",
    "    print('Recall: ', recall)\n",
    "    print('Fscore: ', fscore)\n",
    "    print('Support: ', support)\n",
    "    return precision, recall, fscore, support"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "classes = []\n",
    "metrics_train = {\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'fscore': [],\n",
    "    'support': []\n",
    "}\n",
    "metrics_val = {\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'fscore': [],\n",
    "    'support': []\n",
    "}\n",
    "\n",
    "for i, pillar in enumerate(pillars):\n",
    "    # get associated subpillars \n",
    "    sp = [ s for s in subpillars if (pillar + '->') in s ]\n",
    "\n",
    "    print('Pillar: ', pillar)\n",
    "    #print('Subpillars: ', sp)\n",
    "    print(\"Classes:\", models[i].get_labels())\n",
    "    classes.extend(models[i].get_labels())\n",
    "\n",
    "    print('Running evaluation on training set...')\n",
    "    precision, recall, fscore, support = evaluate(gt_train[i], preds_train[i][1])\n",
    "    metrics_train['precision'].extend(precision)\n",
    "    metrics_train['recall'].extend(recall)\n",
    "    metrics_train['fscore'].extend(fscore)\n",
    "    metrics_train['support'].extend(support)\n",
    "\n",
    "    print('Running evaluation on validation set...')\n",
    "    precision, recall, fscore, support = evaluate(gt_val[i], preds_val[i][1])\n",
    "    metrics_val['precision'].extend(precision)\n",
    "    metrics_val['recall'].extend(recall)\n",
    "    metrics_val['fscore'].extend(fscore)\n",
    "    metrics_val['support'].extend(support)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T14:08:14.650541Z",
     "start_time": "2021-05-31T14:08:14.628034Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Calculating macro training metrics...')\n",
    "for metric in metrics_train:\n",
    "    metrics_train['macro_' + metric] = np.array(metrics_train[metric]).mean()\n",
    "    print(metric, metrics_train['macro_' + metric])\n",
    "\n",
    "print()\n",
    "\n",
    "print('Calculating macro validation metrics...')\n",
    "for metric in metrics_val:\n",
    "    metrics_val['macro_' + metric] = np.array(metrics_val[metric]).mean()\n",
    "    print(metric, metrics_val['macro_' + metric])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tracking"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import mlflow\n",
    "\n",
    "from deep.constants import MLFLOW_SERVER\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_SERVER)\n",
    "mlflow.set_experiment('fasttext_v0.5_1D')\n",
    "mlflow.log_params(hparams)\n",
    "\n",
    "for metric in metrics_train:\n",
    "    if 'macro' in metric:\n",
    "        mlflow.log_metric(f'train_{metric}', metrics_train[metric])"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T14:08:48.543098Z",
     "start_time": "2021-05-31T14:08:47.964843Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edf37a5c134433dc7c91edbc64d783a1f377a9eacd02d109a75d6112334aa942"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('deepl': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}