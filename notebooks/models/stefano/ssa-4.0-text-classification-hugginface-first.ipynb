{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8963ad8",
   "metadata": {},
   "source": [
    "Using kernel `conda_pytorch_latest_p36`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84454f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1715e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e91fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfdd83f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8dd9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    'Agricolture',\n",
    "    'Cross',\n",
    "    'Education',\n",
    "    'Food Security',\n",
    "    'Health',\n",
    "    'Livelihoods',\n",
    "    'Logistics',\n",
    "    'Nutrition',\n",
    "    'Protection',\n",
    "    'Shelter',\n",
    "    'Wash'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64060ef",
   "metadata": {},
   "source": [
    "## Sagemaker Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd532b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::519887312542:role/AmazonSageMakerFullAccessRole\n",
      "sagemaker-stefano\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\n",
    "    role\n",
    ")  # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "bucket = 'sagemaker-stefano'\n",
    "print(bucket)\n",
    "prefix = \"huggingface/first\"  # Replace with the prefix under which you want to store the data if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a107c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fde06f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'per_device_train_batch_size': 32,\n",
    "                 'model_name_or_path': 'distilbert-base-uncased'\n",
    "                 }\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point='train.py',\n",
    "        source_dir='./',\n",
    "        instance_type='ml.p3.2xlarge',\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        transformers_version='4.4',\n",
    "        pytorch_version='1.6',\n",
    "        py_version='py36',\n",
    "        hyperparameters = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b17d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "huggingface_estimator.fit(\n",
    "  {'train': 's3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/train',\n",
    "   'test': 's3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/test'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628525eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05757aab",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efa1453",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a8a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path('data_prep/final_data/en/')\n",
    "raw = pd.read_csv('data_prep/data/entries_raw.csv')\n",
    "\n",
    "train = pd.read_csv(data / 'sentences_en_train.csv')\n",
    "test = pd.read_csv(data / 'sentences_en_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ff2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, train):\n",
    "    relevant_train = df[df.is_relevant == 1]\n",
    "    relevant_train.sector_ids = relevant_train.sector_ids.apply(eval)\n",
    "    relevant_train = relevant_train[relevant_train.sector_ids.apply(len) > 0]\n",
    "    \n",
    "    relevant_train['tokenized_sentence'] = relevant_train.sentence_text.apply(\n",
    "        lambda x: nltk.word_tokenize(x.lower())\n",
    "    )\n",
    "    relevant_train['file_content'] = relevant_train.apply(\n",
    "        lambda x: {\"source\":' '.join(x.tokenized_sentence), \"label\":x.sector_ids}, axis=1\n",
    "    )\n",
    "        \n",
    "    return relevant_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocessing(train, True)\n",
    "test_df = preprocessing(test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de441f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('immap.train', 'w') as f:\n",
    "    for line in train_df.file_content:\n",
    "        f.write(json.dumps(line) + '\\n')\n",
    "        \n",
    "with open('immap.validation', 'w') as f:\n",
    "    for line in test_df.file_content:\n",
    "        f.write(json.dumps(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb929150",
   "metadata": {},
   "source": [
    "### Bucket upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = prefix + \"/train/immap.train\"\n",
    "validation_channel = prefix + \"/validation/immap.validation\"\n",
    "\n",
    "sess.upload_data(path=\"immap.train\", bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path=\"immap.validation\", bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = \"s3://{}/{}\".format(bucket, train_channel)\n",
    "s3_validation_data = \"s3://{}/{}\".format(bucket, validation_channel)\n",
    "\n",
    "s3_output_location = \"s3://{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc98449a",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edffc60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print(\"Using SageMaker BlazingText container: {} ({})\".format(container, region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4739d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.4xlarge\",\n",
    "    volume_size=30,\n",
    "    max_run=360000,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    hyperparameters={\n",
    "        \"mode\": \"supervised\",\n",
    "        \"epochs\": 4,\n",
    "        \"min_count\": 2,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"vector_dim\": 10,\n",
    "#         \"early_stopping\": False,\n",
    "        \"patience\": 4,\n",
    "#         \"min_epochs\": 5,\n",
    "#         \"word_ngrams\": 2,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a01a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train data channel with S3_data_type as 'AugmentedManifestFile' and attribute names.\n",
    "# train_data = sagemaker.session.s3_input(\n",
    "#     your_augmented_manifest_file,\n",
    "#     distribution='FullyReplicated',\n",
    "#     content_type='application/x-recordio',\n",
    "#     s3_data_type='AugmentedManifestFile',\n",
    "#     attribute_names=['source-ref', 'annotations'],\n",
    "#     input_mode='Pipe',\n",
    "#     record_wrapping='RecordIO'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a510e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_train_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"AugmentedManifestFile\",\n",
    "    input_mode='Pipe',\n",
    "    attribute_names=['source', 'label'],\n",
    "    record_wrapping='RecordIO',\n",
    ")\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_validation_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"AugmentedManifestFile\",\n",
    "    input_mode='Pipe',\n",
    "    attribute_names=['source', 'label'],\n",
    "    record_wrapping='RecordIO',\n",
    ")\n",
    "data_channels = {\"train\": train_data, \"validation\": validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f9409",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a4a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "text_classifier = bt_model.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m4.xlarge\", serializer=JSONSerializer()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebc826",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(test_df.sentence_text)\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [\" \".join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "payload = {\"instances\": tokenized_sentences, \"configuration\": {\"k\": 5}}\n",
    "\n",
    "response = text_classifier.predict(payload)\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fcf908",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00977925",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "thresh = 0.05\n",
    "for pred in predictions:\n",
    "    labels = [int(x.replace('__label__', '')) for x, y in zip(pred['label'], pred['prob']) if y > thresh]\n",
    "    a.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t['preds'] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d837f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "f1_scores = []\n",
    "\n",
    "for i, class_ in enumerate(classes):\n",
    "    class_preds = [1 if i in x else 0 for x in t.preds]\n",
    "    class_targets = [1 if i in x else 0 for x in t.sector_ids]\n",
    "\n",
    "    indexes.append(class_)\n",
    "    precisions.append(precision_score(class_targets, class_preds))\n",
    "    recalls.append(recall_score(class_targets, class_preds))    \n",
    "    f1_scores.append(f1_score(class_targets, class_preds))        \n",
    "\n",
    "\n",
    "all_metrics = pd.DataFrame(\n",
    "    {\n",
    "        'class': indexes,\n",
    "        'recall': recalls,\n",
    "        'precision': precisions,\n",
    "        'f1_score': f1_scores\n",
    "    }\n",
    ").set_index('class', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f49426",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics.plot(\n",
    "    figsize=(20, 10), xticks=range(12), yticks=[x/10 for x in range(11)], ylim=(0, 1), grid=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38133223",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06127542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d50da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
