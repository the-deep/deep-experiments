{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da175455",
   "metadata": {},
   "source": [
    "Using kernel `conda_pytorch_latest_p36`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ea103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a548b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01727bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc79461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fef4595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae160a",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "317eb62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FRAMEWORKS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8efbe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefano/miniconda3/envs/deep-exp/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "afexportable = pd.read_csv(data /'afexportable_of_af_of_projects_of_interest.csv')\n",
    "all_afs = pd.read_csv(data /'all_afs.csv')\n",
    "\n",
    "proj_interest = pd.read_csv(data / 'projects_of_interest.csv')\n",
    "entr_proj_interest = pd.read_csv(data / 'entries_of_projects_of_interest.csv')\n",
    "exp_proj_interest = pd.read_csv(data / 'exportdata_of_entries_of_projects_of_interest.csv')\n",
    "wid_proj_interest = pd.read_csv(data / 'widgets_of_afs_of_interest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23af072b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefano/miniconda3/envs/deep-exp/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(FRAMEWORKS_PATH / 'data_v0.3_train.csv')\n",
    "val = pd.read_csv(FRAMEWORKS_PATH / 'data_v0.3_val.csv')\n",
    "test = pd.read_csv(FRAMEWORKS_PATH / 'data_v0.3_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce66a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['excerpt'] + DIMENSION_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b868434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_sector(df, sector):\n",
    "    df = df[['excerpt', sector]]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d150d2be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = process_for_sector(train, 'Humanitarian Conditions')\n",
    "test_df = process_for_sector(test, 'Humanitarian Conditions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24425c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>excerpt</th>\n",
       "      <th>Humanitarian Conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our concern for the impact of the pandemic on ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WHO has provided sanitizers, PPE, and 92 infra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For COVID-19: • Another batch of supplies, und...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On 28 April, the first repatriation flight of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNICEF has further provided additional support...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85917</th>\n",
       "      <td>Kismayo IDPs registered a GAM prevalence of 12...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85918</th>\n",
       "      <td>The poor nutrition situation in Kismayo IDPs i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85919</th>\n",
       "      <td>Kismayo Urban recorded a GAM prevalence of 12....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85920</th>\n",
       "      <td>Galkacyo IDPs recorded a GAM prevalence of 10....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85921</th>\n",
       "      <td>As of 7 April, Somalia has recorded close to 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85922 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 excerpt  \\\n",
       "0      Our concern for the impact of the pandemic on ...   \n",
       "1      WHO has provided sanitizers, PPE, and 92 infra...   \n",
       "2      For COVID-19: • Another batch of supplies, und...   \n",
       "3      On 28 April, the first repatriation flight of ...   \n",
       "4      UNICEF has further provided additional support...   \n",
       "...                                                  ...   \n",
       "85917  Kismayo IDPs registered a GAM prevalence of 12...   \n",
       "85918  The poor nutrition situation in Kismayo IDPs i...   \n",
       "85919  Kismayo Urban recorded a GAM prevalence of 12....   \n",
       "85920  Galkacyo IDPs recorded a GAM prevalence of 10....   \n",
       "85921  As of 7 April, Somalia has recorded close to 1...   \n",
       "\n",
       "       Humanitarian Conditions  \n",
       "0                            1  \n",
       "1                            0  \n",
       "2                            0  \n",
       "3                            0  \n",
       "4                            0  \n",
       "...                        ...  \n",
       "85917                        1  \n",
       "85918                        1  \n",
       "85919                        0  \n",
       "85920                        0  \n",
       "85921                        0  \n",
       "\n",
       "[85922 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5379ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle('train_df.pickle', protocol=4)\n",
    "test_df.to_pickle('test_df.pickle', protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be975a",
   "metadata": {},
   "source": [
    "## Sagemaker Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1891c670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmazonSageMaker-ExecutionRole-20210519T102514\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = 'AmazonSageMaker-ExecutionRole-20210519T102514'\n",
    "print(\n",
    "    role\n",
    ")  # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "bucket = SAGEMAKER_BUCKET\n",
    "prefix = \"huggingface/first\"  # Replace with the prefix under which you want to store the data if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc69433",
   "metadata": {},
   "source": [
    "### Bucket upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c84bf094",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bucket_path = 'test1/data'\n",
    "train_channel = bucket_path + \"/train_df.pickle\"\n",
    "validation_channel = bucket_path + \"/test_df.pickle\"\n",
    "\n",
    "sess.upload_data(path=\"train_df.pickle\", bucket=SAGEMAKER_BUCKET, key_prefix=bucket_path)\n",
    "sess.upload_data(path=\"test_df.pickle\", bucket=SAGEMAKER_BUCKET, key_prefix=bucket_path)\n",
    "\n",
    "s3_train_data = f\"s3://{SAGEMAKER_BUCKET}/{train_channel}\"\n",
    "s3_validation_data = f\"s3://{SAGEMAKER_BUCKET}/{validation_channel}\"\n",
    "\n",
    "s3_output_location = f\"s3://{SAGEMAKER_BUCKET}/{bucket_path}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c19cf858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'per_device_train_batch_size': 32,\n",
    "                 'model_name': 'distilbert-base-uncased'\n",
    "                 }\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point='train.py',\n",
    "        source_dir=str(SCRIPTS_MODELS_PATH / 'stefano'),\n",
    "        instance_type='ml.p3.2xlarge',\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        transformers_version='4.4',\n",
    "        pytorch_version='1.6',\n",
    "        py_version='py36',\n",
    "        hyperparameters = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6e1e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_arguments = {\n",
    "    'train': f's3://{SAGEMAKER_BUCKET}/{bucket_path}',\n",
    "    'test': f's3://{SAGEMAKER_BUCKET}/{bucket_path}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bf46cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 13:09:12 Starting - Starting the training job...\n",
      "2021-05-25 13:09:14 Starting - Launching requested ML instancesProfilerReport-1621948150: InProgress\n",
      "......\n",
      "2021-05-25 13:10:38 Starting - Preparing the instances for training......\n",
      "2021-05-25 13:11:53 Downloading - Downloading input data...\n",
      "2021-05-25 13:12:19 Training - Downloading the training image...............\n",
      "2021-05-25 13:15:09 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-05-25 13:15:10,386 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-05-25 13:15:10,410 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-05-25 13:15:16,679 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-05-25 13:15:17,167 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"per_device_train_batch_size\": 32,\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2021-05-25-13-09-08-999\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-961104659532/huggingface-pytorch-training-2021-05-25-13-09-08-999/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"per_device_train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-961104659532/huggingface-pytorch-training-2021-05-25-13-09-08-999/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"per_device_train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2021-05-25-13-09-08-999\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-961104659532/huggingface-pytorch-training-2021-05-25-13-09-08-999/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--per_device_train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --epochs 1 --model_name distilbert-base-uncased --per_device_train_batch_size 32\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2021-05-25 13:15:21,909 - __main__ - INFO -  loaded train_dataset length is: (85922, 2)\u001b[0m\n",
      "\u001b[34m2021-05-25 13:15:21,909 - __main__ - INFO -  loaded test_dataset length is: (11617, 2)\u001b[0m\n",
      "\u001b[34m2021-05-25 13:15:22,019 - filelock - INFO - Lock 139954082310520 acquired on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m2021-05-25 13:15:22,048 - filelock - INFO - Lock 139954082310520 released on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m2021-05-25 13:15:22,065 - filelock - INFO - Lock 139954132156032 acquired on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m2021-05-25 13:15:22,097 - filelock - INFO - Lock 139954132156032 released on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m2021-05-25 13:15:22,151 - filelock - INFO - Lock 139954132156032 acquired on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\u001b[0m\n",
      "\u001b[34m2021-05-25 13:15:22,168 - filelock - INFO - Lock 139954132156032 released on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\u001b[0m\n",
      "\u001b[34m2021-05-25 13:15:22,231 - filelock - INFO - Lock 139954132156032 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-05-25 13:15:22,246 - filelock - INFO - Lock 139954132156032 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-05-25 13:15:22,265 - filelock - INFO - Lock 139954082308336 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m2021-05-25 13:15:27,794 - filelock - INFO - Lock 139954082308336 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2021-05-25 13:16:00.761 algo-1:25 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:00.977 algo-1:25 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:00.977 algo-1:25 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:00.978 algo-1:25 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.087 algo-1:25 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.087 algo-1:25 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.319 algo-1:25 INFO hook.py:550] name:distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.319 algo-1:25 INFO hook.py:550] name:distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.319 algo-1:25 INFO hook.py:550] name:distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.319 algo-1:25 INFO hook.py:550] name:distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.320 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.320 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.320 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.320 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.320 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.320 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.320 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.320 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.320 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.320 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.321 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.321 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.321 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.321 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.321 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.321 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.321 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.321 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.321 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.321 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.321 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.322 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.322 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.322 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.322 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.322 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.322 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.322 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.322 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.322 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.323 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.323 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.323 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.323 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.323 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.323 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.323 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.323 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.323 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.323 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.323 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.324 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.324 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.324 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.324 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.324 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.324 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.324 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.324 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.325 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.325 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.325 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.325 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.325 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.325 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.325 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.325 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.326 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.326 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.326 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.326 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.326 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.326 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.326 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.326 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.326 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.326 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.327 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.327 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.327 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.327 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.327 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.327 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.327 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.327 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.327 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.328 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.328 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.328 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.328 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.328 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.328 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.328 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.328 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.328 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.328 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.329 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.329 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.329 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.329 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.329 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.329 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.329 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.329 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.329 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.329 algo-1:25 INFO hook.py:550] name:distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.330 algo-1:25 INFO hook.py:550] name:pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.330 algo-1:25 INFO hook.py:550] name:pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.330 algo-1:25 INFO hook.py:550] name:classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.330 algo-1:25 INFO hook.py:550] name:classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.330 algo-1:25 INFO hook.py:552] Total Trainable Params: 66955010\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.330 algo-1:25 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:01.333 algo-1:25 INFO hook.py:476] Hook is writing from the hook with pid: 25\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:02.698 algo-1:25 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:02.698 algo-1:25 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-05-25 13:16:02.806 algo-1:25 WARNING hook.py:1033] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "huggingface_estimator.fit(fit_arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c68b6d3",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a60754",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a88b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print(\"Using SageMaker BlazingText container: {} ({})\".format(container, region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.4xlarge\",\n",
    "    volume_size=30,\n",
    "    max_run=360000,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    hyperparameters={\n",
    "        \"mode\": \"supervised\",\n",
    "        \"epochs\": 4,\n",
    "        \"min_count\": 2,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"vector_dim\": 10,\n",
    "#         \"early_stopping\": False,\n",
    "        \"patience\": 4,\n",
    "#         \"min_epochs\": 5,\n",
    "#         \"word_ngrams\": 2,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4597989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train data channel with S3_data_type as 'AugmentedManifestFile' and attribute names.\n",
    "# train_data = sagemaker.session.s3_input(\n",
    "#     your_augmented_manifest_file,\n",
    "#     distribution='FullyReplicated',\n",
    "#     content_type='application/x-recordio',\n",
    "#     s3_data_type='AugmentedManifestFile',\n",
    "#     attribute_names=['source-ref', 'annotations'],\n",
    "#     input_mode='Pipe',\n",
    "#     record_wrapping='RecordIO'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9accc1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_train_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"AugmentedManifestFile\",\n",
    "    input_mode='Pipe',\n",
    "    attribute_names=['source', 'label'],\n",
    "    record_wrapping='RecordIO',\n",
    ")\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_validation_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"AugmentedManifestFile\",\n",
    "    input_mode='Pipe',\n",
    "    attribute_names=['source', 'label'],\n",
    "    record_wrapping='RecordIO',\n",
    ")\n",
    "data_channels = {\"train\": train_data, \"validation\": validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f3f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ff3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb8b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "text_classifier = bt_model.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m4.xlarge\", serializer=JSONSerializer()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0a8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(test_df.sentence_text)\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [\" \".join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "payload = {\"instances\": tokenized_sentences, \"configuration\": {\"k\": 5}}\n",
    "\n",
    "response = text_classifier.predict(payload)\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc8e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "thresh = 0.05\n",
    "for pred in predictions:\n",
    "    labels = [int(x.replace('__label__', '')) for x, y in zip(pred['label'], pred['prob']) if y > thresh]\n",
    "    a.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bb830",
   "metadata": {},
   "outputs": [],
   "source": [
    "t['preds'] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d049d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "f1_scores = []\n",
    "\n",
    "for i, class_ in enumerate(classes):\n",
    "    class_preds = [1 if i in x else 0 for x in t.preds]\n",
    "    class_targets = [1 if i in x else 0 for x in t.sector_ids]\n",
    "\n",
    "    indexes.append(class_)\n",
    "    precisions.append(precision_score(class_targets, class_preds))\n",
    "    recalls.append(recall_score(class_targets, class_preds))    \n",
    "    f1_scores.append(f1_score(class_targets, class_preds))        \n",
    "\n",
    "\n",
    "all_metrics = pd.DataFrame(\n",
    "    {\n",
    "        'class': indexes,\n",
    "        'recall': recalls,\n",
    "        'precision': precisions,\n",
    "        'f1_score': f1_scores\n",
    "    }\n",
    ").set_index('class', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c9975",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics.plot(\n",
    "    figsize=(20, 10), xticks=range(12), yticks=[x/10 for x in range(11)], ylim=(0, 1), grid=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d3daed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63941d06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
